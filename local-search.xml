<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>公众号“看图学”试题合集（5）</title>
    <link href="/2025/02/12/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/AI%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98005%EF%BC%9A%E5%85%AC%E4%BC%97%E5%8F%B7%E2%80%9C%E7%9C%8B%E5%9B%BE%E5%AD%A6%E2%80%9D%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86%EF%BC%885%EF%BC%89/"/>
    <url>/2025/02/12/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/AI%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98005%EF%BC%9A%E5%85%AC%E4%BC%97%E5%8F%B7%E2%80%9C%E7%9C%8B%E5%9B%BE%E5%AD%A6%E2%80%9D%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86%EF%BC%885%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="1-RoPE-旋转位置编码"><a href="#1-RoPE-旋转位置编码" class="headerlink" title="1.  RoPE 旋转位置编码"></a>1.  RoPE 旋转位置编码</h1><p><a href="https://mp.weixin.qq.com/s/57NF280gvfM7MR0UqSx3Uw">这么解释 RoPE 旋转位置编码，女朋友睁大了双眼（上）</a></p><p><a href="https://mp.weixin.qq.com/s/pH6MSDyllZREq5v-Bevrqg">RoPE 旋转位置编码，详细解释（下）NLP 面试的女生彻底说明白了</a></p><p><a href="https://mp.weixin.qq.com/s/lYXNXvQHWtm7faVJvUKOew">初中生能看懂的绝对位置编码和旋转位置编码（RoPE），甚至会认表的小学生也行</a></p><h1 id="2-Transformers-中的-Layer-Norm-可以并行么？"><a href="#2-Transformers-中的-Layer-Norm-可以并行么？" class="headerlink" title="2. Transformers 中的 Layer Norm 可以并行么？"></a>2. Transformers 中的 Layer Norm 可以并行么？</h1><p><a href="https://mp.weixin.qq.com/s/mr2t3w6-sb7cEgNnxoPXVQ">NLP 面试八股：“Transformers 中的 Layer Norm 可以并行么？” 拿到 offer 的女同学这样回答</a></p><h1 id="3-大模型常用的-Normalization-都有什么？"><a href="#3-大模型常用的-Normalization-都有什么？" class="headerlink" title="3. 大模型常用的 Normalization 都有什么？"></a>3. 大模型常用的 Normalization 都有什么？</h1><ul><li>LayerNorm</li><li>RMSNorm</li><li>DeepNorm</li><li>BatchNorm</li></ul><p><a href="https://mp.weixin.qq.com/s/pvXuWi3F4UJj7JkGuzHlRA">NLP面试官：“大模型常用的 Normalization 都有什么？ ” 算法女生表示易如反掌</a></p><h1 id="4-Transformers-中为什么使用-Layer-Norm"><a href="#4-Transformers-中为什么使用-Layer-Norm" class="headerlink" title="4. Transformers 中为什么使用 Layer Norm"></a>4. Transformers 中为什么使用 Layer Norm</h1><h2 id="为什么使用-Layer-Norm"><a href="#为什么使用-Layer-Norm" class="headerlink" title="为什么使用 Layer Norm"></a>为什么使用 Layer Norm</h2><p>不仅仅是使用 Layer Norm，各种 Normalize 的操作，首先是<strong>为了保证训练的稳定性。</strong>因为当神经网络很深的时候，反向传播的参数计算往往都是指数级的变化，太大或者太小的数值送入激活函数后就容易造成梯度消失或者梯度爆炸，训练就挂了。</p><p>其次是<strong>加速模型的收敛</strong>。这个也比较容易理解，模型的每一层都在拟合一个数据分布，而如果不进行 normalize，那么每次的输入分布可能随时都在变化，这样学习起来就很困难。在 normalize 之后，绝大部分数值都集中在了一个可接受的范围内，每一层的参数就安心拟合这个分布就好了，这个范围正好又是激活函数的“舒适区”，所以模型收敛速度会更快。</p><p>还有一个额外的好处就是<strong>让模型的训练不再那么依赖权重的初始化</strong>，早期的时候初始化对模型结果的影响还是蛮大的，也是个很火热的研究方向。就包括现在也有很多研究，比如 torch.manual_seed(3407) is all you need</p><h2 id="为什么不用-Batch-Norm"><a href="#为什么不用-Batch-Norm" class="headerlink" title="为什么不用 Batch Norm*"></a>为什么不用 Batch Norm*</h2><p>其实在 Transformers 论文刚出的那个时间点，就是两种 Normalize 比较流行，早一点提出的是 Batch Norm, 后来是 Layer Norm, 至于再后来的 Instance Norm 和 Group Norm 等都可以认为是这两种基础上的扩展。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739341227340-72.png" alt="img"></p><p>当时 Batch Norm 在 CV 领域比较流行，而 NLP 则使用 Layer Norm 比较多。但也并不是一定要按照任务这么划分。</p><p>当时为什么 CV 都使用 Batch Norm 呢？我个人觉得是因为站在 CNN 卷积核的计算方式上看，在 Batch 上进行 Normalize 是比较契合的，因为这样每个卷积核在计算的时候数据的 Normalize 的方式是一样的。</p><p>但是 Batch Norm 也有些问题，下面简单说几点：</p><ol><li><strong>Batch 需要大</strong>，小的 Batch 训练不稳定。因为 Batch Norm 需要跨样本的 Normalize，所以采样要足够大才能捕捉到样本的分布。</li><li>加大 Batch 有一些副作用。一个最明显的问题，那就是现在模型越来越大，如果想实现多机多卡的 GPU 并行，<strong>那 Batch Norm 需要额外的通信</strong>，因为一个 Batch 很可能分布在不同的机器上，而 Normalize 又需要计算整个样本的数据分布才行。现在大概有两种解法，一种是类似 mini batch，放弃跨机器通信；一种是 Pytorch 实现的 SyncBatchNorm，在前面的基础上尽量减少通信的数据。但是不管怎么样，额外的通信开销在模型足够大的时候也是个问题。</li><li><strong>训练和预测的不一致性</strong>。训练的时候有大批的数据可以组成 Batch，但是预测的时候，我如果只想预测一个样本，那 Batch Norm 就废了。所以在预测的时候实际上是采用了训练时候的数据分布来进行 Normalize 的。这样就必须要保证训练和预测的分布必须一致，泛化能力没那么强。</li><li>并不适合当时的 NLP 主流框架比如 RNN。</li><li><strong>并不太适合长度不固定的 NLP 序列</strong>。因为每个 Batch 最后总有些 pad，这些 pad 会干扰 Batch Norm 的数据分布。如果把 pad 都不参与计算，那就相当于 batch 越来越小，根据第一条也不太好。</li></ol><p>关于数据分布还有 Batch 的讨论，可以看下论文 Facebook 的《Rethinking “Batch” in BatchNorm》，其中还提到了 Batch 内的信息泄漏等问题，感兴趣的可以阅读一下原始的论文。</p><p>所以说 Layer Norm 改成了按 特征 来 Normalize，可以很好的处理小 Batch 和变长输入的问题，也没有额外的通信，可以说基本解决了上面的问题。</p><p>所以后来 Layer Norm 越来越流行， Batch Norm 反而不太用了，尤其是大模型时代一直在追求更高的训练效率， Batch Norm 的额外通信就有些不合时宜了。当然现在大模型也有其他的 Normalize 方法，比如 RMSNorm、DeepNorm 等。</p><h2 id="实验论证1：Transformers-使用-Batch-Norm-效果并不太好"><a href="#实验论证1：Transformers-使用-Batch-Norm-效果并不太好" class="headerlink" title="实验论证1：Transformers 使用 Batch Norm 效果并不太好"></a>实验论证1：Transformers 使用 Batch Norm 效果并不太好</h2><p>上面说了一些 Batch Norm 和 Layer Norm 的对比，有人可能会说你这都是理论上的，有证据么？还真有，2020年的一篇论文专门测试了把 Transformers 中的 Layer Norm 变成了 Batch Norm，打了个擂台。论文的名字是《PowerNorm: Rethinking Batch Normalization in Transformers》</p><p>这篇论文中，作者发现 Transformers 中的 LayerNorm 换成 Batch Norm 后，在分类和机器翻译的任务上性能下降明显。如下图</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739341171523-57.png" alt="img"></p><p>可以看出，Batch Norm 效果并不太好。分析其原因呢，作者指出采用 Batch Norm 的 Transformers， 其 Batch Norm 的均值和方差震荡明显，并不稳定，所以收敛的就很慢。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739341181726-60.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739341188848-63.png" alt="img"></p><p>当然作者后来对 BN 进行了改进，提出了 PowerNorm, 感兴趣的可以看看。</p><h2 id="实验论证2：Layer-Norm-会改善-Transformers-的注意力"><a href="#实验论证2：Layer-Norm-会改善-Transformers-的注意力" class="headerlink" title="实验论证2：Layer Norm 会改善 Transformers 的注意力"></a><strong>实验论证2：Layer Norm 会改善 Transformers 的注意力</strong></h2><p>来自2023年的论文：《On the Expressivity Role of LayerNorm in Transformers’ Attention》</p><p>这篇文章在<strong>较小的 Transformers 模型</strong>上做了实验，发现 Layer Norm 为 Attention 提供了两个功能</p><ol><li>Projection：会将输入投影到 query 和 key 正交的超平面，这样方便所有的 key 可以同等访问。</li><li>Scaling：每一个 Key 都能被选中，都有机会获得最高分。</li></ol><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739341197081-66.png" alt="img"></p><p>Layer Norm 的存在可以让 Attention 不用自己去学习这两点。<strong>但是随着模型规模的增大，这个辅助作用是被削弱的</strong>，也就是模型能自己学会这两点。但是仍旧从一个比较有意思的角度阐述了 Layer Norm 的作用。</p><h2 id="实验论证3-Layer-Norm-在图像中表现怎么样？"><a href="#实验论证3-Layer-Norm-在图像中表现怎么样？" class="headerlink" title="实验论证3: Layer Norm 在图像中表现怎么样？"></a><strong>实验论证3: Layer Norm 在图像中表现怎么样？</strong></h2><p>2018年，何凯明提出的 《Group Normalization》对 ResNet 中使用不同的 Normalize 方式进行了对比，可以看出 Layer Norm 是不如 Batch Norm 的</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739341208959-69.png" alt="img"></p><p>在 Transformers 大火后，CV 领域也开始使用 Transformers，Vit 中使用的就是 Layer Norm.</p><p>2022年的一篇文章，也是何凯明之前所在的 FAIR 小组，发表了论文《A ConvNet for the 2020s》，论文里表示，在借鉴了一些 Vision Transformers 的思想对 ConvNet 修改后，使用 LayerNorm 效果比 Batch Norm 还要好一点。</p><p><a href="https://mp.weixin.qq.com/s/qbRaF2ilz0fT-Mi0GokAmQ">NLP面试官：“Transformers 中为什么使用 Layer Norm ” 算法女生这么回答轻松拿下</a></p><h1 id="5-Transformers-中的-Position-Embedding-的作用"><a href="#5-Transformers-中的-Position-Embedding-的作用" class="headerlink" title="5. Transformers 中的 Position Embedding 的作用"></a>5. Transformers 中的 Position Embedding 的作用</h1><p><a href="https://mp.weixin.qq.com/s/TsSxAkab1uWyePLNEZoHHw">NLP面试官：“Transformers 中的 Position Embedding 的作用” 算法女生这么回答就很赞</a></p><h1 id="6-Transformers-中-FFN-的作用"><a href="#6-Transformers-中-FFN-的作用" class="headerlink" title="6. Transformers 中 FFN 的作用"></a>6. Transformers 中 FFN 的作用</h1><p><a href="https://mp.weixin.qq.com/s/dLTHD05wezA5pAM2c4j07Q">NLP面试官：“Transformers 中 FFN 的作用” 算法女生这么回答下午就想安排入职</a></p><h1 id="7-Attention为什么要除以根号d"><a href="#7-Attention为什么要除以根号d" class="headerlink" title="7. Attention为什么要除以根号d"></a>7. Attention为什么要除以根号d</h1><p><a href="https://mp.weixin.qq.com/s/3o0NgpFPKS1RNICNuMuygg">NLP面试官：“Attention为什么要除以根号d” 算法女生这么回答当场想发 offer</a></p><h1 id="8-如何根据模型参数量估计需要的显存？"><a href="#8-如何根据模型参数量估计需要的显存？" class="headerlink" title="8. 如何根据模型参数量估计需要的显存？"></a>8. 如何根据模型参数量估计需要的显存？</h1><p><a href="https://mp.weixin.qq.com/s/edOUscAzAPqqLUSh_H_KJQ">如何根据模型参数量估计需要的显存？</a></p><h1 id="9-如何让大模型处理更长的文本？"><a href="#9-如何让大模型处理更长的文本？" class="headerlink" title="9. 如何让大模型处理更长的文本？"></a>9. 如何让大模型处理更长的文本？</h1><p><a href="https://mp.weixin.qq.com/s/Flm8cO_RQvVnsl9noN-87g">如何让大模型处理更长的文本？</a></p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>笔试面试</category>
      
      <category>AI算法</category>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>笔试面试</tag>
      
      <tag>算法面试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>公众号“看图学”试题合集（4）</title>
    <link href="/2025/02/12/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/AI%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98004%EF%BC%9A%E5%85%AC%E4%BC%97%E5%8F%B7%E2%80%9C%E7%9C%8B%E5%9B%BE%E5%AD%A6%E2%80%9D%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86%EF%BC%884%EF%BC%89/"/>
    <url>/2025/02/12/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/AI%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98004%EF%BC%9A%E5%85%AC%E4%BC%97%E5%8F%B7%E2%80%9C%E7%9C%8B%E5%9B%BE%E5%AD%A6%E2%80%9D%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86%EF%BC%884%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="1-为什么-output-token-的价格比-input-token-更贵？"><a href="#1-为什么-output-token-的价格比-input-token-更贵？" class="headerlink" title="1. 为什么 output token 的价格比 input token 更贵？"></a>1. 为什么 output token 的价格比 input token 更贵？</h1><p>翻一翻各大厂家的 API 定价，会发现基本上 输出 token 的价格是输入 token 价格的好几倍。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739327904870-28.png" alt="img"></p><p>首先从计算量的角度来看，<strong>对于输入的 D 个 token，和输出 D 个token来说，FLOPs 都大约是 2ND</strong>，其中 N 为参数量。至于为什么 FLOPs 为什么是 2ND， 可以看这篇：<a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247486124&idx=1&sn=c9294e6696047bb616e0f640fca3ff5f&chksm=fa677475cd10fd63f99450f3d6d1287e79a38a3a99eeee28515e84c0e982d745e7d51a4ac65f&scene=21#wechat_redirect">学妹问：“反向传播的计算量是前向传播计算量的几倍？”</a>。</p><p><strong>从内存角度来看</strong>，输入 token 和 QKV 等矩阵大小和 输出 token 的<strong>也差不太多</strong>，只不过输出 token 采用 KV Cache 的形式。</p><p>既然计算量和内存占用都差不多，从资源的角度来讲，成本是差不多的。那最终成本究竟差在哪里呢？</p><p>其实是<strong>差在资源的利用率上。</strong></p><p>要知道 GPU 在运算的时候，既有计算，又有数据通信。这就存在一个最佳的 ops:bytes ratio。</p><p>通俗点来讲，就是每读取一份数据 (比如一个 FP16&#x2F;BF16)， 应该执行多少 FLOPs。</p><p>如果算的比读的快，那通信就是瓶颈，因为这个时候 GPU 的 SM 在等待。</p><p>如果算的比读的慢，那计算就是瓶颈，这个时候通信需要等待。</p><p><strong>大模型训练，大多数情况下，通信是瓶颈</strong>，所以都是算的快，读的慢。这样计算下来，整体的 MFU (Model FLOPs utilization) 很难打满。</p><p>经过这么长时间的优化，目前大模型的训练 MFU 在50-60% 就已经很厉害了。</p><p>回到题目，对于 D 个输入 token 来说，模型只需要执行一个 forward 计算，可以充分的并行，整个计算过程的利用率能接近训练的最高水平。</p><p>然而对于 输出 token 来说，必须是一个 token 一个 token 的生成，对于 D 个输出 token 来说，需要执行 D 次 forward 操作。本来通信就是瓶颈，现在 D 次 forward 的额外通信更是雪上加霜。虽然现在也有batch 上，还有动态填充等优化，但是 <strong>GPU 利用率上来说，输出是远低于输入的</strong>。</p><p>假如你是老板，招聘了两个员工，他们干的活是一样多的，消耗资源成本也都一样。一个完成一件事情需要1个小时，一个完成一件事情需要1天，你该怎么发工资呢？</p><p><a href="https://mp.weixin.qq.com/s/Kgh5kIeaYaADwS6waxDcNw">阿里大模型面试原题：为什么 output token 的价格比 input token 更贵？</a></p><h1 id="2-Flash-Attention-的数学原理"><a href="#2-Flash-Attention-的数学原理" class="headerlink" title="2. Flash Attention 的数学原理"></a>2. Flash Attention 的数学原理</h1><p><a href="https://mp.weixin.qq.com/s/Nv9iS96J7pVZRvH7U8fWsA">字节Transformers二面原题：Flash Attention 的数学原理</a></p><h1 id="3-Softmax-如何并行？"><a href="#3-Softmax-如何并行？" class="headerlink" title="3. Softmax 如何并行？"></a>3. Softmax 如何并行？</h1><h3 id="Softmax-计算公式"><a href="#Softmax-计算公式" class="headerlink" title="Softmax 计算公式"></a>Softmax 计算公式</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250212104019319.png" alt="Softmax"></p><h3 id="安全的-Softmax-运算"><a href="#安全的-Softmax-运算" class="headerlink" title="安全的 Softmax 运算"></a>安全的 Softmax 运算</h3><p>softmax 有个问题，那就是很容易溢出。比如采用半精度，由于float16的最大值为65504，所以只要 $x\geq11$ 那么softmax就溢出了。即使是float32，x也不能超过88。</p><p>好在 exp 有这么一个性质，那就是 $e^{x-y}&#x3D;\frac{e^x}{e^y}.$</p><p>根据这个性质，可以在分子分母上同时除以一个数，这样可以将 x 的范围都挪到非正实数域。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250212104911344.png" alt="image-20250212104911344"></p><p>其中$m&#x3D;max({x_1,x_2,\ldots,x_N})$</p><p>这样，就可以保证计算 softmax 时的数值稳定性。</p><p>这个算法可以分成三次迭代来执行。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250212105116467.png" alt="image-20250212105116467"></p><p>分析上面的步骤，可以发现，如果是不做任何优化的话，至少要进行和 GPU 进行6次通信（3次写入，3次写出）。</p><p>如果对每一步的for 循环进行一些并行切分的的话，还要加上 reduce_sum 和 reduce_max 之类的通信成本。</p><p>是否能将某些操作进行融合，减少通信呢？按照之前 layernorm 并行的经验，我们需要寻找一个 Online Algorithm。</p><h3 id="Online-Softmax"><a href="#Online-Softmax" class="headerlink" title="Online Softmax"></a>Online Softmax</h3><p>2018年 Nvidia 提出了《Online normalizer calculation for softmax》</p><p>既然是 Online 的算法，我们需要找出递归的表达式。</p><p>对于第二步中的 $d_i&#x3D;d_{i-1}+e^{x_i-m_N}$， 我们期望去掉这个式子对 $m_N$ 的依赖。</p><p>设$d_i^{\prime}&#x3D;\sum_j^ie^{x_j-m_i}$  ,注意，这里减去的全局最大值变成了当前最大值。这个式子有如下的性质</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250212105414927.png" alt="image-20250212105414927"></p><p>这里可以发现，$d_{i}^{\prime}$ 的计算，依赖于 $d_{i-1’}^{\prime}m_{i,}m_{i-1’}$这样，就可以将前两步合并到一起。上面的3步可以变成2步。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250212111819458.png" alt="image-20250212111819458"></p><p>还能不能进一步融合算子呢？没办法了，因为第二步的分母依赖于第一步的计算。</p><p>但是可以借助 GPU 的 share memory 来存储中间结果，将上面的两步只用一个 kernel 实现，这样就只需要与 global memory 通信两次，一次写入数据，一次读取结果。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>整体来说，有两个重要的优化点：</p><ol><li>将前两步的算子融合，减少 Reduce_max 和 Reduce_sum 之类的通信成本。</li><li>借助 share memory 存储中间结果，减少与 global memory 的通信成本。</li></ol><p>这一篇只是从数学上给出了一些 Softmax 的并行理论基础。具体实现还有很多细节上的优化点，比如：</p><p>感兴趣的可以看看 oneflow 的一个 softmax 深度优化：<a href="https://www.oneflow.org/a/share/jishuboke/54.html">https://www.oneflow.org/a/share/jishuboke/54.html</a> . 源代码在<a href="https://github.com/Oneflow-Inc/oneflow/blob/master/oneflow/core/cuda/softmax.cuh">https://github.com/Oneflow-Inc/oneflow/blob/master/oneflow/core/cuda/softmax.cuh</a></p><p>还有 Nvidia 自己实现的一个可读性很好的版本：<a href="https://github.com/NVIDIA/FasterTransformer/blob/release/v1.0_tag/fastertransformer/cuda/open_attention.cu#L189-L268">https://github.com/NVIDIA/FasterTransformer/blob/release/v1.0_tag/fastertransformer/cuda/open_attention.cu#L189-L268</a> 但是速度没有 oneflow 的好。</p><p><a href="https://mp.weixin.qq.com/s/5MbYt_795Iyel5y73ntdAQ">字节 Transformers 面试原题：Softmax 如何并行？</a></p><h1 id="4-权重共享是怎么回事？"><a href="#4-权重共享是怎么回事？" class="headerlink" title="4. 权重共享是怎么回事？"></a>4. 权重共享是怎么回事？</h1><p><strong>权重共享（****Weight Sharing</strong> <strong>）<strong><strong>就是在模型的不同部分使用相同参数的技术。</strong></strong>有时候也叫</strong> <strong>Parameter sharing</strong><strong>, 或者</strong> <strong>Weight Tying。</strong></p><p>权重共享其实可以算作是老技术了，我们可能不知不觉的就在使用，比如 CNN 网络和 Transformers 中的 FNN 层。</p><p>CNN 中，同一个卷积核的权重在整个图像的不同位置是共享的。而 Transformers 中的 FNN 层也类似，序列的不同位置共享 MLP。</p><p><strong>权重共享有好处：</strong></p><ol><li>可学习的参数变少了，模型更容易训练，收敛更快了。</li><li>在内存受限、需要通信的推理场景，能够加快推理速度。</li><li>权重共享有一定的正则化的作用，因为可学习的参数变少了。所以相当于提升了模型的泛化能力。这个理论可以从 Bias Variance Trade-off 的角度来证明，可以看一篇旧文：<a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247483714&idx=1&sn=364707eb2483be5357421da1915975f1&chksm=fa677f9bcd10f68d46a9f075c12f80771fce2aecefc4952fdbe7810f1d7b49a71b822c573355&scene=21#wechat_redirect">码农要术：机器学习篇：泛化挑战</a></li></ol><p><strong>权重共享的坏处：</strong></p><p>强扭的瓜不一定甜。一些权重可能理论上在完成任务的功能上就是正交的，共享之后可能会有新的问题。但是这些坏处要具体情况具体分析。比如 词向量与 LM Head 的共享可能导致各向异性的问题。</p><p>下面列举一下神经网络中常见的一些权重共享。</p><h2 id="基于位置的共享"><a href="#基于位置的共享" class="headerlink" title="基于位置的共享"></a>基于位置的共享</h2><p>比如 CNN 和  Transformers 中的 FNN 层。这个不再赘述。</p><h2 id="嵌入层和输出层权重共享"><a href="#嵌入层和输出层权重共享" class="headerlink" title="嵌入层和输出层权重共享"></a>嵌入层和输出层权重共享</h2><p>这个就是常说的 Weight Tying, 将词向量与最后的 lm head 的权重进行共享。具体可以见：<a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247486000&idx=1&sn=74abbd3c3306652d86319c0239d8b644&chksm=fa6774e9cd10fdff2f404f54335b8f886cfa495390843ce7f7a16b16c8e86c02ddb1d8273739&scene=21#wechat_redirect">阿里面试官：“Transformers 中的 Weight Tying 是什么?”</a>，里面好处坏处，还有一些分析写的比较清楚。</p><h2 id="MQA-和-GQA"><a href="#MQA-和-GQA" class="headerlink" title="MQA 和 GQA"></a>MQA 和 GQA</h2><p>MQA 就是 Attention 共享同一个 K 和 V。</p><p>GQA 觉得这样是不是有点太暴力，于是选择了 MQA(共享一个) 和 MHA（完全不共享） 的一个中间形态，按组来共享。如下图所示：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426444171-6.png" alt="img"></p><p>目前使用 GQA 的越来越多了，比如 Llama2，deepseek，yi等。</p><h2 id="Layer-Sharing-（或者-Block-Sharing"><a href="#Layer-Sharing-（或者-Block-Sharing" class="headerlink" title="Layer Sharing （或者 Block Sharing)"></a>Layer Sharing （或者 Block Sharing)</h2><p>将 Transformers 中的 block 进行共享。</p><p>早在 BERT 时代，就有人提出了 AlBERT 模型，就是把 Transformers 堆叠的层都用一个来表示。</p><p>后来又有 Universal Transformers 和 PonderNet 等模型也做了类似的尝试，但不只是共享那么简单了。</p><p>田渊栋团队最新的论文 MobileLLM 中也探讨了 Block 间不同的共享方式。 </p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426429461-3.png" alt="img"></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>上面列的都是主流的共享方法。也有很多其他的尝试，但是目前用的不多。</p><p>比如 Sharing in branches, Sharing in branches, 还有借助其他方法辅助的。</p><p>感兴趣的可以看：</p><ul><li>《Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers》</li><li>《Understanding Parameter Sharing in Transformers》</li><li>《Pea-KD: Parameter-efficient and accurate Knowledge Distillation on BERT.》</li><li>《LightFormer: Light-weight Transformer Using SVD-based Weight Transfer and Parameter Sharing. 》</li></ul><p>最后，大家可以思考一下，权重共享的模块出现在模型的不同位置时，如何进行梯度的反向传播？</p><p><a href="https://mp.weixin.qq.com/s/efYCi2UFZSfEew9I677O1w">腾讯面试官：“权重共享是怎么回事？” 封面依然眼熟</a></p><h1 id="5-Tokenization-是什么"><a href="#5-Tokenization-是什么" class="headerlink" title="5. Tokenization 是什么"></a>5. Tokenization 是什么</h1><p><a href="https://mp.weixin.qq.com/s/0ewnWvf8sQflmamcXpcUfQ">小米面试官：“Tokenization 是什么”。封面看着眼熟</a></p><h1 id="6-什么是张量并行-Tensor-Parallelism-？"><a href="#6-什么是张量并行-Tensor-Parallelism-？" class="headerlink" title="6. 什么是张量并行(Tensor Parallelism) ？"></a>6. 什么是张量并行(Tensor Parallelism) ？</h1><p>张量并行（Tensor Parallelism） 是一种分布式矩阵算法。</p><p>随着模型越来越大，模型内的矩阵也越来越大。一个大矩阵的乘法可以拆分成多个小矩阵的运算，这个些运算就可以充分利用 GPU 的多核还有多 GPU 来进行分布式计算，从而提高运算速度。</p><p>Megatron-LM 提出了 1D Tensor Parallelism， 也就是两个矩阵之间的分布式计算方法。后面陆续又有 2D、2.5D、3D Tensor Parallelism。</p><p>本文先讲一下 Megatron-LM 的 1D Tensor Parallelism 算法。</p><h2 id="1D-Tensor-Parallelism"><a href="#1D-Tensor-Parallelism" class="headerlink" title="1D Tensor Parallelism"></a>1D Tensor Parallelism</h2><p>其实 1D Tensor Parallelism 的算法完全来源于矩阵运算的性质。如下图所示：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739331028467-30.png" alt="img"></p><p><strong>切分方法1</strong></p><p>假设两个矩阵相乘，左矩阵按列分割成两个，右矩阵按行分割成2个，那么有如下性质：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250212113054552.png" alt="image-20250212113054552"></p><p>证明如下：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/32105a41870c59a214433219327d96c6.png" alt="32105a41870c59a214433219327d96c6"></p><p>这样的切分方法需要一个 Reduce 操作，因为要把各部分的结果求和得到最终结果。</p><h3 id="切分方法2"><a href="#切分方法2" class="headerlink" title="切分方法2"></a><strong>切分方法2</strong></h3><p>假设两个矩阵相乘，左矩阵按行分割成两个，右矩阵按列分割成2个，那么有如下性质：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250212113141977.png" alt="image-20250212113141977"></p><p>证明如下：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/5a7470c5185d7e1804e4bc981b452c55.png" alt="5a7470c5185d7e1804e4bc981b452c55"></p><p>这样的切分方法最终需要把结果 Concat 起来。但是由于每一部分的计算结果都是最终结果的一部分，所以可以不着急 Reduce 结果，可以直接作为下一次并行计算的输入。</p><h3 id="两种切分方法组合"><a href="#两种切分方法组合" class="headerlink" title="两种切分方法组合"></a><strong>两种切分方法组合</strong></h3><p>假设我们有多个矩阵进行相乘，比如 $\boldsymbol{A}\cdot\boldsymbol{B}\boldsymbol{\cdot}\boldsymbol{C}\boldsymbol{\ldots}\boldsymbol{X}$， 相邻之间的矩阵可以一个横切，一个纵切，然后放到不同的 device 上。从而达到并行计算的目的。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/6ab3f1df53fb68e9eb614398214b0b5c.png" alt="6ab3f1df53fb68e9eb614398214b0b5c"></p><p>分割成多个也是类似的结论。所以对于矩阵相乘来说，如果有 N 个 GPU，完全可以将参数平分到 N 个GPU上，每个 GPU 只负责计算 $\frac{1}{N}$ 的参数，而不用都塞到一个里面，显存也吃不消。</p><h2 id="FFN-的-Tensor-Parallelism"><a href="#FFN-的-Tensor-Parallelism" class="headerlink" title="FFN 的 Tensor Parallelism"></a><strong>FFN 的 Tensor Parallelism</strong></h2><p>来看一个具体的案例。Transformers 的 FFN 层涉及两次矩阵乘法。<br>$$<br>\mathrm{FFN}(X)&#x3D;g(X\cdot W_1)W_2<br>$$<br>其中g是激活函数 Gelu。激活函数的非线性导致：<br>$$<br>Gelu(X+Y)\neq Gelu(X)+Gelu(Y)<br>$$<br>由于有这个激活函数的存在，我们最好按照切分方法2 来进行。因为如果采用第一种，那么需要先进行 Reduce 之后才能执行 Gelu 操作。然后再拆分，再 Reduce。这里有2步 Reduce 操作。</p><p>如果采用第二种，则仅需要最后一步进行 Reduce 即可，少了中间的 Reduce 再拆分的工作。如下图所示：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/afda72feb2634f34c110484312f1a749.png" alt="afda72feb2634f34c110484312f1a749"></p><p>当然这里并没有对 X 进行拆分，仅拆分了$W_1$  和 $W_2$。要拆分也是可以的。</p><p><a href="https://mp.weixin.qq.com/s/XQLbTU3dX29nBjdkWOrSqg">京东面试官：“ 什么是张量并行(Tensor Parallelism) ？”</a></p><h1 id="7-Transformers-中的-weight-tying-是什么"><a href="#7-Transformers-中的-weight-tying-是什么" class="headerlink" title="7. Transformers 中的 weight tying 是什么?"></a>7. Transformers 中的 weight tying 是什么?</h1><p>Transformers 的输入会从一个词向量矩阵中获取对应 token 的词向量，这个词向量矩阵的大小为 (vocab_size, hidden_size)。</p><p>在预测一个词的输出概率时，transformer 有个预测头(prediction head), 这个预测头是 Transformers 的最后一层，大小为 (hidden_size, vocab_size)，可能还有一个 bias。</p><p>如果预测头没有bias的话，这两个矩阵的大小是一样的，如果这两个矩阵使用同一个矩阵，就被称作 weight typing。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426468188-8.jpeg" alt="img"></p><p>这项技术是由两拨人独立提出的，一波人是Ofir Press, Lior Wolf 发表了《Using the Output Embedding to Improve Language Models》，一波是Hakan Inan, Khashayar Khosravi, Richard Socher 提出的《Using the Output Embedding to Improve Language Models》。</p><p>这里面比较出名的是 Richard Socher，创办了you.com</p><p>下面简单回顾一下这两篇论文关于 Weight Typing 的部分。</p><h3 id="Using-the-Output-Embedding-to-Improve-Language-Models"><a href="#Using-the-Output-Embedding-to-Improve-Language-Models" class="headerlink" title="Using the Output Embedding to Improve Language Models"></a><strong>Using the Output Embedding to Improve Language Models</strong></h3><p>这篇文章的出发点是基于词向量和预测头的功能考虑的。作者认为，词向量最终应该满足这样一个条件，那就是相似词的词向量应该也相似（在向量空间中的距离应该更近）。而预测头需要参与 softmax 去预测某一个词，我们期望两个同义词互相交换位置后，得分应该也差不多，这也就要求相似的词在预测头中对应的向量也应该相似才行。</p><p>基于这一点的考虑，作者认为词向量和预测头可以共享权重。然后做了一些实验，证明出了结构极其简单的 word2vec，其他的稍微复杂一点的模型，weight tying 之后效果都变好了。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426477981-11.png" alt="img"></p><h3 id="Tying-Word-Vectors-and-Word-Classifiers-A-Loss-Framework-for-Language-Modeling"><a href="#Tying-Word-Vectors-and-Word-Classifiers-A-Loss-Framework-for-Language-Modeling" class="headerlink" title="Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"></a><strong>Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling</strong></h3><p>这篇文章提了个新的loss，在这个loss下，从数学上证明了词向量和预测头这两个矩阵的相似性。具体证明有点繁琐，感兴趣的可以看看原文。</p><h2 id="Weight-Tying-的好处"><a href="#Weight-Tying-的好处" class="headerlink" title="Weight Tying 的好处"></a>Weight Tying 的好处</h2><p><strong>最明显的好处就是降低了模型参数。</strong></p><p>在词表不大的时候并没有什么感觉，但是词表越大，词向量占参数的比例就越大。</p><p>比如 llama2 有 32000 个 token ，参数量为 32000 * 4096 &#x3D; 131072000 个，整体参数量为 6738415616， 占比 1.95%.</p><p>llama3 有 151936 个 to，参数量为 151936 * 4096 &#x3D; 622329856 个，整体参数量为 8030261248， 占比 7.75%.</p><p>然而 llama3 并没有使用 Weight Tying， 如果使用的话，参数量会缩减 7.75%，后面会看到，一些词表更大的模型，都用了 Weight Tying。</p><h3 id="加速模型收敛"><a href="#加速模型收敛" class="headerlink" title="加速模型收敛"></a><strong>加速模型收敛</strong></h3><p>模型参数变小，自然收敛更快。但是从原理上还有另外一层解释。</p><p>如果没有 weight tying， 词向量矩阵只会更新自己见过的 token。但是当使用 weight tying 后，<strong>所有的 token 的词向量都会更新，即使没见到的 token，模型也会分配合适的概率。</strong></p><p>这个问题在 BERT 之类的 Encoder-only 的模型中更为显著，因为每个样本只会预测15%左右的词汇，而不是像 Decoder 那样所有的 token 都会更新。<strong>所以 Encoder-only 的模型更喜欢使用 Weight Tying</strong></p><h2 id="Weight-Tying-的坏处"><a href="#Weight-Tying-的坏处" class="headerlink" title="Weight Tying 的坏处"></a>Weight Tying 的坏处</h2><p>Weight Tying 也不只是有好处，也有坏处。从根本上说，预测头和词向量所肩负的任务是完全不一样的，强扭的瓜不一定甜。</p><p>在论文《Improving Low Compute Language Modeling with In-Domain Embedding Initialisation》 也提到，在一些领域内的低词频的词汇得到充分的训练后，Weight Tying 并没有像 Press &amp; Wolf 那样改善模型的性能。所以更多的语料会削弱 Weight Tying 的效果。</p><p>还有在论文《Representation Degeneration Problem in Training Natural Language Generation Models》中提到，<strong>使用 weight tying 会导致各向异性问题</strong>。关于各向异性可以见 <a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247484639&idx=1&sn=8ed482b29c8c25150aa6d83a38b7014c&chksm=fa677a06cd10f310ac066fc0a095cda55c3084ef46b73451a5aec9c6b40a38fa3b10d163395a&scene=21#wechat_redirect">NLP名词解释：各向异性(Anisotropic)</a></p><h2 id="常用模型使用-Weight-Tying-的情况"><a href="#常用模型使用-Weight-Tying-的情况" class="headerlink" title="常用模型使用 Weight Tying 的情况"></a>常用模型使用 Weight Tying 的情况</h2><ul><li>Gemma: True</li><li>qwen: False</li><li>llama : False</li><li>deepseek:</li><li>yi: False</li><li>glm2&#x2F;4: false</li><li>glm1: True</li><li>command R: True</li><li>mistral: False</li></ul><p>可以看出， Gemma 和 command R 由于词表确实很大（256000 个），使用了 Weight Tying，其他的词表在 100k+ 的搜没有采用 Weight Tying</p><h1 id="8-给一个网络结构，如何计算模型的参数量？"><a href="#8-给一个网络结构，如何计算模型的参数量？" class="headerlink" title="8. 给一个网络结构，如何计算模型的参数量？"></a>8. 给一个网络结构，如何计算模型的参数量？</h1><p>这个题目其实就是问细节，看看对一些网络结构熟不熟悉。</p><p>知道了常用的网络结构的计算方法，然后就是加法和乘法了。</p><p>一些常见的网络结构列举如下：</p><ul><li><p><strong>Linear</strong></p><p>就是个矩阵，有时候会加上个 bias。一个 Linear(in_features&#x3D;w, out_features&#x3D;h, bias&#x3D;True) 的参数量为：w * h + h, 如果 bias &#x3D; False, 则 为 w * h</p></li><li><p><strong>Embedding</strong></p><p>可以认为是一个没有 bias 的 Linear。</p></li><li><p><strong>Norm</strong></p><p>关于 Norm 的原理和涉及的参数，可以看：<a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247485766&idx=1&sn=b84e23c78ddee76afdab8c6710bfa25b&chksm=fa67779fcd10fe89edffd0295f2226159a39469c1951227950f34b352d51abea078cefd21464&scene=21#wechat_redirect">NLP面试官：“大模型常用的 Normalization 都有什么？” 算法女生表示易如反掌</a></p><ul><li>Layer Norm 里面有两个可训练参数, $\gamma$ 和$\beta$ , 假设hidden_size 的大小为 h, hidden_size 的每一维都有两个，所以是 $h\times2$ 个</li><li>RMSNorm 每一维则只有一个可训练参数$\gamma$ , 所以有 h 个</li></ul></li><li><p><strong>Active</strong> 和 <strong>Dropout</strong></p><ul><li>没有可训练参数</li></ul></li></ul><h2 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h2><p>知道了上面的参数数量，剩下的就是小学数学题了。以 llama3 和 qwen2 来举例：</p><h3 id="meta-llama-Meta-Llama-3-8B"><a href="#meta-llama-Meta-Llama-3-8B" class="headerlink" title="meta-llama&#x2F;Meta-Llama-3-8B"></a><strong>meta-llama&#x2F;Meta-Llama-3-8B</strong></h3><p>可以从论文里查找参数量，但是最方便的还是打印一下模型的结构，代码如下</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> transformers import AutoModelForCausalLM<br>model_llama3 = AutoModelForCausalLM.from_pretrained(<br>    model_path, <span class="hljs-attribute">tie_word_embeddings</span>=<span class="hljs-literal">False</span>, <br>    <span class="hljs-attribute">token</span>=access_token)<br><span class="hljs-built_in">print</span>(model_llama3)<br></code></pre></td></tr></table></figure><p>结构如下:</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739331710316-37.png" alt="img"></p><p>然后挨个计算就可以了：128256 * 4096 + 32 * (4096 * 4096 * 2 + 4096 * 1024 * 2 + 4096 * 14336 * 3 + 2 * 4096) + 4096 + 128256 * 4096 &#x3D; 8030261248.</p><p>所以 llama3 8B 的真实参数量为：8030261248</p><h3 id="Qwen-2-7B"><a href="#Qwen-2-7B" class="headerlink" title="Qwen 2 7B"></a><strong>Qwen 2 7B</strong></h3><p>如法炮制</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">model_qwen_lm = AutoModelForCausalLM.from_pretrained(<br>    model_qwen, <span class="hljs-attribute">trust_remote_code</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(model_qwen_lm)<br></code></pre></td></tr></table></figure><p>结构如下：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739331717753-40.png" alt="img"></p><p>对比一下可以看出，llama3 和 qwen2 还是有点区别，在 Embedding、Attention 和 FNN 的参数都有所差异。</p><p>通过参数，可以看出 llama3 和 qwen2 的词表大小都是64的倍数，具体原因见： <a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247485933&idx=1&sn=080978167f972ed3c686e417c6f792f9&chksm=fa677734cd10fe227cd6edbdf61b9e786e40df5b7d7c5e51d30b5906002ba0ec256f8f89658c&scene=21#wechat_redirect">NLP 面试八股：“Transformers &#x2F; LLM 的词表应该选多大?” 学姐这么告诉我答案</a></p><p>通过参数，你能看出 llama3 是 Grouped Query Attention 么?</p><p>参数量计算：151936 * 4096  + 32 * (4096 * 2 + 12288 * 4096 + 12288 + 4096 * 4096 + 11008 * 4096 * 3) + 4096 + 4096 * 151936 &#x3D; 7721324544</p><p>所以 qwen 7B 的真实参数量为：7721324544.</p><h2 id="我堂堂大学生，不想手动计算"><a href="#我堂堂大学生，不想手动计算" class="headerlink" title="我堂堂大学生，不想手动计算"></a>我堂堂大学生，不想手动计算</h2><p>上面是手动计算的方法，利用 pytorch 提供的函数，我们可以很方便的计算出模型的参数量，只需要一行就行，代码如下：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">count_parameters</span>(model):<br>    return <span class="hljs-built_in">sum</span>(p.<span class="hljs-built_in">numel</span>() for p in model.<span class="hljs-built_in">parameters</span>() if p.requires_grad)<br></code></pre></td></tr></table></figure><h1 id="9-Pre-Norm-和-Post-Norm-各自的优缺点？"><a href="#9-Pre-Norm-和-Post-Norm-各自的优缺点？" class="headerlink" title="9. Pre Norm 和 Post Norm 各自的优缺点？"></a>9. Pre Norm 和 Post Norm 各自的优缺点？</h1><p><a href="https://mp.weixin.qq.com/s/9Bf7Kb17-uNGZSBDnHYDaA">字节面试官:”Pre Norm 和 Post Norm 各自的优缺点？” 学妹这么回答</a></p><h1 id="10-Transformers-LLM-的词表应该选多大"><a href="#10-Transformers-LLM-的词表应该选多大" class="headerlink" title="10. Transformers &#x2F; LLM 的词表应该选多大?"></a>10. Transformers &#x2F; LLM 的词表应该选多大?</h1><h2 id="答案"><a href="#答案" class="headerlink" title="答案"></a>答案</h2><p>先说一下结论：</p><ol><li><strong>数据量够大的情况下</strong>，vocabulary 越大，压缩率越高，模型效果越好。</li><li>太大的 vocabulary 需要做一些训练和推理的优化，所以要<strong>平衡计算和效果。</strong></li><li>要考虑内存对齐。vocabulary 的<strong>大小设置要是 8 的倍数，在 A100 上则是 64 的倍数。</strong>（不同的GPU可能不一样）</li></ol><p>下面分别说一下这三点.</p><h2 id="越大越好"><a href="#越大越好" class="headerlink" title="越大越好"></a>越大越好</h2><p>目前已经有很多研究表明，词表越大模型效果会更好。 比如最近刚发的一篇《Large Vocabulary Size Improves Large Language Models》，里面就详细对比了词表大小分别为：5k, 10k, 50k, 100k and 500k 的效果。如下图所示</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739332256825-46.webp" alt="图片"></p><p>注意这里是完全从头训练的 GPT-3 Large 模型，模型的参数量为 760M。</p><p>然后作者还尝试了在 llama 的基础上扩大词表继续训练，扩大了词表后效果依然有提升。如下表所示：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739332267846-49.webp" alt="图片"></p><p>为什么词表变大会更好？个人觉得有如下几个原因：</p><h3 id="计算效率的提升"><a href="#计算效率的提升" class="headerlink" title="计算效率的提升"></a><strong>计算效率的提升</strong></h3><p>相同的文本，转换为token后越短越好。通常用压缩率来衡量文本转换为token后的压缩比例。</p><p>更高的压缩率代表了相同数量的token能够表达更多的信息，相同的信息 token 越短则训练效率更高。</p><p>Baichuan 在技术报告里给出的一些模型的压缩率如下</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739332283396-52.webp" alt="图片">然后千问在技术报告里也提到自家模型的压缩率比其他家更好。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739332191882-43.webp" alt="图片"></p><h3 id="有助于理解文本"><a href="#有助于理解文本" class="headerlink" title="有助于理解文本"></a><strong>有助于理解文本</strong></h3><p>更多的词汇能够减少 OOV (Out of Vocabulary)的影响, 训练的信息不会丢失，推理的时候泛化能力也更强。</p><p>同时更多的词汇可以减少词汇分解后的歧义，从而更好地理解和生成文本。</p><h3 id="更长的上下文"><a href="#更长的上下文" class="headerlink" title="更长的上下文"></a><strong>更长的上下文</strong></h3><p>预训练阶段往往都有最大序列长度的限制，<strong>压缩率更高代表着能看到更多的上下文</strong>，就能 attention 到更多的信息。</p><p>还有一些论文比如《Impact of Tokenization on Language Models: An Analysis for Turkish》等也有类似的结论。</p><h2 id="计算效率的考虑"><a href="#计算效率的考虑" class="headerlink" title="计算效率的考虑"></a>计算效率的考虑</h2><p>虽然 vocabulary 越大越好，但是也不能无限扩大。因为 vocabulary 变大后，Embedding 层变大，最后输出的 Head layer 也会变大。比如 Llama3 将 vocabulary 从 llama2 的 32000 扩展到 128256，参数量就变大了。llama2 还不到7B，但是 Llama3 有 8B了（当然这里面还有其他参数的改动）。</p><p>参数更大，更占内存，而且输出的时候 softmax 也更大，计算就更慢。虽然大多数情况下 token 量的减少，整体上是算得更快的。</p><p>所以也不能设置得过大，目前<strong>业界普遍设置在 10万 到 20万左右</strong>。比如 Qwen 的 词表大小为 152064，baichuan2为125696，llama3 为128256，deepseek 为 102400。</p><p>多模态的会更大一些。</p><p>当然 softmax 过大的问题目前也有解法，可以用 Adaptive softmax，参考论文《Efficient softmax approximation for GPUs》。</p><h2 id="内存对齐"><a href="#内存对齐" class="headerlink" title="内存对齐"></a>内存对齐</h2><p>之前就有人看到 qwen 的readme 和 训练代码中 vocabulary 的数量不一样， readme 中为 151643，但是实际上代码里写的是 152064。</p><p>这就是为了内存对齐。所以<strong>很多时候模型上的一些设置其实跟硬件息息相关</strong>。</p><p>因为最终Embedding 矩阵和输出时候的 Head Layer 最终都会转换成矩阵放到 GPU 的 Tensor Core 中计算。而根据英伟达的开发手册，矩阵运算最好根据 GPU 和计算类型满足如下的条件：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426572811-17.webp" alt="图片"></p><p>Karpathy 也曾经证实了这一点，他当时写了个 nanoGPT，提升最大的点就是把词表从 50257 改成了 50304，后一个是64的倍数。然后带来了25%的速度上的提升。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426566396-14.webp" alt="图片"></p><p>所以目前大多数训练都在 A100 上训练，所以基本上都是64的倍数。如果某个模型的词表不是 64的倍数，那可能不知不觉浪费了很多计算资源。</p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>笔试面试</category>
      
      <category>AI算法</category>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>笔试面试</tag>
      
      <tag>算法面试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>公众号“看图学”试题合集（3）</title>
    <link href="/2025/02/12/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/AI%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98003%EF%BC%9A%E5%85%AC%E4%BC%97%E5%8F%B7%E2%80%9C%E7%9C%8B%E5%9B%BE%E5%AD%A6%E2%80%9D%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86%EF%BC%883%EF%BC%89/"/>
    <url>/2025/02/12/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/AI%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98003%EF%BC%9A%E5%85%AC%E4%BC%97%E5%8F%B7%E2%80%9C%E7%9C%8B%E5%9B%BE%E5%AD%A6%E2%80%9D%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86%EF%BC%883%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="1-tanh-和-sigmoid-什么关系？为什么-tanh-作为激活函数比-sigmoid-要好？"><a href="#1-tanh-和-sigmoid-什么关系？为什么-tanh-作为激活函数比-sigmoid-要好？" class="headerlink" title="1. tanh 和 sigmoid 什么关系？为什么 tanh 作为激活函数比 sigmoid 要好？"></a>1. tanh 和 sigmoid 什么关系？为什么 tanh 作为激活函数比 sigmoid 要好？</h1><p>sigmoid 的性质导致其导数全为正数，详细看：<a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247486543&idx=1&sn=fb01702155d9e6e477fa3e2b21405aa2&chksm=fa677296cd10fb8072900b63063e73107827719bbbdb4914f40b19a61172e279055e05f9a3ea&scene=21#wechat_redirect">我用Sigmoid 作为激活函数，导师建议延毕</a>，导致这样的其中一个原因（并不是全部的原因）是：sigmoid 的值的范围在 0-1 之间。</p><p>如果将 sigmoid 函数变成 zero centered， 那么其值就有正有负， sigmoid 收敛慢，不稳定的原因就解决了。</p><p>那么 sigmoid 如何 变成 zero centered 呢？</p><p>sigmoid 的范围在[0, 1] 之间， 其实只需要减去 0.5 就可以了，这样就变成了 [-0.5, 0.5] 之间。</p><p>我们定义这么一个的函数：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426648465-25.png" alt="img"></p><p>我们把这个式子推导一下，看看能得到什么</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426636401-22.png" alt="img"></p><p>结果另一个常用的激活函数就出现了。</p><p>结合上面公式，tanh 也可以写作</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426655808-27.png" alt="img"></p><p>所以 <strong>tanh 作为激活函数，本质上就是对 sigmoid 做了个 zero centered 操作</strong>， 先把 <strong>sigmoid 在x轴挤了一下，然后在 y 轴上拉伸，最后减去中心点</strong>，相当于平移成一个 zero centered 的函数。</p><p>其实我感觉不做这些缩放的变换，直接用 g(x) &#x3D; σ(x) -0.5 作为激活函数也是完全可以的。</p><p>这样解决了梯度全为正（或者全为负）的问题。</p><p>但是毕竟跟 sigmoid 是同源的，依然没有解决梯度消失的问题。</p><p>下面是 tanh 的导数 和 sigmoid 导数的对比：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739321758907-3.png" alt="img"></p><p>可以看出，tanh 的饱和区大概在 [-3,3] 之间。</p><p>但是即使这样，<strong>tanh 在实际的使用效果上，已经比 sigmoid 要好很多了。</strong></p><h1 id="2-基于-Llama-的模型都有哪些？有什么细微的差异？"><a href="#2-基于-Llama-的模型都有哪些？有什么细微的差异？" class="headerlink" title="2. 基于 Llama 的模型都有哪些？有什么细微的差异？"></a>2. 基于 Llama 的模型都有哪些？有什么细微的差异？</h1><h2 id="Llama-生态"><a href="#Llama-生态" class="headerlink" title="Llama 生态"></a><strong>Llama 生态</strong></h2><p>现在的模型架构基本都是 Llama 了。即使本来也有一些自己独创的结构，但是随着 Llama 生态环境的日趋统一，也都被迫向 Llama 低头了，不然没人适配你的特殊架构，自然就不带你玩了。比如 GLM 之前属于 Prefix LM，但是现在也变成 Llama 类似了。</p><p><strong>虽然大家都长的很像，但是细微之处还是有些不太一样</strong>。今天就聊聊跟 Llama 很像的模型之间的细微差异。</p><p>Llama 目前有3代，先看一下 Llama 自己的变化，然后再以 Llama 为基准看一下其他模型与 Llama 的不同。</p><h2 id="Llama-1-2-3"><a href="#Llama-1-2-3" class="headerlink" title="Llama 1 2 3"></a><strong>Llama 1 2 3</strong></h2><h3 id="Llama-1"><a href="#Llama-1" class="headerlink" title="Llama 1"></a><strong>Llama 1</strong></h3><p>Llama 1 的架构是基于 GPT 来的，做了如下的升级：</p><ul><li>采用了 Pre-RMSNorm</li><li>把 Gelu 改成了 SwiGLU</li><li>位置编码改成了 RoPE</li></ul><p>需要注意的是，<strong>这些内容都不是 Meta 首创的，但是 Meta 的 Llama 团队将他们组合到了一起并且取得了开源的 SOTA 效果</strong>。至于闭源的，那肯定早都用了。</p><p>其结构如下所示(Llama 7B)：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs routeros">LlamaForCausalLM(<br>  (model): LlamaModel(<br>    (embed_tokens): Embedding(32000, 4096, <span class="hljs-attribute">padding_idx</span>=0)<br>    (layers): ModuleList(<br>      (0-31): 32 x LlamaDecoderLayer(<br>        (self_attn): LlamaAttention(<br>          (q_proj): Linear(<span class="hljs-attribute">in_features</span>=4096, <span class="hljs-attribute">out_features</span>=4096, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>)<br>          (k_proj): Linear(<span class="hljs-attribute">in_features</span>=4096, <span class="hljs-attribute">out_features</span>=4096, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>)<br>          (v_proj): Linear(<span class="hljs-attribute">in_features</span>=4096, <span class="hljs-attribute">out_features</span>=4096, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>)<br>          (o_proj): Linear(<span class="hljs-attribute">in_features</span>=4096, <span class="hljs-attribute">out_features</span>=4096, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>)<br>          (rotary_emb): LlamaRotaryEmbedding()<br>        )<br>        (mlp): LlamaMLP(<br>          (gate_proj): Linear(<span class="hljs-attribute">in_features</span>=4096, <span class="hljs-attribute">out_features</span>=11008, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>)<br>          (up_proj): Linear(<span class="hljs-attribute">in_features</span>=4096, <span class="hljs-attribute">out_features</span>=11008, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>)<br>          (down_proj): Linear(<span class="hljs-attribute">in_features</span>=11008, <span class="hljs-attribute">out_features</span>=4096, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>)<br>          (act_fn): SiLU()<br>        )<br>        (input_layernorm): LlamaRMSNorm((4096,), <span class="hljs-attribute">eps</span>=1e-06)<br>        (post_attention_layernorm): LlamaRMSNorm((4096,), <span class="hljs-attribute">eps</span>=1e-06)<br>      )<br>    )<br>    (norm): LlamaRMSNorm((4096,), <span class="hljs-attribute">eps</span>=1e-06)<br>    (rotary_emb): LlamaRotaryEmbedding()<br>  )<br>  (lm_head): Linear(<span class="hljs-attribute">in_features</span>=4096, <span class="hljs-attribute">out_features</span>=32000, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>)<br>)<br></code></pre></td></tr></table></figure><h3 id="Llama-2"><a href="#Llama-2" class="headerlink" title="Llama 2"></a><strong>Llama 2</strong></h3><p>Llama2 和 Llama1 结构基本相同，但是在更大的模型上（34B和70B） 采用了 grouped-query attention，主要是为了加速。</p><p>还有就是将上下文从 2048 扩展到了 4096.</p><h3 id="Llama-3"><a href="#Llama-3" class="headerlink" title="Llama 3"></a><strong>Llama 3</strong></h3><p>Llama3 做了如下改变</p><ul><li>GQA 变成标配。</li><li>上下文 从 4096 扩展到了 8192</li><li>词表大小从 32k 变成了 128k。前两代都是基于 SentencePiece 的，Llama 3 直接采用了 Openai 的 tiktoken。因为 tiktoken 用 rust 进行了底层的深度优化，效率比其他家要好很多。</li></ul><h2 id="Baichuan-系列"><a href="#Baichuan-系列" class="headerlink" title="Baichuan 系列"></a><strong>Baichuan 系列</strong></h2><h3 id="Baichuan-1"><a href="#Baichuan-1" class="headerlink" title="Baichuan 1"></a><strong>Baichuan 1</strong></h3><p>Baichuan 1 可以说是完全复用了 Llama 1 的架构。把权重的名字改一改可以完全用 baichuan 的代码来加载 llama 的权重。具体怎么修改的代码放在付费内容了，感兴趣可以看看。</p><p>有如下的差异：</p><ul><li>llama 的 qkv 三个权重矩阵，在 baichuan 里变成了一个矩阵，相当于 qkv concat 起来了。</li><li>扩充了 llama 的词表，加入了中文，词表大小为 64k，llama 1 为 32k。</li><li>上下文为 4096， llama 1 为 2048.</li></ul><h3 id="Baichuan-2"><a href="#Baichuan-2" class="headerlink" title="Baichuan 2"></a><strong>Baichuan 2</strong></h3><p>Baichuan 2 的架构在 Llama 2 的基础上做了一些创新。</p><ul><li>在 lm_head 模块加了一个 norm，论文中说是可以提升效果</li><li>在 13B 的模型上采用了 Alibi 位置编码。</li><li>词表从 64k 扩充到了  125,696</li></ul><h3 id="Baichuan-3-4"><a href="#Baichuan-3-4" class="headerlink" title="Baichuan 3 &amp; 4"></a><strong>Baichuan 3 &amp; 4</strong></h3><p>没有开源。</p><h2 id="Yi"><a href="#Yi" class="headerlink" title="Yi"></a><strong>Yi</strong></h2><p>yi 的架构和 llama2 一样。需要注意的是 llama2 只在更大的模型上使用了 GQA, 但是 Yi 在所有系列都用了。</p><p>在经历过一些开源协议的质疑之后，<strong>现在 yi 的模型可以用 LlamaForCausalLM 加载了</strong>。</p><h2 id="Qwen"><a href="#Qwen" class="headerlink" title="Qwen"></a><strong>Qwen</strong></h2><h3 id="Qwen-1"><a href="#Qwen-1" class="headerlink" title="Qwen 1"></a><strong>Qwen 1</strong></h3><p>Qwen 1 和 Llama 1 的区别如下：</p><ul><li>qkv 矩阵和 baichuan 类似，变成了一个 concat 后的大矩阵。</li><li><strong>这个 qkv 的矩阵有 bias</strong>，这一点和大多数模型都不一样。这是因为苏剑林的一篇文章，认为加入 bias 可以提高模型的外推能力：<a href="https://spaces.ac.cn/archives/9577">https://spaces.ac.cn/archives/9577</a></li><li>词表大小为：151936</li><li>训练的长度是2048， 但是通过一些外推手段来扩展长度。</li></ul><h3 id="Qwen-1-5"><a href="#Qwen-1-5" class="headerlink" title="Qwen 1.5"></a><strong>Qwen 1.5</strong></h3><p>其实 Qwen 1.5 开始，比起 Llama 就多了很多自己的东西，只不过 Qwen 1 仍然和 Llama 很相似，所以这里也一并写一下吧。</p><p>1.5 的版本更像是在 1 的基础上做了很多扩展，重点如下：</p><ul><li>扩展长度到 32K</li><li>sliding window attention 和 full attention 的混合</li><li>32B 的模型尝试了使用 GQA</li><li>tokenizer 针对代码做了一些优化。</li></ul><h3 id="Qwen-2"><a href="#Qwen-2" class="headerlink" title="Qwen 2"></a><strong>Qwen 2</strong></h3><p>Qwen 2 包含了 1.5 的所有改变。和 llama 2 的区别：</p><ul><li>qkv 矩阵有 bias</li><li>全尺寸使用了 GQA</li><li>上下文扩展为 32K</li><li>采用了 Dual Chunk Attention with YARN</li><li>还有一点就是在同等尺寸上，Qwen 2 相对于 1.5 和 1，将 MLP 模块的 hidden size 变大了，其他模块的 hidden size 变小了。以提高模型的表达的记忆能力。</li><li>词表又扩充了一点点。</li></ul><h2 id="ChatGLM"><a href="#ChatGLM" class="headerlink" title="ChatGLM"></a><strong>ChatGLM</strong></h2><p>GLM 最开始的时候采用的是 Prefix LM，但是后来也都改成 Decoder Only LM 了。</p><p>所以虽然 GLM 要早于 Llama，但是最后还是和 Llama 变得很像。上面提到的其实最像 Qwen 1.</p><p>所以也说一下与 Llama 的区别：</p><ul><li>qkv 矩阵和 baichuan 类似，变成了一个 concat 后的大矩阵。</li><li>这个 qkv 的矩阵有 bias。</li></ul><h2 id="MiniCPM"><a href="#MiniCPM" class="headerlink" title="MiniCPM"></a><strong>MiniCPM</strong></h2><p>目前已经转战 size 略小一点的模型，也取得了很不错的效果。</p><p>我粗看其架构应该和 llama 3 差不多，区别：</p><ul><li>采用了 Weight Tying</li><li>整体框架采用了 deep and thin 的结构。</li></ul><p>有个细节是，我看论文里写的词表大小为：122,753， 似乎有点非主流。因为一般都需要设置成 8 或者64 的倍数。具体可以看：<a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247485933&idx=1&sn=080978167f972ed3c686e417c6f792f9&chksm=fa677734cd10fe227cd6edbdf61b9e786e40df5b7d7c5e51d30b5906002ba0ec256f8f89658c&scene=21#wechat_redirect">NLP 面试八股：“Transformers &#x2F; LLM 的词表应该选多大?” 学姐这么告诉我答案</a></p><h2 id="Gemma"><a href="#Gemma" class="headerlink" title="Gemma"></a><strong>Gemma</strong></h2><p>我要说 Gemma 是基于 Llama 的，Google 肯定是不承认的。</p><p>Google 有不承认的底气，毕竟 Transformers 是人家搞出来的， GLU 也是人家的，MQA 和 GQA 也是人家搞出来的。</p><p>最终发现 <strong>Llama 中除了 Pre-RMSNorm 和 RoPE，其他都是 Google 的成果</strong>。只能说 Google 真的是 “斗宗强者，恐怖如斯”。</p><p>但是最后的架构和 Llama 其实还是很像。区别如下：</p><h3 id="Gemma-1"><a href="#Gemma-1" class="headerlink" title="Gemma 1"></a><strong>Gemma 1</strong></h3><ul><li>MLP 的激活采用了 GeGLU 而不是 SwiGLU</li><li>采用了 MHA。但是 2 代还是换成了 GQA</li><li>使用了 Weight Tying</li></ul><h3 id="Gemma-2"><a href="#Gemma-2" class="headerlink" title="Gemma 2"></a><strong>Gemma 2</strong></h3><ul><li>MLP 的激活采用了 GeGLU 而不是 SwiGLU</li><li>融合了 Local and Global Attention</li><li>使用了 Weight Tying</li></ul><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a><strong>其他</strong></h2><p>至于 Mistral 和 DeepseekV2 和 Llama 还是有些不太一样，所以这次就先不介绍了。</p><h1 id="3-大模型一个-token-能代表几个单词和汉字？"><a href="#3-大模型一个-token-能代表几个单词和汉字？" class="headerlink" title="3. 大模型一个 token 能代表几个单词和汉字？"></a>3. 大模型一个 token 能代表几个单词和汉字？</h1><h2 id="答案"><a href="#答案" class="headerlink" title="答案"></a><strong>答案</strong></h2><p>每个模型的 Tokenizer 都不太一样，所以这个问题不能给出很精确的答案，<strong>更多的是考察一些大模型的使用经验。</strong></p><p>**文末有一些目前 Tokenizer 的看法，感兴趣的可以讨论。<br>**</p><p>也可以换着法子问，比如 一段一万字的 prompt，输入到最大长度为 8192 的模型，是否能正确的输出？</p><p>但是每个模型的 Tokenization 都是在自己的语料上训练出来的，怎么知道具体某一个 Tokenizer 每个 token 平均代表几个汉字呢？</p><p>有的模型的技术报告会在 Tokenization 那一章提供一个“压缩率” 的指标，比如 qwen 和 baichuan 的，但是有些技术报告并不会提。</p><p>虽然说不同的 tokenizer 在不同的训练语料上训练的不一样，但是大家采用的方法其实无非就那么几种，感兴趣的可以看：<a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247486043&idx=1&sn=79c199f8a3261646963e0cb5ba66e1d3&chksm=fa677482cd10fd94a4c04f0fda46ce89edc4138e340b600eeb994ab6e7605e2f348ff0c14551&scene=21#wechat_redirect">小米面试官：“Tokenization 是什么”。封面看着眼熟</a></p><p>其实只要训练语料里<strong>主要的语言一样，在大量数据的堆积下，最终的的结果差异并不大</strong>。下面会给出以 英文为主的模型和中英文为主的模型的一些结果对比。</p><p>为了测试，我选择了两本小说，《孔乙己》 和 《哈利波特》第一章，分别测试不同 tokenizer 对这两篇小说的中文版和英文版的效果。</p><p>结果如下图所示：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739322623949-5.png" alt="img"></p><p>虽然这只能算是个抽样，但是也能看出一些问题。</p><p>每个模型在<strong>英文上的效果基本差不太多。一个 token 大概占 0.75～0.8 个单词</strong>。这与 OpenAI 官网上写的差不多：“A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~&#x3D; 75 words).” </p><p><strong>国内的模型在中文语料上特训之后</strong>，中文编码的效率显著高于英文的 ChatGPT 和 Llama。<strong>一个 token 大概占1.5 个汉字。</strong></p><h2 id="目前的-Tokenizer-的编码效率够么？"><a href="#目前的-Tokenizer-的编码效率够么？" class="headerlink" title="目前的 Tokenizer 的编码效率够么？"></a><strong>目前的 Tokenizer 的编码效率够么？</strong></h2><p>周一曾经写了一一篇交叉熵的文章：<a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247486447&idx=1&sn=975e04a89b4e3b11b782b6579b2d8de9&chksm=fa677536cd10fc209f2f2a0e30daffe49aa8075bf844679e6b02ded5131d26ec254a4d864f04&scene=21#wechat_redirect">华为面试官：“交叉熵 (cross entropy) ，KL 散度的值，到底有什么含义？”</a>， 里面有提到通信的问题。</p><p><strong>如何把语料用最少的 bit 位传输给模型，其实也是个通信的问题</strong>。只不过现在模型参数的通信远高于数据的通信，所以数据 与 GPU 的通信目前还不需要优化。</p><p>如果哪一天模型需要大量的输入的时候，tokenizer 的编码效率可能还会被研究。</p><p>当前的 tokenization 是否是最优编码？目前只能说有最优编码的影子，但是还不完全是。</p><p>比如 BPE 的算法其实就是在构建 Huffman 树，但是构建了之后仍然采用了相同比特位数来编码。这么做的好处省去了解码的过程，直接查表就获取到了 Embedding，但是其实引入解码这点计算量也算不了啥。坏处就是通信上其实还有优化的空间。</p><p>还有一点就是<strong>中文的编码效率其实理论上还可以更高</strong>，因为<strong>目前所有的处理流程都是按照英文的流程来的。</strong></p><p>比如 subword，对中文就完全没生效啊。之前也举过一个例子，oarfish 我虽然不知道是啥，但是猜测是条鱼。对于中文来说，“鲥”这个字我可能也不认识，但是我也猜测这是条鱼，但是这个字在中文肯定被表示成 bytes 了，就没啥意义了。</p><p>所以中文如何高效的编码，也应该是一个研究课题，我甚至感觉中文这种二维的文字，应该和图像的 tokenizer 有某种联系，比如在训练的时候，除了 id embedding，还有这个字对应的图片信息的 embedding。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739322635546-8.png" alt="img"></p><p>我也不知道这个想法之前有没有人提过，要是没有的话，后续有人研究 id embedding + token image embedding 的话，可以引用一下这篇</p><h1 id="4-交叉熵-cross-entropy-，KL-散度的值，到底有什么含义？"><a href="#4-交叉熵-cross-entropy-，KL-散度的值，到底有什么含义？" class="headerlink" title="4. 交叉熵 (cross entropy) ，KL 散度的值，到底有什么含义？"></a>4. 交叉熵 (cross entropy) ，KL 散度的值，到底有什么含义？</h1><p><a href="https://mp.weixin.qq.com/s/KQUDz8cP95RQpHVl7TpaVA">华为面试官：“交叉熵 (cross entropy) ，KL 散度的值，到底有什么含义？”</a></p><h1 id="5-说一下-DPO-的原理。"><a href="#5-说一下-DPO-的原理。" class="headerlink" title="5. 说一下 DPO 的原理。"></a>5. 说一下 DPO 的原理。</h1><h2 id="简易版答案"><a href="#简易版答案" class="headerlink" title="简易版答案"></a><strong>简易版答案</strong></h2><ol><li>RLHF 的目的是求一个最优的policy，但是因为 RLHF 的整体上的流程不可微,虽然可以用 TRPO&#x2F;PPO 等策略梯度算法来优化，但是很耗资源。</li><li>DPO 重新定义了问题，将<strong>求解最优的 policy 变成了求解 reward model</strong>。</li><li>在求解 reward model 的过程中，又很巧妙的去掉了无法计算的部分，整个求解过程也是可微的。</li><li>DPO 理论上非常漂亮，实际使用中也能用少量资源达到不错的效果，但是也有很多值得注意的地方。</li></ol><p><a href="https://mp.weixin.qq.com/s/PSo9d0VtHiAGYiZUPVasmg">阿里大模型原题：“请讲述一下 DPO 的原理”</a></p><h1 id="6-RLHF-为什么不直接对-loss-进行梯度下降来求解？"><a href="#6-RLHF-为什么不直接对-loss-进行梯度下降来求解？" class="headerlink" title="6. RLHF 为什么不直接对 loss 进行梯度下降来求解？"></a>6. RLHF 为什么不直接对 loss 进行梯度下降来求解？</h1><h2 id="从强化学习说起"><a href="#从强化学习说起" class="headerlink" title="从强化学习说起"></a><strong>从强化学习说起</strong></h2><p>这里先简单讲述一下强化学习和 RLHF，来了解一下 RLHF 的 loss 是怎么来的。</p><p><strong>强化学习的目标</strong>：假设我们有探索一个未知环境(environment)的执行代理(agent)，这个代理可以通过与环境互动来获取一定奖励。代理应当对所执行的动作(action)有所偏好，<strong>以最大化累计收益</strong>。</p><h3 id="举个一般的例子"><a href="#举个一般的例子" class="headerlink" title="举个一般的例子** **"></a><strong>举个一般的例子</strong>**<img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426684501-30.png" alt="img"> **</h3><p>利用上面的例子来说明一些强化学习的基本概念。</p><ul><li><p>Agent: Robot</p></li><li><p>State：position (x, y)</p></li><li><p>Action: 移动到下一个格子</p></li><li><p>Reward model：</p><ul><li>移动到空白格子：0分</li><li>移动到火上：-10分</li><li>移动到钻石：+100分</li></ul></li><li><p>Policy：</p><ul><li>假设action只由状态决定$\pi(a|s)$</li></ul><h3 id="语言模型的例子"><a href="#语言模型的例子" class="headerlink" title="语言模型的例子"></a><strong>语言模型的例子</strong></h3><p>在语言模型中，对应的强化学习的概念如下：</p><ul><li>Agent:  语言模型</li><li>State：the prompt (input tokens)</li><li>Action:  下一个 token 是什么</li><li>Reward model：<ul><li>人类给生成的结果打分，来确定好坏</li></ul></li><li>Policy：<ul><li>语言模型本身，因为语言模型的建模就是跟进前面的token去预测下一个。</li></ul></li></ul><h2 id="RLHF"><a href="#RLHF" class="headerlink" title="RLHF"></a><strong>RLHF</strong></h2><p>RLHF 是利用人类的反馈训练了一个reward model，其流程如下：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426697282-33.png" alt="img"></p><p>RLHF 也是强化学习，所以最终要<strong>求一个最优的 Policy</strong>，其优化目标如下：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250212101125829.png" alt="RLHF"></p><p>公式的第一部分是强化学习的目标，那就是<strong>最大化奖励</strong>。</p><p>公式的第二部分是一个 <strong>KL 散度，用来约束模型要尽可能和优化前的模型接近</strong>。</p><p>KL 散度用来刻画两个分布的距离，KL 散度越大代表两个分布越不一样。之所以要加上 KL 散度的约束，是因为模型的优化都是贪婪的，如果没有 KL 散度的约束，那训练会一直往 reward model 定义的方向去优化，最后就只会输出reward 分数最高的那一部分结果。那最后模型就只会输出“好好好”，“很赞”这一类无意义的话，通过大量数据训练，花了那么多钱训练出来的base model 的能力都会丢失，变成了电子垃圾。</p><h2 id="为什么不直接对-loss-求梯度"><a href="#为什么不直接对-loss-求梯度" class="headerlink" title="为什么不直接对 loss 求梯度"></a><strong>为什么不直接对 loss 求梯度</strong></h2><p>核心原因就是<strong>因为 loss 或者优化目标不可微</strong>，看一下优化目标的红色框部分：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426709790-36.png" alt="img"></p><p>这里的 y 是采样出来的，可能是 greedy，beam search 等，这个操作在词汇表上进行采样或选择，而不是产生一个连续的、可微分的输出。所以也就没法直接使用梯度下降，而是用 PPO等策略梯度来求解。</p></li></ul><p><a href="https://mp.weixin.qq.com/s/Qxue1q9n9q06HLg_ijjqRw">头条大模型面试：“RLHF 为什么不直接对 loss 进行梯度下降来求解？”</a></p><h1 id="7-为什么现在深度学习都用-ResNet"><a href="#7-为什么现在深度学习都用-ResNet" class="headerlink" title="7. 为什么现在深度学习都用 ResNet?"></a>7. 为什么现在深度学习都用 ResNet?</h1><p><a href="https://mp.weixin.qq.com/s/8dEl63KBB_AHx8p1KaGnWA">字节大模型一面：“为什么现在深度学习都用 ResNet?”</a></p><h1 id="8-KV-Cache-原理是什么？"><a href="#8-KV-Cache-原理是什么？" class="headerlink" title="8. KV Cache 原理是什么？"></a>8. KV Cache 原理是什么？</h1><p><a href="https://mp.weixin.qq.com/s/mKdliGu4WhUx4PHatBpewA">阿里大模型面试原题：“ KV Cache 原理是什么？”</a></p><h1 id="9-共享权重如何求梯度？"><a href="#9-共享权重如何求梯度？" class="headerlink" title="9. 共享权重如何求梯度？"></a>9. 共享权重如何求梯度？</h1><p>之前的文章：<a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247486000&idx=1&sn=74abbd3c3306652d86319c0239d8b644&chksm=fa6774e9cd10fdff2f404f54335b8f886cfa495390843ce7f7a16b16c8e86c02ddb1d8273739&scene=21#wechat_redirect">阿里面试官：“Transformers 中的 Weight Tying 是什么?”</a>中有提到过 Weight Tying 技术。把 Embedding 层和最后的 LM Head 层进行了共享，假设共享的矩阵为 W。</p><p>我们知道反向传播是一层一层从后往前计算的，但是权重 W 最开始就要计算梯度，然后传播到最后还要计算梯度。</p><p>W 的梯度，到底是计算了一次还是计算了两次？</p><p>如果是计算了2次，那两次权重是怎么更新的呢？</p><p>其实，可以先考虑一个简单一点的例子，就知道答案。</p><p>假设<br>$$<br>f(x)&#x3D;xy<br>$$</p><p>$$<br>g(x)&#x3D;xf(x)<br>$$</p><p>求 g(x) 对于 x 的导数。这个例子我们一种做法是把式子展开<br>$$<br>g(x)&#x3D;x\times x\times y&#x3D;x^2y<br>$$<br>很容易知道<br>$$<br>g^{\prime}(x)&#x3D;2xy<br>$$<br>但是实际神经网络反向传播的计算中，并不是全展开了才进行计算，而是迭代的计算。我们换一种思路来求解：<br>$$<br>\begin{aligned}g^{\prime}(x)&amp;&#x3D;x^{\prime}f(x)+xf^{\prime}(x)\&amp;&#x3D;f(x)+x\times y\&amp;&#x3D;xy+xy&#x3D;2xy\end{aligned}<br>$$<br>所以我们就大概猜到，<strong>共享权重执行了2次计算，然后结果相加</strong>。下面举个具体的例子来说明：</p><p>我们假设有一个三层的神经网络，第一层的权重和第三层的权重是共享的，来模拟 Weight Tying 的情况。<br>$$<br>\begin{aligned}h_1&amp;&#x3D;\mathrm{ReLU}(W_1x+b_1)\h_2&amp;&#x3D;\mathrm{ReLU}(W_2h_1+b_2)\y&amp;&#x3D;W_1^Th_2+b_3\end{aligned}<br>$$<br>假设损失函数为<br>$$<br>L&#x3D;\frac1{NM}\sum_{i&#x3D;1}^N\sum_{j&#x3D;1}^M(y_{ij}-y_{true,ij})^2<br>$$<br>其反向传播的公式为：<br>$$<br>\begin{aligned}&amp;\frac{\partial L}{\partial y}&#x3D;\frac{2(y-y_{true})}{NM}\&amp;\frac{\partial L}{\partial h_2}&#x3D;W_1\frac{\partial L}{\partial y}\odot I(h_2&gt;0)\&amp;\frac{\partial L}{\partial W_2}&#x3D;\frac{\partial L}{\partial h_2}h_1^T\&amp;\frac{\partial L}{\partial h_1}&#x3D;W_2^T\frac{\partial L}{\partial h_2}\odot I(h_1&gt;0)\&amp;\frac{\partial L}{\partial W_1}&#x3D;\frac{\partial L}{\partial h_1}x^T+\left(\frac{\partial L}{\partial y}h_2^T\right)^T\end{aligned}<br>$$</p><h2 id="Pytorch-等框架怎么计算？"><a href="#Pytorch-等框架怎么计算？" class="headerlink" title="Pytorch 等框架怎么计算？"></a><strong>Pytorch 等框架怎么计算？</strong></h2><p>上面只是从原理上进行演示，但是 pytorch，tensorflow 等工具并不是像上面那样一步一步通过公式推导来进行计算的。<strong>而是通过 autograd 的技术来进行</strong>。</p><p>autograd 细节很多，但是大体上可以理解为整个神经网络都抽象成一个计算图，然后每个节点都有对应的前向传播的计算逻辑，还有对应的反向传播的计算逻辑。整个计算都是在计算图上流动。</p><p>对于上面的小例子，对应的反向传播的计算图如下：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739327000639-14.png" alt="img"></p><p>从这里，可以更清楚的看到，<strong>共享权重的梯度，在最后是有合并操作的</strong>。</p><h2 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a><strong>代码演示</strong></h2><p>我们和 pytorch 内部的反向传播进行比较，可以看到，与pytorch 的结果一样。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739426815485-39.png" alt="img"></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Custom</span> vs PyTorch gradients:<br><span class="hljs-attribute">W1</span>:<br>  <span class="hljs-attribute">Custom</span> grad mean: -<span class="hljs-number">2</span>.<span class="hljs-number">979950</span><br>  <span class="hljs-attribute">PyTorch</span> grad mean: -<span class="hljs-number">2</span>.<span class="hljs-number">979950</span><br>  <span class="hljs-attribute">Mean</span> difference: <span class="hljs-number">0</span>.<span class="hljs-number">000000</span><br><span class="hljs-attribute">W2</span>:<br>  <span class="hljs-attribute">Custom</span> grad mean: <span class="hljs-number">5</span>.<span class="hljs-number">116329</span><br>  <span class="hljs-attribute">PyTorch</span> grad mean: <span class="hljs-number">5</span>.<span class="hljs-number">116329</span><br>  <span class="hljs-attribute">Mean</span> difference: <span class="hljs-number">0</span>.<span class="hljs-number">000000</span><br></code></pre></td></tr></table></figure><p><a href="https://mp.weixin.qq.com/s/98DzW-3fjDA7ZgKS2NtajQ">字节大模型面试原题：共享权重如何求梯度？</a></p><h1 id="10-BF16-和-FP16-的区别？"><a href="#10-BF16-和-FP16-的区别？" class="headerlink" title="10. BF16 和 FP16 的区别？"></a>10. BF16 和 FP16 的区别？</h1><h2 id="浮点数如何表示"><a href="#浮点数如何表示" class="headerlink" title="浮点数如何表示"></a><strong>浮点数如何表示</strong></h2><p>计算机是二进制的世界，所以浮点数也是用二进制来表示的，与整型不同的是，浮点数通过3个区间来表示。</p><p>这三个区间分别是 sign，exponent，fraction.</p><p>浮点数的计算逻辑如下，以 BF16 举例（图中的数字是BF16中最接近π的）</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739327166736-17.png" alt="img"></p><h3 id="sign-表示正负，和整型一样。"><a href="#sign-表示正负，和整型一样。" class="headerlink" title="sign 表示正负，和整型一样。"></a><strong>sign 表示正负，和整型一样。</strong></h3><p>1表示正数，0表示负数。</p><h3 id="exponent-用来确定数字的范围"><a href="#exponent-用来确定数字的范围" class="headerlink" title="exponent 用来确定数字的范围"></a><strong>exponent 用来确定数字的范围</strong></h3><p>如果这一部分有 k 个bit，这 k 个 bit 的二进制表示的数字为 x，那么这一部分表示的值为$2^{x-(2^{k-1}-1)}$。<strong>所以 k 越大，浮点数能表示的范围就越大</strong>。</p><h3 id="fraction-部分用来确定精度"><a href="#fraction-部分用来确定精度" class="headerlink" title="fraction 部分用来确定精度"></a><strong>fraction 部分用来确定精度</strong></h3><p>浮点数的表示，会有一个规范化的动作，那就是所有的数字都会先规范化为 $1.abc\times2^z$​这种表示。</p><p>比如数字 10.0， 表示成二进制是 B1010.0，要变成 B1010.0 &#x3D; B1.010 * 23。前面的 B 代表的是二进制的表示。所以实际上 10.0 在二进制里是<br>$$<br>2^3\times(1\times2^0+0\times2^{-1}+1\times2^{-2}+0<em>2^{-3})&#x3D;8</em>1.25&#x3D;10.0<br>$$<br>而后面这个 1.abc 就是fraction。假设 faction 有 n 个bit，这些 bit 表示的数字为 y，则 fraction 部分代表的数字为$1+\frac y{2^n}$</p><p><strong>fraction 的位数越多</strong>，就代表有更小的$2^{-i}$  参加运算，就可以切割的越细，<strong>能表示的精度就越高</strong>。</p><h3 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a><strong>最终结果</strong></h3><p>浮点数的最终结果由上面3部分组合而成, 假设 exponent 有 k 个 bit，bit 的表示的数为 x；faction 有 n 个bit，这些 bit 表示的数字为 y，则表示的浮点数为<br>$$<br>\mathrm{sign}\times2^{x-(2^{k-1}-1)}\times(1+\frac y{2^n})<br>$$</p><h2 id="BF16-vs-FP16"><a href="#BF16-vs-FP16" class="headerlink" title="BF16 vs FP16"></a>BF16 vs FP16</h2><p>有了上面的基础知识，就很容易知道 BF16 和 FP16 的区别了。</p><p>BF16 一共 16 bit，sign 占 1 bit，exponent 占 8 bit， fraction 占 7 bit。</p><p>FP16 一共 16 bit，sign 占 1 bit，exponent 占 5 bit， fraction 占 10 bit。</p><p>如下图：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739327377568-20.png" alt="img"></p><p>对比 FP16，可以认为 BF16 从 fraction 挪了 3 位给了 exponent。为什么 exponent 选择 8 bit 呢？因为 FP32 的 exponent 是 8 bit。</p><p>所以 BF16 能表示的数字范围更大，但是表示的精度更低。FP16 表示的数字范围更小，但是表示的精度更高。</p><p>具体差多少呢？可以看下表：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739327386996-23.png" alt="img"></p><p>这个表能直观的看到表示范围的差异，<strong>BF16 最大可以表示 3.39e+38， 但是 FP16 最大只能表示65504.0</strong></p><p>但是精度感觉不太出来，好像 roundoff 也差别没那么大。</p><p>但是当数字很大后，所表示的数字就会出现很大的间隔。</p><p>以数字 <strong>19968.0</strong> 举例</p><ul><li>BF16 可以表示这个数字，<strong>但是 BF16 可表示的下一个数字是 20096.0</strong>, (19958, 20096) 这里面的数字 在 FB16 的世界中是不存在的。</li><li><strong>FP16 可表示的下一个数字是 19984.0</strong>，这个要比 BF16 要好一些了，但是仍然有不小的间隔。</li><li><strong>FP32 可以表示的下一个数字是 19968.001953125</strong>，FP32 的表示则要好很多，最起码能表示到小数点后3位左右。</li></ul><p>神经网络的训练过程中，训练的稳定性很重要，如果用 FP16，则会经常溢出，所以采用 BF16 是个不错的选择，但是精度会损失很多，影响收敛速度。很多数值敏感的运算最好还是采用 FP32，可以采用混合精度训练。还有个问题就是 BF16 目前并不是所有的硬件都支持，但是目前越来越多的硬件都开始支持了。</p><p>FP16 则很适合推理，算的快，精度也不错，通信也少。</p><p>当然有钱有资源还是用 FP32，上面都是想省钱的产物，各有优缺点。</p><p><a href="https://mp.weixin.qq.com/s/8KtcgNuafBj2K3_gnzv0aw">华为大模型面试原题：BF16 和 FP16 的区别？</a></p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>笔试面试</category>
      
      <category>AI算法</category>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>笔试面试</tag>
      
      <tag>算法面试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>公众号“看图学”试题合集（2）</title>
    <link href="/2025/02/11/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/AI%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98002%EF%BC%9A%E5%85%AC%E4%BC%97%E5%8F%B7%E2%80%9C%E7%9C%8B%E5%9B%BE%E5%AD%A6%E2%80%9D%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86%EF%BC%882%EF%BC%89/"/>
    <url>/2025/02/11/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/AI%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98002%EF%BC%9A%E5%85%AC%E4%BC%97%E5%8F%B7%E2%80%9C%E7%9C%8B%E5%9B%BE%E5%AD%A6%E2%80%9D%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86%EF%BC%882%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="1-大模型的参数量为什么设计成-7B，13B，33B，65B-等如此怪异的数字？"><a href="#1-大模型的参数量为什么设计成-7B，13B，33B，65B-等如此怪异的数字？" class="headerlink" title="1. 大模型的参数量为什么设计成 7B，13B，33B，65B 等如此怪异的数字？"></a>1. 大模型的参数量为什么设计成 7B，13B，33B，65B 等如此怪异的数字？</h1><h3 id="1-1-从推理出发"><a href="#1-1-从推理出发" class="headerlink" title="1.1 从推理出发"></a>1.1 从推理出发</h3><p>很多答案都是从推理出发，认为之所以这么设计，是为了适配常见的显卡。</p><p>比如,采用半精度的话</p><ul><li>7B 的模型参数占14G, 可以放到16G 的 T4 上</li><li>13B 的模型参数占26G， 可以放到 32G 的 V100 上</li><li>33B 的模型参数占66G， 可以放到 80G 的 A100 上</li><li>65B 的模型参数占130G， 可以放到两张 80G 的 A100 上</li></ul><p>剩余的显存可以用来放 KV Cache， 还有其他的一些功能性显存占用，比如 beam search 等。</p><p><strong>这么回答也算合理，****但是只能算是回答了一个方面，而且不是最重要的方面。</strong></p><h3 id="1-2-沿用GPT3的参数标准"><a href="#1-2-沿用GPT3的参数标准" class="headerlink" title="1.2 沿用GPT3的参数标准"></a>1.2 沿用GPT3的参数标准</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/ac629514f0af77c4aa8ade45c81d7f0c.png" alt="GPT3参数"></p><p>GPT 3 当时选定了 6.7B， 13B， 和 175 B。<strong>后面复现的人得做对比实验吧，那自然要对标 GPT 3</strong>，不然一个 6.7 B，一个 10 B，那对比起来也没什么意义。</p><p>所以这些参数的设定可以说是从 GPT 3 传下来的，因为大家都想和 GPT 3 PK 一下。所以那个时候很多模型都是 7B 和 13 B 左右，但是略有差异，也许是 6B，也许是 14B。</p><p><a href="https://mp.weixin.qq.com/s/y3UUtPB1_fkWr8gfLSN8cQ">字节校招一面：“大模型的参数量为什么设计成 7B，13B，33B，65B 等如此怪异的数字？”</a></p><h1 id="2-为什么-Qwen-设计成-72B？"><a href="#2-为什么-Qwen-设计成-72B？" class="headerlink" title="2. 为什么 Qwen 设计成 72B？"></a>2. 为什么 Qwen 设计成 72B？</h1><p><a href="https://mp.weixin.qq.com/s/7sVvvbaViYJtSL15JeE8XA">校招面试：”为什么 Qwen 设计成 72B？”</a></p><h1 id="3-torch-no-grad-和-torch-inference-mode-的区别？"><a href="#3-torch-no-grad-和-torch-inference-mode-的区别？" class="headerlink" title="3. torch.no_grad() 和 torch.inference_mode() 的区别？"></a>3. torch.no_grad() 和 torch.inference_mode() 的区别？</h1><p><code>torch.no_grad()</code> 和 <code>torch.inference_mode()</code> 都在推理的时候禁用了梯度计算。</p><p>虽然从功能上两者类似，但是这两者的实现有很大的不同。</p><p><code>torch.no_grad()</code> 属于是在 pytorch 原有的机制上禁用了梯度的计算，<strong>底层还是有梯度计算的框架(autograd)，属于可以算，但是逻辑上不进行计算</strong>。</p><p>但是 <code>torch.inference_mode()</code> 的是完全另起炉灶，完全脱离了 autograd 系统，而且把 View Tracking（视图追踪）和 Version Counter Bumps（版本计数器更新）同样砍掉了。<strong>从底层上就完全放弃了梯度计算的逻辑。</strong></p><p>举一个不太恰当的例子，为了追求更快的推理速度，<code>torch.no_grad()</code> 的选择是加入少林寺，远离俗世的一切联系，专心推理，但是还是可以还俗的。而 <code>torch.inference_mode()</code> 则是练了辟邪剑谱，从根本上断绝了俗世的干扰。</p><p>那你说谁的推理速度快，当然是练了辟邪剑谱的更快。</p><h2 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a><strong>代码演示</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>x = torch.tensor([[<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], [<span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">6.0</span>]], requires_grad=<span class="hljs-literal">True</span>)<br>x.retain_grad()<br><br><span class="hljs-keyword">with</span> torch.inference_mode():<br><span class="hljs-comment"># with torch.no_grad():</span><br>    y = x * <span class="hljs-number">2</span><br><br><span class="hljs-comment"># Check Version Counter Bumps, 第一个 fail 的点</span><br><span class="hljs-built_in">print</span>(y._version)  <span class="hljs-comment"># will fail here</span><br><br><span class="hljs-comment"># Check View Tracking, 第二个 fail 的点</span><br>yy = y.view(<span class="hljs-number">6</span>)<br>y.add_(<span class="hljs-number">1</span>)  <span class="hljs-comment"># will fail here</span><br><br><span class="hljs-comment"># Check autograd, 第三个 fail 的点</span><br>y.requires_grad = <span class="hljs-literal">True</span> <span class="hljs-comment"># will fail</span><br></code></pre></td></tr></table></figure><p>上述代码中，如果采用 <code>torch.no_grad()</code>, 则可以正常运行。但是如果采用 <code>torch.inference_mode()</code>, 则在三个检查点都会 fail。</p><p>第一个检查点是 Version Counter Bumps。因为有时候 tensor 会在原地修改以避免内存的拷贝，加快运行速度。所以每个 tensor 都有一个 version。</p><p>第二个检查点是 View Tracking， 这个主要是 tensor 当作一些 视图（view） 上的变化，比如view()、reshape()、transpose() 等，这些虽然呈现为不同的形状，但是底层的内存是共享的。这就要有一个机制来追踪这些视图。</p><p>第三个检查点则是 Autograd。因为 <code>torch.inference_mode()</code> 压根就没有 Autograd 的梯度计算机制，所以当试图修改梯度计算状态的时候也会失败。</p><h1 id="4-model-eval-会像-torch-no-grad-那样停止中间激活的保存么？"><a href="#4-model-eval-会像-torch-no-grad-那样停止中间激活的保存么？" class="headerlink" title="4. model.eval() 会像 torch.no_grad() 那样停止中间激活的保存么？"></a>4. model.eval() 会像 torch.no_grad() 那样停止中间激活的保存么？</h1><p>不会。<code>model.eval()</code> 和梯度的计算是正交的，各算个的，可以认为完全没有任何关系。</p><p>上周的一篇：《<a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247486758&idx=1&sn=8e64261fdf0fb0478d293e8eb1762ab1&chksm=fa6773ffcd10fae917ab0b1c719a7b0ede88feca309504090798eb1348a83517cfd00cb1691f&scene=21#wechat_redirect">学妹问：“model.train() 和 model.eval() 什么作用？” 我给她分享了个bug</a>》发布后，有朋友私信说 <code>model.eval()</code> 是否和 <code>torch.no_grad()</code> 类似，停止中间激活的保存？因为推理也用不到反向传播。</p><p>然而事实是 <code>model.eval()</code> 除了上篇文章中说的适配训练和预测的不一致性以外，再也没有做更多事情了。</p><p>像是停止中间激活的计算，禁用反向传播，节省内存等等，都是<strong>大家根据 eval 这个名字臆想出来功能。****就跟川普要当总统了，然后股民看到“川大智胜”疯狂买入导致涨停是一样的。</strong></p><p>至于 <code>torch.no_grad()</code> 则是将该上下文中的所有变量都不在参与梯度的计算，所以中间激活，梯度都都不需要保存了，自然可以省一些显存。</p><p>但是一定要注意，<code>torch.no_grad()</code> 虽然不计算中间激活和梯度，<strong>但是 autograd 的计算图还是在的。</strong></p><p>当退出 <code>torch.no_grad()</code> 后，后续的代码依然运行在 autograd 的计算图上。如下面的小例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torchviz <span class="hljs-keyword">import</span> make_dot<br><br>x = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], requires_grad=<span class="hljs-literal">True</span>)<br>x.retain_grad()<br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    y = x * <span class="hljs-number">2</span><br><br><span class="hljs-comment"># y.requires_grad = True # 可以赋值</span><br><br>z = (y + <span class="hljs-number">1</span>).requires_grad_()<br>z.retain_grad()<br><br>loss = z.<span class="hljs-built_in">sum</span>()<br>loss.backward()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x是否需要梯度:&quot;</span>, x.requires_grad)  <span class="hljs-comment"># 预期输出: True</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y是否需要梯度:&quot;</span>, y.requires_grad)  <span class="hljs-comment"># 预期输出: False</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;z是否需要梯度:&quot;</span>, z.requires_grad)  <span class="hljs-comment"># 预期输出: True</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x的梯度:&quot;</span>, x.grad)  <span class="hljs-comment"># 预期输出: None</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y的梯度:&quot;</span>, y.grad)  <span class="hljs-comment"># 预期输出: None</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;z的梯度:&quot;</span>, z.grad)  <span class="hljs-comment"># 预期输出: tensor([1., 1., 1.])</span><br><br><span class="hljs-comment"># 使用 make_dot 生成计算图</span><br>dot = make_dot(loss, params=&#123;<span class="hljs-string">&quot;x&quot;</span>: x, <span class="hljs-string">&quot;y&quot;</span>: y, <span class="hljs-string">&quot;z&quot;</span>: z, <span class="hljs-string">&quot;loss&quot;</span>: loss&#125;)<br><br><span class="hljs-comment"># 保存图像</span><br>dot.render(<span class="hljs-string">&quot;autograd_graph&quot;</span>, <span class="hljs-built_in">format</span>=<span class="hljs-string">&quot;png&quot;</span>)<br></code></pre></td></tr></table></figure><p>在退出计算图后，执行 <code>y.requires_grad = True</code> 是可以的，因为整个的 Autograd 体系还在。</p><p>当设置 y 处在 <code>torch.no_grad()</code> 的上下文后，y 之前的梯度都没有了，即使 x 设置了 <code>requires_grad</code> 为 True。</p><p>但是 y 之后的 z 则可以正常的求导。<strong>整个求导过程在 y 就被熔断了</strong>。</p><h1 id="5-model-train-和-model-eval-什么作用？"><a href="#5-model-train-和-model-eval-什么作用？" class="headerlink" title="5. model.train() 和 model.eval() 什么作用？"></a>5. model.train() 和 model.eval() 什么作用？</h1><p>model.train() 会让模型进入 train mode，而 model.eval() 会让模型进入 eval mode。</p><p>为什么要有这两种模式呢？<strong>是因为模型中的有些模块在训练和预测不一致导致的</strong>。典型的模块就是 Batch Norm 和 Dropout。</p><h3 id="Batch-Norm"><a href="#Batch-Norm" class="headerlink" title="Batch Norm"></a><strong>Batch Norm</strong></h3><p>对于 Batch Norm 来说，训练的时候其实并不关心整体样本的均值和方差，我只需要在我这个 batch 内稳定训练就可以了。</p><p>但是预测的时候，我们也必须得找一个均值和方差。那最好的选择就是通过整体的样本来进行估计了。</p><p>但是这里面又有一些细节，比如<strong>原始论文中训练时方差用的是有偏估计，但是推理的时候用的是无偏估计</strong>。这个问题在几年前一直是个讨论的热点。甚至 github 上有个 2017年的 issue，7年了，到现在还没 close。<a href="https://github.com/pytorch/pytorch/issues/1410">https://github.com/pytorch/pytorch/issues/1410</a></p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a><strong>Dropout</strong></h3><p>Dropout 的训练和预测也不一致。</p><p>训练的时候会随机 drop 一些神经元，但是预测的时候则是使用全部的神经元然后进行缩放。</p><p>正是由于这种训练和预测的不一致性，就导致我们必须要告诉模型，什么时候是训练的状态，什么时候是预测的状态。</p><p>我之前还写过一个目前还有印象的bug，就是 model.train() 写了在 for 循环迭代外面，结果 for 循环里面调用了一个评估函数，在评估函数里面会执行 model.eval()，结果就导致 model 基本全程处于 eval 状态。</p><h2 id="适配-train-和-eval"><a href="#适配-train-和-eval" class="headerlink" title="适配 train 和 eval"></a><strong>适配 train 和 eval</strong></h2><p>有时候我们自己写模型，可能也会存在训练和预测不一致的情况，怎么适配 model.train() 和 model.eval() 的接口？</p><p>只需要改变 torch.nn.Module 的 is_training 状态即可，比如下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(SimpleModel, <span class="hljs-variable language_">self</span>).__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.training:<br>            <span class="hljs-keyword">return</span> x * <span class="hljs-number">2</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> x<br><br><br>model = SimpleModel()<br><br>model.train()<br><span class="hljs-built_in">print</span>(model(<span class="hljs-number">107</span>))<br><br>model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-built_in">print</span>(model(<span class="hljs-number">116</span>))<br><br>model.train()<br><span class="hljs-built_in">print</span>(model(<span class="hljs-number">120</span>))<br></code></pre></td></tr></table></figure><p>就是这么简单。</p><h1 id="6-如何防止-Checkpoint-注入代码攻击？"><a href="#6-如何防止-Checkpoint-注入代码攻击？" class="headerlink" title="6. 如何防止 Checkpoint 注入代码攻击？"></a>6. 如何防止 Checkpoint 注入代码攻击？</h1><h3 id="答案"><a href="#答案" class="headerlink" title="答案"></a><strong>答案</strong></h3><p>使用 safetensors 存储和加载。<strong>攻击原理和防范方法还有代码演示见后面</strong>。</p><p>但是<strong>这个方案也只能是防君子不防小人</strong>。只要一个人想干坏事，有太多的方法来实现了。但是对于公司来说，还是提高一些做坏事的门槛比较好。</p><h3 id="攻击原理和防范"><a href="#攻击原理和防范" class="headerlink" title="攻击原理和防范"></a><strong>攻击原理和防范</strong></h3><p>稍微说一下利用 checkpoint 进行攻击的原理。</p><p>其实也并不是什么特别高明的技术，核心就是 <strong>pytorch 的开发人员偷懒，在保存和加载模型的时候，采用了 pickle 格式，而 pickle 格式本身就是不安全的</strong>。</p><p>更具体一点来说，pickle 设计的初衷是为了方便的序列化和反序列化数据。而为了方便用户自定义自己的序列化方式，开放了一个 <code>__reduce__</code> 的接口，而这个 <code>__reduce__</code> 接口则可以让用户为所欲为，如果有足够的权限，用户甚至可以在里面执行 <code>rm -rf /</code>。</p><p>所以后来 huggingface 推出了 safetensors 的格式。这个格式就是一份数据，不能执行代码。而且优化了加载速度，还可以在不加载权重的情况下就获取数据的 meta 信息，比如模型的网络结构等。所以以后大家尽量用 safetensors 就好了。</p><p>下面的代码，加载模型后进行代码注入(<strong>这里以让系统 echo hello world 为例，如果load模型后系统打印了Hello World，则表示攻击cheng g</strong>)，然后使用使用 pytorch 来存储和加载模型，可以发现用torch.load 被注入代码的模型后，打印了 Hello world。而采用 safetensors 则没有任何问题。</p><p>为了防止攻击代码的恶意扩散，下面的代码中具体的注入方式被隐藏。想学习完整代码可以看文末的付费专栏，日后要是惹出祸事，不要说是我教的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> safetensors.torch <span class="hljs-keyword">import</span> load_model, save_model<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.linear1 = nn.Linear(<span class="hljs-number">107</span>, <span class="hljs-number">116</span>)<br>        <span class="hljs-variable language_">self</span>.linear2 = nn.Linear(<span class="hljs-number">116</span>, <span class="hljs-number">120</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.linear1(x)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.linear2(x)<br><br><span class="hljs-comment"># 创建模型实例</span><br>model = SimpleModel()<br><br><span class="hljs-comment"># 正常保存模型</span><br>torch.save(model, <span class="hljs-string">&#x27;simple_model.pth&#x27;</span>)<br><br><span class="hljs-meta">... </span>注入 os.system(<span class="hljs-string">&#x27;echo Hello World&#x27;</span>)。<br><span class="hljs-meta">... </span>注入为了防止该方法恶意扩散，这里省略具体的注入方法。<br><br><span class="hljs-comment"># 加载原始模型</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;simple_model.pth&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    original_model = torch.load(f)<br><br><span class="hljs-comment"># 插入恶意代码</span><br>original_model ... 为了防止该方法恶意扩散，这里省略具体的注入方法。<br><br><span class="hljs-comment"># 保存被污染的模型</span><br>torch.save(original_model, <span class="hljs-string">&#x27;poisoned_model.pth&#x27;</span>)<br>save_model(original_model, <span class="hljs-string">&#x27;safe_model.safetensor&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;load by torch.load...&#x27;</span>)<br>poisoned_model = torch.load(<span class="hljs-string">&#x27;poisoned_model.pth&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;load by safetensors...&#x27;</span>)<br>load_model(original_model, <span class="hljs-string">&#x27;safe_model.safetensor&#x27;</span>)<br></code></pre></td></tr></table></figure><h1 id="7-分布式训练常用的通信后端都有什么？应该怎么选？"><a href="#7-分布式训练常用的通信后端都有什么？应该怎么选？" class="headerlink" title="7. 分布式训练常用的通信后端都有什么？应该怎么选？"></a>7. 分布式训练常用的通信后端都有什么？应该怎么选？</h1><p>目前最流行的深度学习框架当属 Pytorch ，Pytorch 支持3个通信框架：MPI，Gloo，NCCL。</p><p>但是也有很多自研的框架，比如阿里的 ACCL, 微软的MSCCL, Intel 的 oneCCL， AMD 的 RCCL， 华为的 HCCL 等。</p><p>通过名字大概可以看出，这些<strong>通信框架大致分为两类， MPI 和 xCCL。</strong></p><p>MPI 的全称是 Message Passing Interface。而 CCL 的全称是 Communication Collectives Library。然后前面还会加一个代号，来区分是哪家公司的。比如 NCCL 就是 Nvidia Communication Collectives Library， 阿里的就是 Alibaba Communication Collectives Library，也有不是公司名的，比如 Intel 和 AMD 分别用了 one 和 ROCm。</p><p>下面分别简单介绍一下这些框架。</p><h3 id="MPI"><a href="#MPI" class="headerlink" title="MPI"></a><strong>MPI</strong></h3><p>经典的分布式通信框架。当年在百度的时候，经常用 MPI 版的逻辑回归。那个时候刚出校门，没见过世面，感觉分布式训练贼牛逼。</p><p>在 NCCL 出现之前，MPI 在 CPU 和 GPU 分布式框架中都占据主导地位。不过自从 NCCL 出现后， MPI 目前只用在 CPU 的通信场景中了。</p><h3 id="Gloo"><a href="#Gloo" class="headerlink" title="Gloo"></a><strong>Gloo</strong></h3><p>Gloo 是 Facebook 的开源框架。对 CPU 和 GPU 的通信都做了一些优化。早期的时候，优化的算法不太多，现在也加了很多优化算法，具体见：</p><p><a href="https://github.com/facebookincubator/gloo/blob/main/docs/algorithms.md">https://github.com/facebookincubator/gloo/blob/main/docs/algorithms.md</a></p><p>但是在 GPU 上，效果依然没有 NCCL 好，所以目前只要用 Nvidia 的 GPU，基本上都会把通信后端设置为 NCCL。</p><h3 id="NCCL"><a href="#NCCL" class="headerlink" title="NCCL"></a><strong>NCCL</strong></h3><p>NCCL 是英伟达开发的。当时的起因是 MPI 虽然也对 GPU 做了优化，但是并没有完全发挥出性能，所以英伟达亲自下场，在传输和计算的 overlap 上做了很多优化工作。</p><p>一经发布，就成了业界扛把子。只要用 Nvidia 的卡，用 NCCL 就行。</p><p>NCCL 会自动根据网络拓扑和通信协议来自动选择算法，比如 Ring based, Tree based 或者 CollNet，选择一个最快的来运行。</p><p>体感上来看，NCCL 在最开始通信的时候由于有一些前置步骤，所以第一次通信的时候很慢，但是只要开始通信之后，就非常快了。所以测速度的时候千万不要把第一次通信的时间当作结果。</p><p>英伟达也并没有想着要取代 MPI，而是采用了共存共荣的策略，只在 GPU 上优化。</p><h3 id="MSCCL"><a href="#MSCCL" class="headerlink" title="MSCCL"></a><strong>MSCCL</strong></h3><p>MSCCL 是微软开发的，可以认为 NCCL 的一个扩展。关键是可以兼容 NCCL 的 API，pytorch 只需要将 backend 做一下替换即可。</p><p>MSCCL 提供了更灵活和可定制的集体通信算法，引入了一个 chunk-oriented dataflow language， 叫 DSL。同时还有个编译器，用来编译和优化数据如何在 GPU 之间进行流动。这个工作还是比较硬核的，光编译原理估计大部分人就看不懂。</p><p>根据他们的测试报告，MSCCL 在推理上可以加速1.22x–1.29x， MoE 的训练可以加速 1.10x–1.89x 。</p><h3 id="ACCL"><a href="#ACCL" class="headerlink" title="ACCL"></a><strong>ACCL</strong></h3><p>阿里针对阿里云的环境进行的优化。其官网上是这么写的：</p><ul><li>修复了对应NCCL社区开源版本的BUG；</li><li>对集合通信不同算子和不同消息区间进行了调优，使其相比开源NCCL拥有更好的性能；</li><li>支持训练过程中集合通信统计分析，可用于诊断训练过程中设备故障导致的计算&#x2F;通信Slow（慢）和Hang（挂起）等问题，配合阿里云PAI的AIMaster：弹性自动容错引擎和C4D：模型训练任务问题诊断工具，可以快速的进行任务的异常检测和自动容错；</li><li>支持多路径传输和负载均衡功能，在训练集群中降低甚至消除哈希不均导致的拥塞问题，提升整体训练性能；</li></ul><p><a href="https://help.aliyun.com/zh/pai/user-guide/accl-alibaba-high-performance-collective-communication-library">https://help.aliyun.com/zh/pai/user-guide/accl-alibaba-high-performance-collective-communication-library</a></p><h3 id="HCCL"><a href="#HCCL" class="headerlink" title="HCCL"></a><strong>HCCL</strong></h3><p>华为开发的，基于昇腾硬件的高性能集合通信库。现在很多公司已经在采购昇腾的卡了。</p><p>现在老美一直打压我们，那天要是 Nvidia 的卡不让用了，只能用这个了。</p><h3 id="oneCCL-RCCL"><a href="#oneCCL-RCCL" class="headerlink" title="oneCCL&#x2F;RCCL"></a><strong>oneCCL&#x2F;RCCL</strong></h3><p>oneCCL 是 Intel 开发的，RCCL 是 AMD 开发的。</p><p>看这两家的背景就知道，这两个库在更底层上进行优化，所以对InfiniBand、以太网有很好的支持。</p><h2 id="到底用什么"><a href="#到底用什么" class="headerlink" title="到底用什么"></a><strong>到底用什么</strong></h2><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739262190331-6.png" alt="img"></p><p>之前有人对 MPI， NCCL 和 Gloo 做了测评，结论是：</p><ol><li>当 tensor 比较小时，MPI 的性能更好，而且集群数量越大，MPI 效果越好。</li><li>当 tensor 比较大时，NCCL 性能最好，而且跟集群没有关系。</li></ol><p>最终的结论就是：</p><ol><li>用什么型号的 GPU，就用对应的 CCL。比如 N 卡就用 NCCL，阿里的就用 ACCL，昇腾的就用 HCCL。</li><li>如果是 CPU，最简单的就用 Gloo。爱折腾就用 oneCCL 或者 RCCL。</li></ol><p>当然 pytorch 官方也给出了使用建议，请看：<a href="https://pytorch.org/docs/stable/distributed.html#which-backend-to-use">https://pytorch.org/docs/stable/distributed.html#which-backend-to-use</a></p><h1 id="8-Beam-Search-的缺点？"><a href="#8-Beam-Search-的缺点？" class="headerlink" title="8. Beam Search 的缺点？"></a>8. Beam Search 的缺点？</h1><p>Beam Search 大大提升了推理的速度，但是其自身也有很多缺点。下面分别说明</p><h3 id="缺点1-Beam-Search-本身并不能保证找到最优解。"><a href="#缺点1-Beam-Search-本身并不能保证找到最优解。" class="headerlink" title="缺点1: Beam Search 本身并不能保证找到最优解。"></a><strong>缺点1: Beam Search 本身并不能保证找到最优解。</strong></h3><p>当 beam size 为1时， Beam Search 就退化为 Greedy Search，当 beam size 趋向于无穷时，则变成暴力穷举，这个时候才能保证最优解。</p><p>当 beam size 变大时，<strong>找到最优解的概率会提升，但是收益是递减的</strong>。</p><h3 id="缺点2-Beam-Search-趋近完美且中庸，但没有惊喜"><a href="#缺点2-Beam-Search-趋近完美且中庸，但没有惊喜" class="headerlink" title="缺点2: Beam Search 趋近完美且中庸，但没有惊喜"></a><strong>缺点2: Beam Search 趋近完美且中庸，但没有惊喜</strong></h3><p>之前有人问余华 AI 是否对作家构成威胁。余华老师估计不太懂 AI 的底层原理，但是他却给出了一个非常接近本质的结论。</p><p>余华认为：<strong>生活是不按常理出牌的，AI 写作可以写出中庸的小说，但写不出个性的小说</strong>。人脑总要犯错误，用人脑写作的“伟大文学作品都有败笔”，但这也是人脑最可贵之处。</p><p>需要注意：中庸并不是一个贬义词，讲究的是不偏不倚，折中调和。</p><p>通过对人类文本和 Beam search 生成的文本的困惑度进行对比，也验证了这一点：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739262337169-8.png" alt="img"></p><p>所以 AI 生成的文本往往比较枯燥，没有带来惊喜。甚至在某些情况下，会陷入  positive feedback loop， 倾向于重复的输出一些高概率的词。</p><h3 id="缺点3-Beam-Search-对长序列不友好"><a href="#缺点3-Beam-Search-对长序列不友好" class="headerlink" title="缺点3: Beam Search 对长序列不友好"></a><strong>缺点3: Beam Search 对长序列不友好</strong></h3><p>由于 Beam Search 的概率是累乘的，由于概率又小于1，所以随着长度的增加，句子的概率会越来小。</p><p>这也就造成一个长度为 5 的句子的概率，天然就大于长度为100 的概率。所以 Beam Search 更喜欢短句子。</p><p>通常需要对 Beam Search 进行 Length Normalization。</p><h3 id="缺点4-Beam-Search-会耗费额外的资源"><a href="#缺点4-Beam-Search-会耗费额外的资源" class="headerlink" title="缺点4: Beam Search 会耗费额外的资源"></a><strong>缺点4: Beam Search 会耗费额外的资源</strong></h3><p>尤其是当 beam size 很大和序列长度很大的时候，beam search 会耗费不少的内存。比如 KV Cache 的存储。</p><p>当然可以通过 Trie 树等来进一步优化。</p><h1 id="9-Beam-Search-最坏时间复杂度是多少？"><a href="#9-Beam-Search-最坏时间复杂度是多少？" class="headerlink" title="9. Beam Search 最坏时间复杂度是多少？"></a>9. Beam Search 最坏时间复杂度是多少？</h1><h3 id="答案-1"><a href="#答案-1" class="headerlink" title="答案"></a><strong>答案</strong></h3><p>Beam Search 相信大家都知道怎么回事，这里不再赘述。本文探究一下 Beamsearch 的时间复杂度。</p><p>假设大模型的词表个数为|V|,我们要预测  个 token，beam search 的 beam size 为k, 那么beam search 的时间复杂度是多少呢？</p><p>我们先看下 beam search 的搜索路径，如下图所示(beam size &#x3D; 2)：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/537ada0f171fe620741015f4c00fa22c.png" alt="537ada0f171fe620741015f4c00fa22c">第一个 token 时从|V|中选择 k个概率最大的 token ，剩下的都是从 k|V|个候选 token 选择k个概率最大的。</p><p>从N个数字中选择k个最大的数，经典的 topk 问题。</p><p>top k 问题也经常作为算法面试题出现，但是能完全答对的真不多。这里给出 4 个选项:<br>$$<br>\begin{aligned}&amp;\bullet\text{ А.}O(N\log N)\&amp;\bullet\text{ В.}O(N\log k)\&amp;\bullet\text{ С.}O(N)\&amp;\bullet\text{ D.}O(N^2)\end{aligned}<br>$$<br>可以先选择一下再给答案。评论区可以发一下自己的答案:)</p><p>下面分别说明</p><ul><li>O(NlogN)</li></ul><p>这个时间复杂度肯定可以完全解决，只需要将 数组排序，然后选择最大的 k 个即可,排序的时间复杂度为 O(NlogN)。但是并不是最优的。少数排序算法可以达到O(N)，但是ROI不一定实用。</p><ul><li>O(Nlogk)</li></ul><p>可以维护一个大小为 k 的小顶堆，然后遍历 N 个数，每次更新小顶堆的时间复杂度为 logk,整体为Nlogk. 最后堆内的元素就是答案。</p><ul><li>O(N)</li></ul><p>参考 quicksort，每次选一个 pivot 进行重新排列。但是我每次并不 sort，而是 select，把问题变成 quick select.</p><p>只要我知道了第 k 大的数字，再 O(N) 的遍历一遍，就得到了 TopK 的数字了。</p><p>N个数一次 parition 的时间复杂度为 O(N), 下一次处理的数量期望是当前数量的一半。所以整体期望的时间复杂度为 $O(N)+O(N&#x2F;2)+O(N&#x2F;4)+\ldots&#x3D;O(2N)&#x3D;O(N)$</p><p>但是注意，这只是期望是 O(N).</p><p>如果下一次处理的数量并不是一半，而是只少了一个，那么最坏的时间复杂度则变成 .</p><p>其实回答到这里，基本就可以了，下面的是加分项。</p><p>但是实际上，quick select 可以进一步转化为使用 Median of medians 算法来求解 TopK。</p><p>而 Median of medians 算法是一个真正的线性算法，可以最坏 O(N)的时间复杂度来找到第 k 大的元素。</p><p>虽然理论上， Median of medians 的最坏时间复杂度是O(N),但是<strong>它的渐近常数有点大，实际使用中其实和 quick select 差异并没有那么大</strong>。它的意义就是<strong>保证了 TopK 的最坏时间复杂度为线性</strong>。</p><p>Median of medians 算法也很简单，这里就不写了，感兴趣的可以看下面的付费资料。</p><h3 id="Beam-Search-的时间复杂度"><a href="#Beam-Search-的时间复杂度" class="headerlink" title="Beam Search 的时间复杂度"></a><strong>Beam Search 的时间复杂度</strong></h3><p>回到 Beam Search 的问题上，由于每次是从k|V|个元素中选择 k 个，这种选择进行了 n 次，<strong>所以 Beam Search 的时间复杂度为</strong>O(nk|V|)</p><p>当k&#x3D;1的时候, 退化为 greed search。</p><p>当$k&#x3D;\infty $  的时候，不能直接带入到式子里面，因为k 虽然可以无限，但是刚开始的搜索步骤还是有限制的。这时候相当于 全局搜索，时间复杂度为 $|V|^n$</p><h1 id="10-为什么-sigmoid-采用的是-e-的负-x-次方，而不是别的数的负-x-次方？"><a href="#10-为什么-sigmoid-采用的是-e-的负-x-次方，而不是别的数的负-x-次方？" class="headerlink" title="10. 为什么 sigmoid 采用的是 e 的负 x 次方，而不是别的数的负 x 次方？"></a>10. 为什么 sigmoid 采用的是 e 的负 x 次方，而不是别的数的负 x 次方？</h1><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739263333379-12.png" alt="img"></p><p>不同 k 值的函数图像如下图所示：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/28d16e38349c6afee2a7ee7dd79d9912.png" alt="28d16e38349c6afee2a7ee7dd79d9912"></p><p>看到这里我大概意识到<strong>不同的 k 只是对 sigmoid 做了不同的拉伸</strong>，其实都属于 S 形函数。</p><p><strong>这类函数不仅函数相似，其导数也高度相似</strong>。然后我决定推导一下 g(x) 的导数。如下：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739263467021-28.png" alt="img"></p><p>联想到 sigmoid 的导数为</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739263459253-25.png" alt="img"></p><p>好像知道答案了。g(x) 的导数可以继续化简，为：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739263445888-22.png" alt="img"></p><p>所以说导数这里有个 ln k 的常数。</p><p>一是为了计算的简便，当 k &#x3D; e 时，这一项就没有了。</p><p>二是联想到 ResNet 的导数，里面如果含有一个常数项，累计效应会造成训练的不稳定，具体的推导可以看：<a href="http://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247486359&idx=1&sn=f1fa3f9ffad87683bcc5d6f56fe0a389&chksm=fa67754ecd10fc58f65dee2cb3f6708afffec46ea38cdef4f1e222d3b9cba13b33af940260e7&scene=21#wechat_redirect">字节大模型一面：“为什么现在深度学习都用 ResNet?”</a></p><p>我觉得这应该是 k 取 e 的原因吧。</p><p>晚上吃完饭又想了想，这个 ln k 的常数是怎么出来的呢？突然我一拍大腿，发现其实上面的证明有点绕远路了。</p><p>因为$k&#x3D;e^{\ln k}$ , 然后</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739263437386-19.png" alt="img"></p><p>所以 g(x) 依然是 sigmoid.</p><p>根据链式法则，</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739263428022-16.png" alt="img"></p><p>这就是 ln k 常数的由来。</p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>笔试面试</category>
      
      <category>AI算法</category>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>笔试面试</tag>
      
      <tag>算法面试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>公众号“看图学”试题合集（1）</title>
    <link href="/2025/02/10/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/AI%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98001%EF%BC%9A%E5%85%AC%E4%BC%97%E5%8F%B7%E2%80%9C%E7%9C%8B%E5%9B%BE%E5%AD%A6%E2%80%9D%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86%EF%BC%881%EF%BC%89/"/>
    <url>/2025/02/10/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/AI%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98001%EF%BC%9A%E5%85%AC%E4%BC%97%E5%8F%B7%E2%80%9C%E7%9C%8B%E5%9B%BE%E5%AD%A6%E2%80%9D%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86%EF%BC%881%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="1-如何让大模型输出合法的Json格式"><a href="#1-如何让大模型输出合法的Json格式" class="headerlink" title="1.如何让大模型输出合法的Json格式"></a>1.如何让大模型输出合法的Json格式</h1><h3 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a><strong>后处理</strong></h3><p>最容易想到的当然是重试机制，在 Prompt 中要求 LLM 输出 json，拿到 LLM 的完整输出，判断是否是合法的 json。如果不是，则再重新生成一遍。</p><p>当然这里也有优化空间，比如可以通过 json parser 来判断解析到哪里出错了，重试的时候不需要从头输出了，而只需要从出错的地方往后输出即可。</p><p>比如 strict-json 库就采用的这种方法。</p><p>这属于后处理的方法，最容易实现，但是逼格似乎不太够，而且有点费钱。</p><h3 id="约束解码"><a href="#约束解码" class="headerlink" title="约束解码"></a><strong>约束解码</strong></h3><p>另一种复杂一点的方法就是约束解码，保证一次性生成合法的 json。</p><p>约束解码就是在生成下一个 token 的时候，对词表进行一次筛选，将词表符合约束的 token 留下，其他的 token 都 mask 掉，这样 LLM 在 generate 的时候，每一个时刻的 response 都是符合实现约束的条件的。</p><p>举一个例子来说，我们定义了输出的格式为 json，那么第一个字符就必须是 <code>&#123;</code> 符号，词表中其他的 token 都被 mask 掉了。</p><p>那第二个字符合法的有, 引号<code>&quot;</code>,空白字符还有 <code>&#125;</code>，其余的符号都被 mask 掉了。剩余的步骤也一样，直到生成完整的 json 就停止。</p><p>上面说的是比较简单的情况，有时候还需要约束 json 符合固定的格式，比如第一个 key 是 name，第二个 key 是 age 之类的。这个时候就要采用一些 Grammar Engine 来负责这些 schema 的处理。</p><p>其流程如下：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640" alt="约束解码"></p><p>示意代码如下（暂不考虑 kvcache）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">constrained_decode</span>(<span class="hljs-params">input_x, constraint_fn C, max_length</span>):<br>    <span class="hljs-comment"># 刚开始输出为空</span><br>    output = []<br><br>    <span class="hljs-comment"># 开始生成</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(output) &lt; max_length:<br>        <span class="hljs-comment"># C 为 Grammar Engine，解析当前的输出</span><br>        C.update(output)<br><br>        <span class="hljs-comment"># 计算 mask</span><br>        mask = C.mask()<br><br>        <span class="hljs-comment"># 下一个 token 的 logits</span><br>        logits = model_forward(input_x + output)<br><br>        <span class="hljs-comment"># 将不符合约束的 token mask 掉</span><br>        masked_logits = logits * mask<br><br>        <span class="hljs-comment"># 剩下的和正常的解码一样</span><br>        probs = softmax(masked_logits)<br>        next_token = sample_token(probs)<br><br>        output.append(next_token)<br><br>        <span class="hljs-keyword">if</span> next_token == EOS_TOKEN:<br>            <span class="hljs-keyword">break</span><br><br>    <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><p>通过上面图示和代码可以很清晰的看到约束生成的大概框架。</p><p>目前已经有很多框架支持约束解码，比如 Guidance (Guidance AI, 2023), Outlines (Willard &amp; Louf, 2023), XGrammar (Dong et al., 2024) and the grammar module of Llamacpp (Gerganov &amp; al., 2023)。</p><p>这些框架在这个基本框架上进一步做了很多优化，比如 mask 的计算和 LLM forward 的流程是可以并行的，然后 Grammer Engine 的处理通常比较耗时，可以加入缓存来提高重复或者相似的语法结构的处理，也有提前计算 mask 以达到加速的目的，因为 mask 可以在 cpu 上计算，对于算力的占用没有那么紧张。</p><p>Grammar Engine 内部的一些优化细节就涉及到编译原理了，会有很多自动机相关的研究，这里就不再展开了。</p><p>在论文 《Generating Structured Outputs from Language Models: Benchmark and Studies》 中，作者比较了上面几种常用的框架，结果如下：</p><p>- GCT 是语法编译的时间（Grammar Compilation Time）</p><p>- TTFT 是第一个 token 产出的时间（Time to First Token）</p><p>- TPOT 是产生第一个 token 之后，每个 token 的平均生成时间（Time per Output Token (TPOT)）</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640.png" alt="框架比较"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739151842011-3.png" alt="框架比较"></p><p><a href="https://mp.weixin.qq.com/s/IbYjUlS1gN8vJ5doZpyVbA">Deepseek一面：“如何让大模型输出合法的 Json 格式？”</a></p><h1 id="2-FIM-Fill-in-the-Middle-的原理是什么？"><a href="#2-FIM-Fill-in-the-Middle-的原理是什么？" class="headerlink" title="2. FIM (Fill in the Middle) 的原理是什么？"></a>2. FIM (Fill in the Middle) 的原理是什么？</h1><p>假设有这么一个填空题：白日依山尽，_ _ 入海流。</p><p>在 Bert 的 Encoder 主导的时代，注意力是双向的，所以可以非常方便的填充中间 mask 的内容。</p><p>后来到了 GPT 的 Decoder only 的时代，token 只能从左往右生成，就没法填充中间的内容了。</p><p>然而填充中间内容的需求一直是存在的，比如有一段代码，想在函数名和函数体中间生成注释，这个时候按照 GPT 类似的 Decoder 的默认方法，就没填充，或者只能根据函数名进行填充。</p><p>OpenAI 的研究人员想了个好办法，在论文 《Efficient Training of Language Models to Fill in the Middle》中，提出了 Fill in the Middle 的方法。方法很简单，但是效果却异常的好。</p><p>其思想就是，<strong>既然没法改变 Decoder 的流程，那就改变数据的顺序</strong>。</p><p>比如“白日依山尽，黄河入海流”，采用 FIM 的方法进行训练的时候，可以表示成下面这样：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">PRE</span>&gt;</span>白日依山尽，<span class="hljs-tag">&lt;<span class="hljs-name">SUF</span>&gt;</span>入海流<span class="hljs-tag">&lt;<span class="hljs-name">MID</span>&gt;</span>黄河<span class="hljs-tag">&lt;<span class="hljs-name">EOM</span>&gt;</span><br></code></pre></td></tr></table></figure><p>这样的话，模型就可以根据 special token 来预测中间的“黄河” 两个字。其中 <code>&lt;Pre&gt;</code> 代表前缀，<code>&lt;SUF&gt;</code> 代表后缀，<code>&lt;MID&gt;</code> 表示中间填充的开始，<code>&lt;EOM&gt;</code> 表示中间填充的结束。</p><p>预测的时候只需要输入</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">PRE</span>&gt;</span>白日依山尽，<span class="hljs-tag">&lt;<span class="hljs-name">SUF</span>&gt;</span>入海流<span class="hljs-tag">&lt;<span class="hljs-name">MID</span>&gt;</span><br></code></pre></td></tr></table></figure><p>然后模型就会继续往后预测，直到遇到 <code>&lt;EOM&gt;</code> 符号。</p><p>OpenAI 同时发现，在预训练中引入这样的机制后，对于原来的模型性能几乎没有影响。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739153523841-6.png" alt="Fill in the Middle"></p><p>OpenAI 把这个现象称作 <strong>FIM-for-free property</strong>。</p><p>论文发出以后，基本上所有的 Code 相关的模型，都引入了这个机制。所以很多 copilot 调用的模型都具备中间代码补全的能力。</p><p>到了今天，基本上所有模型的预训练阶段都会加入 FIM。</p><p><a href="https://mp.weixin.qq.com/s/-X1WGsPWnS_dKYDSDzFnkw">百度大模型一面：“FIM (Fill in the Middle) 的原理是什么？有没有用过 Copilot？”</a></p><h1 id="3-大模型推理-repetition-penalty-是如何实现的？"><a href="#3-大模型推理-repetition-penalty-是如何实现的？" class="headerlink" title="3.大模型推理 repetition penalty 是如何实现的？"></a>3.大模型推理 repetition penalty 是如何实现的？</h1><p>大模型推理的时候，一般都有一个 <code>repetition_penalty</code> 的参数，看字面意思可以知道是加入了重复惩罚。但是具体是怎么实现的呢？</p><p>这个方法最早来源于 CTRL: A Conditional Transformer Language Model For Controllable Generation。</p><p>在 Sampling 那一章，作者提出了这么一个思路:</p><p>既然不想生成重复的内容，那就在预测下一个 token 的时候，<strong>让已经出现的 token 的预测概率变小就可以了。</strong></p><p>用公式表达如下：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210101113070.png" alt="公式"></p><p>其中  I(c) &#x3D;θ if c is True else 1， g 则是已经出现的 token。</p><p>θ 就是 repetition penalty 的大小。</p><p>也就是说，对于出现的 token，会让其 logits 变小。极限一点的话，可以让 input_ids 中的 token 再也没法出现。</p><p>具体实现也不难，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RepetitionPenaltyLogitsProcessor</span>(<span class="hljs-title class_ inherited__">LogitsProcessor</span>):<br>    <span class="hljs-string">r&quot;&quot;&quot;</span><br><span class="hljs-string">    [`LogitsProcessor`] enforcing an exponential penalty on repeated sequences.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        repetition_penalty (`float`):</span><br><span class="hljs-string">            The parameter for repetition penalty. 1.0 means no penalty. See [this</span><br><span class="hljs-string">            paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, penalty: <span class="hljs-built_in">float</span></span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(penalty, <span class="hljs-built_in">float</span>) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> (penalty &gt; <span class="hljs-number">0</span>):<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;`penalty` has to be a strictly positive float, but is <span class="hljs-subst">&#123;penalty&#125;</span>&quot;</span>)<br><br>        <span class="hljs-variable language_">self</span>.penalty = penalty<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, input_ids: torch.LongTensor, scores: torch.FloatTensor</span>) -&gt; torch.FloatTensor:<br>        score = torch.gather(scores, <span class="hljs-number">1</span>, input_ids)<br><br>        <span class="hljs-comment"># if score &lt; 0 then repetition penalty has to be multiplied to reduce the previous token probability</span><br>        score = torch.where(score &lt; <span class="hljs-number">0</span>, score * <span class="hljs-variable language_">self</span>.penalty, score / <span class="hljs-variable language_">self</span>.penalty)<br><br>        scores.scatter_(<span class="hljs-number">1</span>, input_ids, score)<br>        <span class="hljs-keyword">return</span> scores<br></code></pre></td></tr></table></figure><p>从代码可以清晰的看到，首先使用 <code>torch.gather(scores, 1, input_ids)</code> 获取到 input_ids 中的 logits，然后修改其 logits 的数值。</p><p>这里有个细节就是对于 小于 0 的数，是乘以 self.penalty, 否则是除以 self.penalty， 这样就让 input_ids 的 logits 统一都变小了，从而达到了避免重复的问题。</p><p>其他的参数比如 <code>no_repeat_ngram_size</code>，可以控制不重复的 ngram 长度，比如 <code>no_repeat_ngram_size = 3</code>， 就不会生成包含重复的 3-gram的序列。</p><p>但是需要注意，一旦总是出现那种不太正常的重复，比如无论怎么调节generate 的参数，输出总是一个词，这种情况大概率是模型用错了。</p><p>比如之前我测试某个模型，就因为使用了一个旧版本的 transformers，然后模型就一直重复，改成 <code>trust_remote_code = True</code> 就可以了。当然也可以升级 Transformers 库。</p><p><a href="https://mp.weixin.qq.com/s/k6SMaKrAtcZkdwO7trJ7Tw">字节实习面试：“大模型推理 repetition penalty 是如何实现的？”</a></p><h1 id="4-给一个-14B-的模型和-20B-token-的数据，在-32张-A100-上要训练多少时间？"><a href="#4-给一个-14B-的模型和-20B-token-的数据，在-32张-A100-上要训练多少时间？" class="headerlink" title="4.给一个 14B 的模型和 20B token 的数据，在 32张 A100 上要训练多少时间？"></a>4.给一个 14B 的模型和 20B token 的数据，在 32张 A100 上要训练多少时间？</h1><p>根据之前 OpenAI 的 Scaling Law 的论文，里面对 Transformers 的计算量进行了估算，大概的过程是这样的：</p><p>假设</p><ul><li>C 是训练的总 FLOPs (floating point operations)</li><li>N 为模型的参数量</li><li>D 为训练的 token 量。</li></ul><p>Transformers 里绝大多数运输都是矩阵相乘，矩阵相乘的运算量是矩阵大小的两倍，因为<strong>对于矩阵中的每个元素来说，需要一次乘法和一次加法</strong>。</p><p>先说前向运算，每一个 token 都会在模型里运算一遍，所以对于一个 token 来说，前向运算的 FLOPs 为 2N。</p><p>那么对于 D 个 token 来说，前向运算的 FLOPs 为 2ND。</p><p>而神经网络的反向传播的 FLOPs 量，大致为前向运算的 2倍，共计 4ND。原理可以看：<a href="https://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247486124&idx=1&sn=c9294e6696047bb616e0f640fca3ff5f&scene=21#wechat_redirect">学妹问：“反向传播的计算量是前向传播计算量的几倍？”</a></p><p>所以，最终 C &#x3D; 6ND。</p><p><strong>当然这只是个大概的估计，实际上运算量会更多一些</strong>，因为还有一些非矩阵的运算。如果为了节省显存，采用了一些比如 Activation recomputation 等，计算量会显著增加。</p><p>知道了总的运算量，下一步就是查一下显卡的运算速度，就能得出题目的答案。</p><p>对于 A100 来说，FP16 的峰值运算速度为 312 TFLOPS (Tera Floating Point Operations Per Second), 也就是 $312\times10^{12}$</p><p>那么对于前面的任务来说，需要的时间为</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210110458915.png" alt="时间"></p><p>上面的单位是秒，换算成天为 1.95 天。</p><p>但是注意这里只是最理想的情况下，实际上运算量比这个大，而且对于大模型的训练来说，<strong>根本无法达到 A100 的峰值运算速度</strong>，因为有大量的通信，而且内部运算也并不全是 FP16 的计算。</p><p>我们看看 llama 当时的训练数据：</p><p>When training a 65B-parameter model, our code processes around 380 tokens&#x2F;sec&#x2F;GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.</p><p>带入上面的公式:</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210110526113.png" alt="时间"></p><p>换算成天数为 9.89 天，但是实际上用了 21 天。从这里可以推断出，他们的 GPU 的平均运算速度为 146.94 TFLOPS, MFU（Model FLOPs Utilization， 算力的利用率）为 47.1%</p><p>而且模型越大，MFU 越低，比如 Llama 3 405B的训练数据如下图， 最高的 MFU 也就 43%：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739156742913-9.png" alt="LLAMA3"></p><p>这已经是业界几乎最顶尖的水平了。</p><p>所以我们按照业界的顶尖水平来估算的话，最开始问题的答案修正为 4.135 天。</p><p><a href="https://mp.weixin.qq.com/s/kCQJwlDIeSfRnLra0JreeA">小米一面：“给一个 14B 的模型和 20B token 的数据，在 32张 A100 上要训练多少时间？”</a></p><h1 id="5-手撕-MHA，阿里的一面问的真是太细了"><a href="#5-手撕-MHA，阿里的一面问的真是太细了" class="headerlink" title="5. 手撕 MHA，阿里的一面问的真是太细了"></a>5. 手撕 MHA，阿里的一面问的真是太细了</h1><p>这个在面试的时候经常会问到， 但是涉及到的细节比较多。</p><p>核心的代码其实并不多，下面结合代码来说一下。</p><p>代码大概 10 行，实际 11行。10行也不是不行，但是为了可读性，还是11行把。</p><p>首先，要将输入 x 做3个线性映射得到 QKV</p><figure class="highlight gml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gml">q, k, v = <span class="hljs-symbol">self</span>.q_proj(<span class="hljs-variable language_">x</span>), <span class="hljs-symbol">self</span>.k_proj(<span class="hljs-variable language_">x</span>), <span class="hljs-symbol">self</span>.v_proj(<span class="hljs-variable language_">x</span>)<br></code></pre></td></tr></table></figure><p>因为要实现 multi attention， 所以要将最后一维切割。比如本来是 (4， 512， 128) 大小的矩阵， 现在有 8个头， 最后一维被切成8块， 变成了 一个 (4, 512, 8, 16) 的矩阵。</p><p>但是现在序列长度 512在矩阵的位置是第二位， 我们要保持 512 这一维和 16 这一维在一起，然后做 attention， 所以还需要做一个 view 的变换， 代码如下</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">k = k<span class="hljs-selector-class">.view</span>(B, T, self<span class="hljs-selector-class">.n_head</span>, C <span class="hljs-comment">// self.n_head).transpose(1, 2) # (B, nh, T, hs)</span><br><span class="hljs-selector-tag">q</span> = <span class="hljs-selector-tag">q</span><span class="hljs-selector-class">.view</span>(B, T, self<span class="hljs-selector-class">.n_head</span>, C <span class="hljs-comment">// self.n_head).transpose(1, 2) # (B, nh, T, hs)</span><br>v = v<span class="hljs-selector-class">.view</span>(B, T, self<span class="hljs-selector-class">.n_head</span>, C <span class="hljs-comment">// self.n_head).transpose(1, 2) # (B, nh, T, hs)</span><br></code></pre></td></tr></table></figure><p>接下来就可以求 attention 的分数了</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">att</span> = (q @ k.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) * (<span class="hljs-number">1</span>.<span class="hljs-number">0</span> / math.sqrt(k.size(-<span class="hljs-number">1</span>)))<br><span class="hljs-attribute">att</span> = F.softmax(att, dim=-<span class="hljs-number">1</span>)          <br></code></pre></td></tr></table></figure><p>注意这里要除以根号d， 至于为什么，可以看：<a href="https://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247485665&idx=1&sn=76dc6c4cfc932e3ccfe37c32cab08c72&scene=21#wechat_redirect">NLP面试官：“Attention为什么要除以根号d” 算法女生这么回答当场想发 offer</a></p><p>得到 attention 之后， 我们还需要对得分进行 mask， 因为当前位置不应该看到后面的信息。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">self.bias</span> = torch.tril(torch.<span class="hljs-literal">on</span>es(max_length, max_length)).view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, max_length, max_length)<br><span class="hljs-attr">att</span> = att.masked_fill(self.bias[:,:,:T,:T] == <span class="hljs-number">0</span>, torch.finfo(x.dtype).min)<br></code></pre></td></tr></table></figure><p>这个 self.bias 是一个 tril 矩阵。右上角全为 0。</p><p>得到分数后， 就可以和 v 相乘了, 并且还原到多头前的维度。</p><figure class="highlight gml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs gml"><span class="hljs-variable language_">y</span> = att @ v<br><span class="hljs-variable language_">y</span> = <span class="hljs-variable language_">y</span>.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(B, T, C)<br><span class="hljs-variable language_">y</span> = <span class="hljs-symbol">self</span>.o_proj(<span class="hljs-variable language_">y</span>)<br></code></pre></td></tr></table></figure><p>上面代码的最后是再一次做了个线性变换， 至此 MHA 算是写完了。</p><p>当然只是最基础版本的 MHA， 现在 flash attention 也已经是标配，如果在面试过程中你说我会写 flash attention， 那绝对是大大的加分项。</p><p>但是写 flash attention 有点难度，大概要 100 行，可以参考这个：</p><p><a href="https://github.com/tspeterkim/flash-attention-minimal/blob/main/flash.cu">https://github.com/tspeterkim/flash-attention-minimal/blob/main/flash.cu</a></p><p><a href="https://mp.weixin.qq.com/s/shExbs3NPKqxqWEGgvnt8Q">手撕 MHA，阿里的一面问的真是太细了</a></p><h1 id="6-大模型推理的时候-top-k-和-top-p-同时设置的时候怎么采样？"><a href="#6-大模型推理的时候-top-k-和-top-p-同时设置的时候怎么采样？" class="headerlink" title="6. 大模型推理的时候 top k 和 top p 同时设置的时候怎么采样？"></a>6. 大模型推理的时候 top k 和 top p 同时设置的时候怎么采样？</h1><p>这个题目就是考察细节， 直接从 transformers 的源代码中找答案即可。</p><p>其实在 transformers 的4.41.x 版本之前， 在 <code>src/transformers/generation_utils.py</code> 中专门有段 topk 和 topp 的处理代码，写的很简洁， 如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">top_k_top_p_filtering</span>(<span class="hljs-params"></span><br><span class="hljs-params">    logits: Tensor,</span><br><span class="hljs-params">    top_k: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>,</span><br><span class="hljs-params">    top_p: <span class="hljs-built_in">float</span> = <span class="hljs-number">1.0</span>,</span><br><span class="hljs-params">    filter_value: <span class="hljs-built_in">float</span> = -<span class="hljs-built_in">float</span>(<span class="hljs-params"><span class="hljs-string">&quot;Inf&quot;</span></span>),</span><br><span class="hljs-params">    min_tokens_to_keep: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params"></span>) -&gt; Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot; Filter a distribution of logits using top-k and/or nucleus (top-p) filtering</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            logits: logits distribution shape (batch size, vocabulary size)</span><br><span class="hljs-string">            if top_k &gt; 0: keep only top k tokens with highest probability (top-k filtering).</span><br><span class="hljs-string">            if top_p &lt; 1.0: keep the top tokens with cumulative probability &gt;= top_p (nucleus filtering).</span><br><span class="hljs-string">                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)</span><br><span class="hljs-string">            Make sure we keep at least min_tokens_to_keep per batch example in the output</span><br><span class="hljs-string">        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> top_k &gt; <span class="hljs-number">0</span>:<br>        top_k = <span class="hljs-built_in">min</span>(<span class="hljs-built_in">max</span>(top_k, min_tokens_to_keep), logits.size(-<span class="hljs-number">1</span>))  <span class="hljs-comment"># Safety check</span><br>        <span class="hljs-comment"># Remove all tokens with a probability less than the last token of the top-k</span><br>        indices_to_remove = logits &lt; torch.topk(logits, top_k)[<span class="hljs-number">0</span>][..., -<span class="hljs-number">1</span>, <span class="hljs-literal">None</span>]<br>        logits[indices_to_remove] = filter_value<br><br>    <span class="hljs-keyword">if</span> top_p &lt; <span class="hljs-number">1.0</span>:<br>        sorted_logits, sorted_indices = torch.sort(logits, descending=<span class="hljs-literal">True</span>)<br>        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-<span class="hljs-number">1</span>), dim=-<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># Remove tokens with cumulative probability above the threshold (token with 0 are kept)</span><br>        sorted_indices_to_remove = cumulative_probs &gt; top_p<br>        <span class="hljs-keyword">if</span> min_tokens_to_keep &gt; <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)</span><br>            sorted_indices_to_remove[..., :min_tokens_to_keep] = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># Shift the indices to the right to keep also the first token above the threshold</span><br>        sorted_indices_to_remove[..., <span class="hljs-number">1</span>:] = sorted_indices_to_remove[..., :-<span class="hljs-number">1</span>].clone()<br>        sorted_indices_to_remove[..., <span class="hljs-number">0</span>] = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># scatter sorted tensors to original indexing</span><br>        indices_to_remove = sorted_indices_to_remove.scatter(<span class="hljs-number">1</span>, sorted_indices, sorted_indices_to_remove)<br>        logits[indices_to_remove] = filter_value<br>    <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure><p>从这个函数中可以清晰的看到， 是先 topk 然后再进行 top p 的。</p><p>先进行 topk 的处理， 选出最大的 k 个 logits， 然后剩余的 logits 值都设置为负无穷， 这样 softmax 的时候概率就变成 0 了。</p><p>然后在这 top k 个 logits 中，计算累计概率， 选出符合条件的 top p 个 logits。</p><p>我之前在自己写一些调试代码的时候，经常 import 这段代码。但这随着版本更新， 发现这个函数没了。</p><p>这一段逻辑跑到了 <code>src/transformers/generation/utils.py</code> 中， 如下所示</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-keyword">if</span> generation_config<span class="hljs-selector-class">.top_k</span> is not None and generation_config<span class="hljs-selector-class">.top_k</span> != <span class="hljs-number">0</span>:<br>    processors<span class="hljs-selector-class">.append</span>(<br>        <span class="hljs-built_in">TopKLogitsWarper</span>(top_k=generation_config<span class="hljs-selector-class">.top_k</span>, min_tokens_to_keep=min_tokens_to_keep)<br>    )<br><span class="hljs-keyword">if</span> generation_config<span class="hljs-selector-class">.top_p</span> is not None and generation_config<span class="hljs-selector-class">.top_p</span> &lt; <span class="hljs-number">1.0</span>:<br>    processors<span class="hljs-selector-class">.append</span>(<br>        <span class="hljs-built_in">TopPLogitsWarper</span>(top_p=generation_config<span class="hljs-selector-class">.top_p</span>, min_tokens_to_keep=min_tokens_to_keep)<br>    )<br></code></pre></td></tr></table></figure><p>可以看到处理的顺序还是没变， 只不过变成了 TopKLogitsWarper 和 TopPLogitsWarper。</p><p>很多代码往往越写越复杂， 封装的越来越多， 比如 langchain， 个人感觉就封装的略有点过头了。</p><p><a href="https://mp.weixin.qq.com/s/bTYvYbDQAzTXtQQ-vmxmQA">大模型推理的时候 top k 和 top p 同时设置的时候怎么采样？</a></p><h1 id="7-给一些-token-id-和-对应的-tokenizer，-可以将其无损的还原为原始文本么？"><a href="#7-给一些-token-id-和-对应的-tokenizer，-可以将其无损的还原为原始文本么？" class="headerlink" title="7. 给一些 token id 和 对应的 tokenizer， 可以将其无损的还原为原始文本么？"></a>7. 给一些 token id 和 对应的 tokenizer， 可以将其无损的还原为原始文本么？</h1><p>需要看 tokenizer 的实现算法， 如果 tokenizer 是采用类似 <strong>Byte-level BPE 的算法， 就可以做到无损还原</strong>。</p><p>这是因为 Byte-level 的 BPE 算法在构建 token 的时候， 完全是基于字节来统计的， 所以其可以对任意的数据进行 encode， 不只是局限于文本数据。</p><p>举一个具体的例子可能更好理解。比如下面这句话：</p><p>stay foolish stay hungry</p><p>如果是 word 粒度的 tokenize 方法， 那处理的对象是 word， 那第一步可能先把 stay 给变成 token， 因为 stay 出现了2次。</p><p>如果是 char 粒度的， 那处理的力度就是字母， 那会先把 st, ta, y_, oo 先变成 token。</p><p>非 Byte level 的， 总是基于现有文本元素来进行组合。比如英文就是 26 个大小写字母还有其他标点等符号， 中文则是汉字。</p><p>这样做总是会存在一个 Out Of Vocabulary 的问题， 比如现在有个单词， clever， 那就没法表示了， 只能用一个统一的token 来表示。</p><p>那自然英文的 tokenizer 就没法给中文做 tokenzier。</p><p>Byte-level 是怎么做的呢？<strong>处理的对象是字节</strong>。不管是什么语言，最终在计算机里都是二进制的数据。</p><p>比如单词 stay 在计算机里表示如下：</p><ul><li>s: 01110011 (0x73)</li><li>t: 01110100 (0x74)</li><li>a: 01100001 (0x61)</li><li>y: 01111001 (0x79)</li></ul><p>在训练 tokenize 模型的时候， 直接统计字节的 pair. 比如 (0x74, 0x74) 出现次数最多， 则优先变成 token。</p><p><strong>由于所有的数据都是由字节组成的，而初始的 token 只需要 256 个， 就覆盖了所有的可能</strong>， 所以完全没有 OOV 的问题。</p><p>比如汉字是由 Unicode 的组成的， 比如“看图学”这三个字， 用 Unicode 的十六进制表示为</p><ul><li>看：0x770B</li><li>图：0x56FE</li><li>学：0x5B66</li></ul><p>Byte level 的不管原始的信息是什么，就是直接用字节组合。</p><p>所以这个时候， 英文的 tokenizer 也可以给汉字编码， 因为汉字也可以表示成字节。</p><p>比如 llama 2 的词表里大概有5k 多个汉字， 其他不在词表里的汉字，可能需要3-5个 token 来表示一个汉字， 这就造成了吞吐效率的极大浪费，所以需要加入新的词表对 llama 进行 post pretrain 才行。</p><p>比如你问 llama2 问题， 对于输入的中文还是可以理解的，但是输出的时候还是倾向于用英文来输出。如下图所示：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739158268947-12.png" alt="img"></p><p><a href="https://mp.weixin.qq.com/s/nf2qVGlSbu6GNB4ODc0epQ">给一些 token id 和 对应的 tokenizer， 可以将其无损的还原为原始文本么？</a></p><h1 id="8-为什么-Attention-最后采用了-Dot-Product-而不是-Addition？"><a href="#8-为什么-Attention-最后采用了-Dot-Product-而不是-Addition？" class="headerlink" title="8. 为什么 Attention 最后采用了 Dot Product 而不是 Addition？"></a>8. 为什么 Attention 最后采用了 Dot Product 而不是 Addition？</h1><p>在 Attention 刚被提出来的时候，其实是通过 Addition 来建立关联关系的。在 Bahdanau Attention (Bahdanau et al., 2014) 中， 表示为<br>$$<br>e_{ij}&#x3D;V_a^T\tanh(W_as_{i-1}+U_ah_j)&#x3D;V_a^T\tanh(Q+K)<br>$$<br>后来， Luong Attention (Luong et al., 2015) 对比了多种 Attention 的方式， 其中就包括了 Transformers 最终采用的 dot attention， 表示为：<br>$$<br>e_{ij}&#x3D;W_Qs_{i-1}(W_Kh_j)^T&#x3D;QK^T<br>$$<br>在论文中， 作者提到 Addition Attention 在他们的实验里的效果并不太好。</p><p>在 《Attention is All your Need》 的论文中， 作者提到：</p><p>“<strong>While the two are similar in theoretical complexity</strong><strong>, dot-product attention is much faster and more space-efficient in practice,</strong> <strong>since</strong> **it can be implemented using highly optimized matrix multiplication code.**”</p><p>所以说无论是效果还是计算效率上， dot attention 似乎都更好一点。</p><p>那么计算效率到底快多少呢？</p><p>在论文《Data Movement Is All You Need: A Case Study On Optimizing Transformers》 的文中， 作者对比了 BERT 中不同运算的耗时。如下表：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210113748910.png" alt="计算效率"></p><p>其中三角形 △ 代表的是矩阵相乘， 方块 ◻️ 代表的是 softmax and layer normalization， 圆形 ○ 则是剩余的运算，比如 biases, dropout, activations, and residual connections。</p><p>可以看到， **非矩阵相乘的运算量只占了 0.2%， 但是耗时却占据了 39%**。</p><p>之所以差异如此明显， 是因为 Nvidia 的显卡里专门设置了 Tensor Core， 和其他运算模块的对比如下：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739158723357-15.png" alt="img"></p><p>可以看出，差距还是很明显的。FP32 使用 Tensor Core 计算的 TFLOPS 是普通 FP32 模块的 156&#x2F;19.5&#x3D; 8 倍！</p><p>在这个疯狂 Scaling 的时代， dot-product attention 似乎也就成为了唯一的选择。</p><p><a href="https://mp.weixin.qq.com/s/SnTCX2RWG8cqNoSs5lNmsw">“为什么 Attention 最后采用了 Dot Product 而不是 Addition？” 字节一面问的好细</a></p><h1 id="9-机器学习的三要素是什么？"><a href="#9-机器学习的三要素是什么？" class="headerlink" title="9. 机器学习的三要素是什么？"></a>9. 机器学习的三要素是什么？</h1><p>现在网络上众说纷纭，大概有几种说法：</p><ol><li>模型+策略+算法。来源：《统计学习方法》</li><li>数据+模型+算法。</li><li>数据+特征+算法。</li><li>表示+评估+优化。来源：Domingos 2012 《A few useful things to know about machine learning》</li><li>表示+策略+优化。</li></ol><p>这几种说法中，1和4是更为确切的。2，3略有欠缺，缺少了评估这一环节。5是1和4的另一种说法。</p><p>巧合的是，李航老师和Domingos教授都是2012年提出来的概念，可以说是英雄所见略同了。</p><p>李航老师所说的模型(Model)，Domingos 教授所说的表示(Representation)，说的都是<strong>把数据和模型参数映射到学习的假设空间(hypothesis space)，更朴素一点的说法可能是建模</strong>。数据、特征、要训练的模型其实都是表示的一部分。</p><p>李航老师所说的策略(Strategy)，Domingos教授所说的评估(Evaluation),说的都是要<strong>构造一个损失函数或者评价函数</strong>，来评价表示&#x2F;模型的好坏。</p><p>李航老师所说的算法(Algorithm)，Domingos教授所说的优化(Optimization),说的都是损失函数的优化算法。<strong>借助于损失函数在假设空间中找到问题最优的解</strong>。</p><p>整体来说，Domingos 教授的表述更宏观一些，李航老师的更接地气一些。</p><p>如下图所示：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739158832564-18.png" alt="表示评估优化"></p><p>需要注意的是，三要素之间不是任意可以组合的，模型适用的损失函数和优化方法都又一些原理蕴含其中。下图列举了一些常见三要素的组合关系</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739158866210-21.png" alt="机器学习三要素"></p><p><a href="https://mp.weixin.qq.com/s/GEKC6t5gZgKRAc9xIHdnzQ">“机器学习的三要素是什么？” 看上去简单，其实一点也…</a></p><h1 id="10-大模型训练为什么用梯度下降，而不是收敛更快的牛顿法？"><a href="#10-大模型训练为什么用梯度下降，而不是收敛更快的牛顿法？" class="headerlink" title="10. 大模型训练为什么用梯度下降，而不是收敛更快的牛顿法？"></a>10. 大模型训练为什么用梯度下降，而不是收敛更快的牛顿法？</h1><p>梯度下降是基于泰勒展开一阶偏微分的优化方法， 而牛顿法则是基于泰勒展开二阶偏微分的方法。</p><p>理论上牛顿法的收敛速度要比梯度下降要快的多，那为什么大模型和机器学习都采用梯度下降这种比较慢的优化方法呢？</p><p>原因有三：</p><h3 id="性能问题"><a href="#性能问题" class="headerlink" title="性能问题"></a><strong>性能问题</strong></h3><p>从<strong>迭代的次数</strong>来看， 牛顿法确实需要更少的迭代次数， 因为牛顿法是直接算出下一个极值点在哪里。但是梯度下降则是一步一步的逼近， 尤其是在接近最小值的时候， 梯度往往越来越小， 迭代也越来越慢，所以在梯度下降的 loss 刚开始下降很快，后面则趋于平缓。用牛顿法的话， loss 则陡峭的多, 如下图所示， 牛顿在第4次的时候就可以停止了：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739159014703-24.png" alt="迭代"></p><p><strong>但是也不能光看迭代次数， 还得看每次迭代的时间复杂度。</strong></p><p>对于梯度下降来说， 每次迭代的时间复杂度是 O(N), 其中 N 是参数量， 是线性的。</p><p>但是对于牛顿迭代来说， 由于要计算 Hessian 矩阵， 时间复杂度是 O(N2), 更糟糕的是， 牛顿法需要计算 Hessian 矩阵的逆， 时间复杂度一下子变成了 O(N3).</p><p>所以当参数规模较小的时候， 牛顿法确实会更快。但是当模型参数量很大的时候，牛顿法虽然总迭代次数少，但是每一步的时间复杂度很高， 最终的结果反而更慢。</p><h3 id="收敛稳定性问题"><a href="#收敛稳定性问题" class="headerlink" title="收敛稳定性问题"></a><strong>收敛稳定性问题</strong></h3><p><strong>牛顿法对于初始值的选择尤为敏感</strong>。如果刚开始就选择在了某个维度比较平坦的地方， 也就是二阶导数接近 0 的地方， 由于要求二阶导数的倒数， 计算出的下一个位置会和当前位置非常远， <strong>由于泰勒展开的二阶近似只在局部有效</strong>， 此时牛顿法的精确性已经失去了， 所以优化过程很容易就发散了。</p><p>当然可以通过加入一些阻尼来限制更新的不要太剧烈， 但是加入阻尼有时候反而丧失了牛顿收敛快的特性。</p><h3 id="鞍点问题"><a href="#鞍点问题" class="headerlink" title="鞍点问题"></a><strong>鞍点问题</strong></h3><p>相对来说， 梯度下降比起牛顿更容易脱离鞍点区域， 因为<strong>鞍点就是牛顿法的一个解， 到了鞍点牛顿法就认为已经完成优化了</strong>。</p><p>而梯度下降总是有个步长， 虽然在鞍点附近走的很慢，但是还是有机会走出来的。除非恰好就在某一个维度的鞍点上来回震荡， 这个时候可能需要加入点随机扰动才能走出鞍点。</p><h2 id="牛顿法的优化"><a href="#牛顿法的优化" class="headerlink" title="牛顿法的优化"></a><strong>牛顿法的优化</strong></h2><p>虽然直接用牛顿法优化不太可行，但是这么多年来也有很多算法借鉴了牛顿法的二阶优化的思想。</p><p>比如拟牛顿法 BFGS和L-BFGS， 而现在流行的一些基于动量的优化器， 比如 Adam中的二阶矩项（momentum term）,个人感觉其实就是在近似二阶Hessian信息，或者至少是在近似其对角线元素。这种近似使得Adam能够在保持计算效率的同时，捕获一定的二阶优化信息。</p><p><a href="https://mp.weixin.qq.com/s/DgH2MneljoGDxUx8tJAQig">大模型训练为什么用梯度下降，而不是收敛更快的牛顿法？阿里二面问的好有难度</a></p><h1 id="11-如何评估大模型的性能？目前的评估方法都有什么？"><a href="#11-如何评估大模型的性能？目前的评估方法都有什么？" class="headerlink" title="11.如何评估大模型的性能？目前的评估方法都有什么？"></a>11.如何评估大模型的性能？目前的评估方法都有什么？</h1><p>关于如何系统的评估一个大模型，之前微软有一篇综述写的很好，而且有中文版，可以直接点击下面网页查看：</p><p><a href="https://www.microsoft.com/en-us/research/articles/evaluation-of-large-language-models/">https://www.microsoft.com/en-us/research/articles/evaluation-of-large-language-models/</a></p><p>但是，这些传统的测评方法目前都面临着一些问题。下面稍微列举几点：</p><h3 id="数据污染造成的刷榜问题"><a href="#数据污染造成的刷榜问题" class="headerlink" title="数据污染造成的刷榜问题"></a><strong>数据污染造成的刷榜问题</strong></h3><p>由于大多数测试集合都是开源的，很容易通过合成数据来构造一批类似的样本。这都报速成班了，那结果自然很不错。</p><p>这种刷榜怎么识别呢？比较简单的方法就是用新的数据去检测，比如每年新出的高考题，之前肯定谁都没见过，在这些题目上做的好，才是真的好。</p><h3 id="当前的测评数据已经落后于模型的真正性能"><a href="#当前的测评数据已经落后于模型的真正性能" class="headerlink" title="当前的测评数据已经落后于模型的真正性能"></a><strong>当前的测评数据已经落后于模型的真正性能</strong></h3><p>现在的模型已经逐渐能解决越来越复杂的问题，比如 GPT o1， 而这些传统的测试集合往往都比较简单，并不能测试出模型的真正能力。</p><h3 id="王自如困境"><a href="#王自如困境" class="headerlink" title="王自如困境"></a><strong>王自如困境</strong></h3><p>除了开源的一些测评，业内也有一些野榜混淆视听。</p><p>毕竟搞测评也得花钱，团队里的人也得发工资，现在有人赞助测评机构了，把人家的排名弄的很低，好意思么？</p><p>所以很多野榜都很难保证公平性。</p><h2 id="目前唯一可信的测评榜单"><a href="#目前唯一可信的测评榜单" class="headerlink" title="目前唯一可信的测评榜单"></a><strong>目前唯一可信的测评榜单</strong></h2><p>目前没办法刷榜的就是  Chatbot Arena 了。这个排行是让大模型随机匹配对手打天梯。</p><p>然后背后有一套积分系统，应该是 Elo 评分系统。随机让两个模型对相同的输入生产答案，然后让人盲评这两个模型的好坏。</p><p>当模型击败一个积分更高的模型时，得分会提升，反之则会下降。就这样最后所有的模型都有一个积分，就可以看出每个模型的好坏。</p><p>但是这个评测有一点需要注意，那就是<strong>一个刚进入系统的新模型，可能会因为对战的数据不足导致测评结果有偏差</strong>。比如 Claude 2 刚进系统的时候，评分要比 Claude 1 要低，但是随着对战次数的增加，效果就越来越好了。</p><p>然后这个榜单还有一些细分选项，比如 Style-controlled ranking。让模型生成一些有约束的风格，比如长度，格式等。这时每个模型等排名会发生变化，也挺有意思。</p><p><a href="https://mp.weixin.qq.com/s/bBCndI-cSuJI_5myzb74vA">字节一面：大模型如何评测以及当前测评的困境</a></p><h1 id="12-Transformers-为什么采用-QKV-Attention？"><a href="#12-Transformers-为什么采用-QKV-Attention？" class="headerlink" title="12.Transformers 为什么采用 QKV Attention？"></a>12.Transformers 为什么采用 QKV Attention？</h1><p>Transformers 的论文在讲述 Attention 这一部分时并没有讲的特别细，尤其是关于 Query， Key 和 Value 的设计似乎是突然就水灵灵的出现了。而且最终的形式比以往的 Attention 都要简略很多。</p><p>在论文中没有找到答案，那只能看看同时期的论文有没有类似的研究。最终在一篇《Frustratingly Short Attention Spans In Neural Language Modeling》的论文找到了类似的结构。我们来看看这篇文章时怎么说的。</p><p>这篇论文是基于最早的 Bahdanau Attention (Bahdanau et al., 2014) 进行改进的。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739161335729-3.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739161348936-6.png" alt="img"></p><p>这篇论文认为当时的 Attention , <strong>隐向量至少承担了3个功能</strong>：</p><ul><li>计算 attention score</li><li>计算 context vector</li><li>作为 hidden state 来进行 RNN 的计算。</li></ul><p>这一个向量的任务实在有些繁重。为了更好的适配任务，作者参考了 Memory Netorkds，一次输出了 3个 隐向量， 一个是 key, 一个是 value，还有一个是 predict。这3个隐向量来计算 Attention， 功能如下：</p><ul><li>key 用来计算 attention score.</li><li>value 用来和 Atention score 相乘。</li><li>predict 用于预测词的分布。</li></ul><p>其中 predict 可以先不管。公式如下</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739161438387-9-1739161450559-11.png" alt="img"></p><p>看公式就可以知道，value 在这里作用和 Transformers 中的 Attention 的 value 是完全一致的。</p><p>那怎么没有 query 呢？<strong>实际上 query 被隐藏在了 key 之中</strong>。在那个  Attention is all your need 之前的时代， Attention 和 RNN 依然是深度绑定的。</p><p>作者在这里用了时刻 t 的 key 向量，和时刻 t 之前的 key 向量做了 Attention，因为作者假设时刻 t 之前的 key 和 value 是 Memory。</p><p>所以 query 就是 k_t, 而 t 时刻前面的就是 key。</p><p>虽然写法不一样，<strong>但是 Transformers 的 qkv Attention 的设计和这篇论文的设计的整体框架是一样的</strong>。细微之处的不同点列举如下：</p><ol><li>Transformers 由于丢弃了 RNN，所以可以更好的并行，采用了矩阵运算的形式。</li><li>Transformers 采用了 dot Attention，应该是参考了 Luong Attention。而这篇文章依然采用了 Bahdanau Attention 所使用的  Addition Attention</li><li>Transformers 的 Attention 是全连接图，而这里是 causal 形式的 Attention。</li></ol><p>虽然原始论文没有细说，但是这应该就是 qkv Attention 的由来的一个解释。</p><p><a href="https://mp.weixin.qq.com/s/9xPFK3yRwFMciTwrCnYpGw">“Transformers 为什么采用 QKV Attention？” 大模型面经</a></p><h1 id="13-Attention-为什么使用-Multi-Head"><a href="#13-Attention-为什么使用-Multi-Head" class="headerlink" title="13. Attention 为什么使用 Multi Head ?"></a>13. Attention 为什么使用 Multi Head ?</h1><p>Multi Head Attention 最早可以追溯到 《A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING》 (Lin et al., 2017) 这篇论文。</p><p>从题目就可以看出，是用 Multi Head Attention 来做表示学习。</p><p>作者参考了 Bahdanau Attention， 只不过在计算 attention score 的时候，输入从 $h_j$、$s_i$改成了双向 LSTM 的两个方向的 hidden states。</p><ul><li>Encoder</li></ul><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739166471278-13.png" alt="img"></p><p>从公式明显可以看出，Self Attention 计算 Score 的设计明显是参考了 Bahdanau Attention 的计算方法。</p><p>上面的计算是单个 Attention 的计算方法。然后借鉴一下卷积网络的多个 kernel 的思想，作者设计了多个 Attention，期望不同的 Attention 学习到不同的注意力，从而更好的提取特征。<strong>这种一个不行就上多个到方法在深度学习中经常使用，比如 CNN 多 kernel， MultiHead Attention， Mixture of Experts 等。</strong></p><p>基于这个思路，作者提出了 Attention 的矩阵运算形式。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739166602571-16.png" alt="img"></p><p>这其实就是MultiHead 了，在公式中是通过 r来体现的。</p><p>然后这个矩阵相乘 AH已经和 Transformers 最终形态的 Attention$AV&#x3D;\text{softmax}(\frac{QK^\top}{\sqrt{d}})V$</p><p>有点类似了。</p><p>文中也对比了随着 head 数量增多后，在具体任务上的表现，如下图所示：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739166616383-19.png" alt="img"></p><p>后来又有学者认为，当前的 MHA 的形式，同一层的 Head 之间没有任何关联，想建立关联得到下一层了，这样的效果可能不如多层的 Single Head。然后做了一些实验，也确实证明了多层 Single Head 比浅层的 MHA 效果好，具体见《<a href="https://arxiv.org/pdf/2106.09650%E3%80%8B%E3%80%82">https://arxiv.org/pdf/2106.09650》。</a></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739166627734-22.png" alt="img"></p><p>那为什么没有流行起来呢？核心还是 MHA 可以非常方便的并行计算，而采用 Single Head 的话，层与层之间有诸多的非线性操作还有 norm 等，不如 MHA 直接做一些视图变换然后直接矩阵相乘效率高。</p><h1 id="14-为什么-LLM-推理时通常采用-left-padding？"><a href="#14-为什么-LLM-推理时通常采用-left-padding？" class="headerlink" title="14. 为什么 LLM 推理时通常采用 left padding？"></a>14. 为什么 LLM 推理时通常采用 left padding？</h1><p>其实在几年前，NLP 的大部分任务都是 right padding 的，因为哪个时候 Encoder 的架构是主流。</p><p>现在的 tokenizer 都会提供 padding 操作，但是当采用 right padding 做tokenize，然后用现在的大模型推理的时候，就会得到如下的警告：</p><figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk">logger.warning(<br>    <span class="hljs-comment">&quot;A decoder-only architecture is being used, but right-padding was detected! For correct &quot;</span><br>    <span class="hljs-comment">&quot;generation results, please set `padding_side=&#x27;left&#x27;` when initializing the tokenizer.&quot;</span><br>)<br></code></pre></td></tr></table></figure><p>原因就是目前的大模型基本清一色都是 Decoder Only 的架构，当采用 left padding 的时候，<strong>一个 batch 内的所有样本预测的就恰好是下一个词。</strong></p><p>举一个例子，对于一个 tensor</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs inform7"><span class="hljs-comment">[</span><br><span class="hljs-comment"> <span class="hljs-comment">[1,2,3]</span>,</span><br><span class="hljs-comment"> <span class="hljs-comment">[1,2,3,4,5,6]</span></span><br><span class="hljs-comment">]</span><br></code></pre></td></tr></table></figure><p>如果采用 0 作为 pad_id, 则 padding 后为</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs inform7"><span class="hljs-comment">[</span><br><span class="hljs-comment"> <span class="hljs-comment">[0,0,0,1,2,3]</span>,</span><br><span class="hljs-comment"> <span class="hljs-comment">[1,2,3,4,5,6]</span></span><br><span class="hljs-comment">]</span><br></code></pre></td></tr></table></figure><p>这个时候预测的下一个 token 直接拼接上就行。</p><p>如果采用 right padding， 则需要进行特殊处理。right padding 后为</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs inform7"><span class="hljs-comment">[</span><br><span class="hljs-comment"> <span class="hljs-comment">[1,2,3,0,0,0]</span>,</span><br><span class="hljs-comment"> <span class="hljs-comment">[1,2,3,4,5,6]</span></span><br><span class="hljs-comment">]</span><br></code></pre></td></tr></table></figure><p>这个时候第一个样本显然不能从最后的位置开始预测，而是要从3后面的位置开始推理。</p><p>所以对于不等长的输入，<strong>right padding 要从最短的序列位置开始推理。</strong></p><p>但是对于其他样本来说，其实是不需要预测的，因为本来就是输入，但是机制问题，必须要从最短的位置开始预测。<strong>其他的样本在哪些不需要预测的位置，其实也预测了，但是把结果丢掉了，这就造成计算资源的浪费。</strong></p><p>最早 llama 发布的时候，就给出了通过 right padding 来进行预测的代码，可以清晰的看到其流程如上所述。</p><p><a href="https://github.com/meta-llama/llama/blob/ea9f33d6d3ea8ed7d560d270986407fd6c2e52b7/llama/generation.py#L106">https://github.com/meta-llama/llama/blob/ea9f33d6d3ea8ed7d560d270986407fd6c2e52b7/llama/generation.py#L106</a></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739177211969-25.png" alt="img"></p><p>那又有人问了，为什么要做 padding？哪些填充的字符不也是白白占了显存和计算资源？串行计算可不可以？</p><p>padding 却是会占用一部分，但是因为 GPU 针对矩阵运算有加速，所以带来的加速效果是远超扔掉 padding 然后串行的。</p><p>这个道理就跟 causal attention 计算的时候，<strong>明明有一半的 score 不需要计算，但是仍旧采用先算完然后在 mask 的位置 fill zero 是一样的道理</strong>。</p><p><a href="https://mp.weixin.qq.com/s/3joYPklFAIffxsNg6Fjurg">“现在大模型为什么都用 left padding？” 字节实习一面，学姐的面经</a></p><h1 id="15-ROC-AUC-的值代表了什么？"><a href="#15-ROC-AUC-的值代表了什么？" class="headerlink" title="15. ROC-AUC 的值代表了什么？"></a>15. ROC-AUC 的值代表了什么？</h1><p>这个问题其实完全可以通过数学来推算出来，这里先说结论：</p><p><strong>AUC的值，就是从样本中任意取一个正例和一个负例，分类器给正例的得分大于给负例得分的概率。</strong></p><p>下面是详细的证明：</p><p>如果把ROC曲线看成是TPR对FPR的函数，TPR&#x3D;F(x)。我们对这个函数进行积分。如下图所示：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178333249-28.png" alt="img"></p><p>则有</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178343104-31.png" alt="img"></p><p>假设样本label为y，模型预测得分为s，阈值为t，正例的概率密度函数为  （图中红色部分）,负例的概率密度函数为  （图中蓝色部分）.则有 </p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178357700-34.png" alt="img"></p><p>这两个公式就对应之前 TPR 和 FPR 的计算公式。</p><p>其实就是下面两个动图。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178365880-37.gif" alt="img"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178375014-40.gif" alt="img"></p><p>从公式上可以看出，x是t的积分上限函数，根据积分上限函数的性质，得到：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178383645-43.png" alt="img"></p><p>带入 AUC 的公式，则有</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178392218-46.png" alt="img"></p><p>上面推导需要解释一下：</p><p>第二行，因为FPR的取值范围从0到1，对应着阈值是从大到小的变化。可以从动图中看出，只不过动图中阈值是从小到大，FPR是从1到0</p><p>第五行， 的含义就是该样本为负例，得分为t的概率。加引号是为了和正例区分。</p><p>第七行，该积分相当于是遍历阈值t,同时负例得分和t相同，也就是负例遍历所有可能的得分情况。</p><p>所以最终得到这么一个结论：<strong>AUC的值，就是从样本中任意取一个正例和一个负例，分类器给正例得分大于负例得分的概率。</strong></p><p>正因为这个性质，AUC 在推荐排序中也经常使用。</p><p><a href="https://mp.weixin.qq.com/s/o8d5q3efILZOwt5CzVD3xQ">算法岗面试八股必问：“ROC-AUC 的值代表了什么？”</a></p><h1 id="16-ROC-AUC-曲线是什么？怎么计算？"><a href="#16-ROC-AUC-曲线是什么？怎么计算？" class="headerlink" title="16. ROC-AUC 曲线是什么？怎么计算？"></a>16. ROC-AUC 曲线是什么？怎么计算？</h1><p>先看两个图，来感受一下 ROC 到底是啥。如果看不明白，后面有解释。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178576834-49.gif" alt="img"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178587734-52.gif" alt="img"></p><p>‍ROC曲线(Receiver operating characteristic curve) 最早用在雷达系统中，前面单词中所谓Receiver就是雷达接收器。后来又广泛应用在医学领域，然后又在数据统计和机器学习中担任了重要角色。</p><p>雷达的操作员(Radar operator)也是花钱雇的，那不得打个绩效。<strong>ROC 就相当于雷达操作员的绩效，用来衡量雷达操作员的识别能力。</strong></p><p>根据雷达的波形，操作员会判断是否出现敌机，当时正处于二战时期，主要就是来识别是不是日本的战斗机。</p><p>如果日本的战斗机真的飞过来了：</p><ul><li>操作员判断是真的飞过来了，此时叫做：正例预测对(**True Positive)**，有时也叫真阳性。</li><li>否则叫做：正例预测错(<strong>False Negtative</strong>)，有时候也叫假阴性。</li></ul><p>如果日本的战斗机没有飞过来：</p><ul><li>操作员判断战斗机没飞过来，此时叫做：负例预测对(<strong>True Negative</strong>)， 有时候也叫真阴性。</li><li>否则叫做：负例预测错(<strong>False Positive</strong>)，有时候也叫假阳性。</li></ul><p>做成图像如下：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178595626-55.png" alt="img"></p><p>那么怎么给操作员打绩效呢？当时的科学家们提出了两个概念：</p><p><strong>TPR(True Positive Rate)，表示操作员从真正的战斗机中识别出的比例，一个更常用的名称叫召回率。</strong> </p><p><strong>FPR(False Positive Rate)， 表示操作员在没有战斗机的事件中误报的比例。</strong></p><p>这两个指标的计算公式见上图。</p><p>然后以 FPR 为横坐标，以 TPR 为纵坐标，在对应位置打一个点，就是这个操作员实际能力的可视化。</p><p>比如有5个操作员打的点如下图：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178604755-58.png" alt="img"></p><p>在这个图里面，我们肯定是希望 误判(FPR) 越小越好，召回(TPR) 越大越好，所以 D &gt; B &gt; C</p><p>A 和 B 各有千秋，没法硬比。D 实际上是整个图里的最优点。</p><p>还有一点需要注意的就是，虚线代表了随机猜测。因为当我们随机猜测为正的概率为 p 时， 假设 N 个正例，M 个负例。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210171029299.png" alt="image-20250210171029299"></p><p><strong>所以当点落到虚线上时，就代表了随机猜测。而 TPR &gt; FPR 则代表了比随机猜测要好，否则则比随机猜测还差。</strong></p><p>前面几个指标，比如 Accuracy， Precision， Recall 等，当 TPR &#x3D; FPR + δ 时，只要 δ &gt; 0, 都会变大。具体的证明只需要带入公式计算就可以了，这里不再赘述。</p><h3 id="当操作员每次的输出是一个概率时"><a href="#当操作员每次的输出是一个概率时" class="headerlink" title="当操作员每次的输出是一个概率时"></a><strong>当操作员每次的输出是一个概率时</strong></h3><p>之前操作员要么判对，要么判错，在二位平面上就一个点。</p><p>但是有时候操作员也拿不准，输出一个概率时，我们可以自己设置阈值来把操作员的每个 (FPR, TPR) 点画出来， 此时就得到了一条 ROC 曲线。</p><p>计算如下，左图的区域闪烁对应同样闪烁的右侧的公式部分。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178645077-61.gif" alt="img"></p><p>TPR 可视化</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178652942-64.gif" alt="img"></p><p>FPR 可视化</p><p>当遍历阈值（移动的竖线）时，即可得到ROC曲线。如下图：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178660819-67.gif" alt="img"></p><p>下面是当分类器的能力变得越来越好时，ROC 曲线的变化情况：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739178671965-70.gif" alt="img"></p><p>可以看到，当分类器能力越来越好时，曲线会往左上角（最优点）靠拢。<strong>此时可以用曲线下的面积来表示分类器的能力， 叫做 AUC</strong>， AUC是ROC曲线下的面积( Area Under the ROC Curve)。</p><p>虽然大家都知道 AUC 越大越好，但是 AUC 的值到底代表了什么呢？这个放在下期更新。</p><p><a href="https://mp.weixin.qq.com/s/kEim7SOvyTktWBKnUIgqKQ">“ROC-AUC 曲线是什么？怎么计算？” 小米一面，问的贼细</a></p><h1 id="17-Qwen-从头开始训练的初始-loss-大概是多少？"><a href="#17-Qwen-从头开始训练的初始-loss-大概是多少？" class="headerlink" title="17.Qwen 从头开始训练的初始 loss 大概是多少？"></a>17.Qwen 从头开始训练的初始 loss 大概是多少？</h1><p>我们在训练模型的时候，会观测很多指标。诸多指标中， loss 肯定是最重要的指标之一。所以训练模型的第一件事，往往都是看 loss 算的对不对。</p><p>这个经验很重要，尤其是现在模型越来越大，训练时经常要做一些 Tensor Parallelism 或者 Pipeline Parallelism。那么有没有正常训练？这个时候看一下 loss 是否符合预期，以防止白白浪费算力。</p><p>那么从零开始训练的初始的 loss 大概是多少呢？</p><p>这个完全可以推算一下。</p><p>这里有一个假设，就是参数随机的情况下，假设预测下一个 token 的概率符合均匀分布，也就是所有 token 的概率是相同的。</p><p>如果词表的大小是 |V|, 那么第 i 个词的概率都是$\frac1{|V|}$</p><p>由于采用交叉熵损失，label 又是 onehot 的， 那么下一个词的 loss 为：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210173040749.png" alt="image-20250210173040749"></p><p>Qwen 的词表大小为 152064， 所以其初始 loss 大概为 $\log152064&#x3D;11.932$</p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>笔试面试</category>
      
      <category>AI算法</category>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>笔试面试</tag>
      
      <tag>算法面试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MiniCPM-V多模态模型源码解析-04：核心模块omnilmm.py</title>
    <link href="/2025/01/14/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9AMiniCPM-V%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-04%EF%BC%9A%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97omnilmm.py%E8%AF%A6%E8%A7%A3/"/>
    <url>/2025/01/14/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9AMiniCPM-V%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-04%EF%BC%9A%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97omnilmm.py%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h3 id="1-导入模块"><a href="#1-导入模块" class="headerlink" title="1. 导入模块"></a><strong>1. 导入模块</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gc  <span class="hljs-comment"># 垃圾回收模块</span><br><span class="hljs-keyword">import</span> math  <span class="hljs-comment"># 数学计算模块</span><br><span class="hljs-keyword">import</span> timm  <span class="hljs-comment"># 视觉模型库</span><br><span class="hljs-keyword">import</span> torch  <span class="hljs-comment"># PyTorch深度学习框架</span><br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> Tensor  <span class="hljs-comment"># PyTorch中的张量类型</span><br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn  <span class="hljs-comment"># PyTorch中的神经网络模块</span><br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> CrossEntropyLoss  <span class="hljs-comment"># 交叉熵损失函数</span><br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span>, <span class="hljs-type">Optional</span>, <span class="hljs-type">Tuple</span>, <span class="hljs-type">Union</span>  <span class="hljs-comment"># 类型注解</span><br><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM  <span class="hljs-comment"># Hugging Face的自动配置和因果语言模型</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MistralForCausalLM, MistralModel, MistralConfig  <span class="hljs-comment"># Mistral模型相关类</span><br><span class="hljs-keyword">from</span> transformers.modeling_outputs <span class="hljs-keyword">import</span> BaseModelOutputWithPast, CausalLMOutputWithPast  <span class="hljs-comment"># 模型输出类型</span><br><br><span class="hljs-keyword">from</span> omnilmm.model.utils <span class="hljs-keyword">import</span> build_transform  <span class="hljs-comment"># 图像变换构建工具</span><br><span class="hljs-keyword">from</span> omnilmm.model.resampler <span class="hljs-keyword">import</span> Resampler  <span class="hljs-comment"># 重采样器模块</span><br></code></pre></td></tr></table></figure><hr><h3 id="2-定义常量"><a href="#2-定义常量" class="headerlink" title="2. 定义常量"></a><strong>2. 定义常量</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">DEFAULT_IMAGE_PATCH_TOKEN = <span class="hljs-string">&quot;&lt;im_patch&gt;&quot;</span>  <span class="hljs-comment"># 默认的图像patch token</span><br>DEFAULT_IM_START_TOKEN = <span class="hljs-string">&quot;&lt;im_start&gt;&quot;</span>  <span class="hljs-comment"># 默认的图像开始token</span><br>DEFAULT_IM_END_TOKEN = <span class="hljs-string">&quot;&lt;im_end&gt;&quot;</span>  <span class="hljs-comment"># 默认的图像结束token</span><br></code></pre></td></tr></table></figure><hr><h3 id="3-OmniLMMConfig-类"><a href="#3-OmniLMMConfig-类" class="headerlink" title="3. OmniLMMConfig 类"></a><strong>3. OmniLMMConfig 类</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">OmniLMMConfig</span>(<span class="hljs-title class_ inherited__">MistralConfig</span>):<br>    model_type = <span class="hljs-string">&quot;omnilmm&quot;</span>  <span class="hljs-comment"># 模型类型为OmniLMM</span><br></code></pre></td></tr></table></figure><ul><li>继承自<code>MistralConfig</code>，用于配置OmniLMM模型。</li><li><code>model_type</code> 设置为 <code>&quot;omnilmm&quot;</code>，表示这是一个OmniLMM模型。</li></ul><hr><h3 id="4-Identity-类"><a href="#4-Identity-类" class="headerlink" title="4. Identity 类"></a><strong>4. Identity 类</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Identity</span>(torch.nn.Identity):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>: Tensor, **kwargs</span>) -&gt; Tensor:<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>().forward(<span class="hljs-built_in">input</span>)  <span class="hljs-comment"># 直接返回输入，不做任何处理</span><br></code></pre></td></tr></table></figure><ul><li>继承自<code>torch.nn.Identity</code>，用于占位或传递输入，不做任何变换。</li></ul><hr><h3 id="5-create-vision-module-函数"><a href="#5-create-vision-module-函数" class="headerlink" title="5. create_vision_module 函数"></a><strong>5. create_vision_module 函数</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_vision_module</span>(<span class="hljs-params">config</span>):<br>    <span class="hljs-comment"># 使用timm库创建一个视觉模型，模型名为&#x27;eva02_enormous_patch14_clip_224.laion2b_plus&#x27;</span><br>    vision_tower = timm.create_model(<span class="hljs-string">&#x27;eva02_enormous_patch14_clip_224.laion2b_plus&#x27;</span>,<br>                                     pretrained=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># 不使用预训练权重</span><br>                                     num_classes=<span class="hljs-number">0</span>,  <span class="hljs-comment"># 输出类别数为0</span><br>                                     dynamic_img_size=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># 动态图像尺寸</span><br>                                     dynamic_img_pad=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 动态图像填充</span><br><br>    <span class="hljs-comment"># 如果视觉模型是VisionTransformer类型，并且有注意力池化层，则将其替换为Identity</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(vision_tower, timm.models.VisionTransformer):<br>        <span class="hljs-keyword">if</span> vision_tower.attn_pool <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            vision_tower.attn_pool = Identity()<br><br>    <span class="hljs-comment"># 使用倒数第二层的输出，将最后一层替换为Identity</span><br>    vision_tower.blocks[-<span class="hljs-number">1</span>] = Identity()<br><br>    <span class="hljs-comment"># 获取嵌入维度</span><br>    embed_dim = config.hidden_size<br>    <span class="hljs-comment"># 创建Resampler对象，用于对视觉特征进行重采样</span><br>    resampler = Resampler(<br>        grid_size=<span class="hljs-built_in">int</span>(math.sqrt(config.num_query)),  <span class="hljs-comment"># 网格大小</span><br>        embed_dim=embed_dim,  <span class="hljs-comment"># 嵌入维度</span><br>        num_heads=embed_dim // <span class="hljs-number">128</span>,  <span class="hljs-comment"># 注意力头数</span><br>        kv_dim=vision_tower.embed_dim,  <span class="hljs-comment"># 键值维度</span><br>    )<br>    <span class="hljs-keyword">return</span> vision_tower, resampler  <span class="hljs-comment"># 返回视觉模型和重采样器</span><br></code></pre></td></tr></table></figure><ul><li>该函数用于创建视觉模块，包括视觉模型和重采样器。</li></ul><hr><h3 id="6-OmniLMMModel-类"><a href="#6-OmniLMMModel-类" class="headerlink" title="6. OmniLMMModel 类"></a><strong>6. OmniLMMModel 类</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">OmniLMMModel</span>(<span class="hljs-title class_ inherited__">MistralModel</span>):<br>    config_class = OmniLMMConfig  <span class="hljs-comment"># 配置类为OmniLMMConfig</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: OmniLMMConfig, mm_vision_tower=<span class="hljs-literal">None</span>, mm_hidden_size=<span class="hljs-literal">None</span>, tune_clip=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>(OmniLMMModel, <span class="hljs-variable language_">self</span>).__init__(config)  <span class="hljs-comment"># 调用父类构造函数</span><br><br>        <span class="hljs-comment"># 如果配置中有视觉模型，则创建视觉模块</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(config, <span class="hljs-string">&quot;mm_vision_tower&quot;</span>):<br>            vision_tower, resampler = create_vision_module(config)<br><br>            <span class="hljs-comment"># <span class="hljs-doctag">HACK:</span> 为了FSDP（完全分片数据并行），将视觉模型包装在列表中</span><br>            <span class="hljs-variable language_">self</span>.vision_tower = [vision_tower]<br>            <span class="hljs-variable language_">self</span>.resampler = resampler<br>            <span class="hljs-keyword">if</span> tune_clip:<br>                <span class="hljs-variable language_">self</span>.vision_tower = <span class="hljs-variable language_">self</span>.vision_tower[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 如果调优CLIP，则直接使用视觉模型</span><br><br>        <span class="hljs-variable language_">self</span>.vision_config = <span class="hljs-keyword">lambda</span> x: <span class="hljs-literal">None</span>  <span class="hljs-comment"># 视觉配置，初始化为空函数</span><br></code></pre></td></tr></table></figure><ul><li>继承自<code>MistralModel</code>，是OmniLMM的核心模型类。</li><li>初始化时根据配置创建视觉模块。</li></ul><hr><h4 id="6-1-initialize-vision-modules-方法"><a href="#6-1-initialize-vision-modules-方法" class="headerlink" title="6.1  initialize_vision_modules 方法"></a><strong>6.1  initialize_vision_modules 方法</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_vision_modules</span>(<span class="hljs-params">self, vision_tower, no_randaug, num_query, image_size, tune_clip=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-variable language_">self</span>.config.mm_vision_tower = vision_tower  <span class="hljs-comment"># 设置视觉模型</span><br>    <span class="hljs-variable language_">self</span>.config.use_mm_proj = <span class="hljs-literal">True</span>  <span class="hljs-comment"># 使用多模态投影</span><br>    <span class="hljs-variable language_">self</span>.config.num_query = num_query  <span class="hljs-comment"># 设置查询数量</span><br>    <span class="hljs-variable language_">self</span>.config.image_size = image_size  <span class="hljs-comment"># 设置图像尺寸</span><br><br>    <span class="hljs-comment"># 如果没有视觉模型，则创建并加载预训练权重</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(<span class="hljs-variable language_">self</span>, <span class="hljs-string">&#x27;vision_tower&#x27;</span>):<br>        vision_tower, resampler = create_vision_module(<span class="hljs-variable language_">self</span>.config)<br>        state_dict = torch.load(<br>            <span class="hljs-string">&#x27;/tt/data/public/multimodal/multimodal_model_ckpts/timm/eva02_enormous_patch14_clip_224.laion2b_plus.pt&#x27;</span>)<br>        vision_tower.load_state_dict(state_dict, strict=<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">del</span> state_dict<br>        gc.collect()<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># 如果视觉模型是列表，则取第一个元素</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(<span class="hljs-variable language_">self</span>.vision_tower, <span class="hljs-built_in">list</span>):<br>            vision_tower = <span class="hljs-variable language_">self</span>.vision_tower[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">else</span>:<br>            vision_tower = <span class="hljs-variable language_">self</span>.vision_tower<br>        resampler = <span class="hljs-variable language_">self</span>.resampler<br>    <span class="hljs-variable language_">self</span>.vision_tower = vision_tower <span class="hljs-keyword">if</span> tune_clip <span class="hljs-keyword">else</span> [vision_tower]  <span class="hljs-comment"># 根据是否调优CLIP设置视觉模型</span><br>    <span class="hljs-variable language_">self</span>.resampler = resampler  <span class="hljs-comment"># 设置重采样器</span><br><br>    <span class="hljs-comment"># 构建训练和评估时的图像变换</span><br>    train_img_transform = build_transform(<br>        is_train=<span class="hljs-literal">True</span>, randaug=<span class="hljs-keyword">not</span> no_randaug, input_size=<span class="hljs-variable language_">self</span>.config.image_size, std_mode=<span class="hljs-string">&#x27;OPENAI_CLIP&#x27;</span>)<br>    eval_img_transform = build_transform(<br>        is_train=<span class="hljs-literal">False</span>, input_size=<span class="hljs-variable language_">self</span>.config.image_size, std_mode=<span class="hljs-string">&#x27;OPENAI_CLIP&#x27;</span>)<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">dict</span>(<br>        image_processor=(train_img_transform, eval_img_transform),  <span class="hljs-comment"># 返回图像处理器</span><br>        image_token_len=num_query,  <span class="hljs-comment"># 返回图像token长度</span><br>        vision_config=<span class="hljs-variable language_">self</span>.vision_config  <span class="hljs-comment"># 返回视觉配置</span><br>    )<br></code></pre></td></tr></table></figure><ul><li>该方法用于初始化视觉模块，包括加载预训练权重、设置图像变换等。</li></ul><hr><h4 id="6-2-get-vision-embedding-方法"><a href="#6-2-get-vision-embedding-方法" class="headerlink" title="6.2 get_vision_embedding 方法"></a><strong>6.2 get_vision_embedding 方法</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_vision_embedding</span>(<span class="hljs-params">self, pixel_values</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(<span class="hljs-variable language_">self</span>.vision_tower, <span class="hljs-built_in">list</span>):<br>        vision_tower = <span class="hljs-variable language_">self</span>.vision_tower[<span class="hljs-number">0</span>]  <span class="hljs-comment"># <span class="hljs-doctag">HACK:</span> 为了FSDP，取列表中的第一个元素</span><br>    <span class="hljs-keyword">else</span>:<br>        vision_tower = <span class="hljs-variable language_">self</span>.vision_tower<br><br>    dtype = vision_tower.pos_embed.data.dtype  <span class="hljs-comment"># 获取视觉模型的位置嵌入数据类型</span><br>    vision_embedding = vision_tower.forward_features(<br>        pixel_values.<span class="hljs-built_in">type</span>(dtype))  <span class="hljs-comment"># 获取视觉特征</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(vision_tower, <span class="hljs-string">&#x27;num_prefix_tokens&#x27;</span>) <span class="hljs-keyword">and</span> vision_tower.num_prefix_tokens &gt; <span class="hljs-number">0</span>:<br>        vision_embedding = vision_embedding[:,<br>                                            vision_tower.num_prefix_tokens:]  <span class="hljs-comment"># 去掉前缀token</span><br>    res = <span class="hljs-variable language_">self</span>.resampler(vision_embedding)  <span class="hljs-comment"># 对视觉特征进行重采样</span><br>    <span class="hljs-keyword">return</span> res  <span class="hljs-comment"># 返回重采样后的视觉特征</span><br></code></pre></td></tr></table></figure><ul><li>该方法用于从输入图像中提取视觉特征，并进行重采样。</li></ul><h4 id="6-3-get-vllm-embedding-方法"><a href="#6-3-get-vllm-embedding-方法" class="headerlink" title="6.3 get_vllm_embedding 方法"></a><strong>6.3 get_vllm_embedding 方法</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_vllm_embedding</span>(<span class="hljs-params">self, data</span>):<br>    <span class="hljs-comment"># 如果数据中没有视觉隐藏状态，则从像素值中提取视觉特征</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;vision_hidden_states&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> data:<br>        pixel_values_list = data[<span class="hljs-string">&#x27;pixel_values&#x27;</span>]<br>        vision_hidden_states = []<br>        <span class="hljs-keyword">for</span> pixel_values <span class="hljs-keyword">in</span> pixel_values_list:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(pixel_values) &gt; <span class="hljs-number">0</span>:<br>                <span class="hljs-comment"># 提取视觉特征</span><br>                vision_hidden_states.append(<span class="hljs-variable language_">self</span>.get_vision_embedding(pixel_values.unsqueeze(<span class="hljs-number">0</span>))[<span class="hljs-number">0</span>])<br>            <span class="hljs-keyword">else</span>:<br>                vision_hidden_states.append([])<br>    <span class="hljs-keyword">else</span>:<br>        vision_hidden_states = data[<span class="hljs-string">&#x27;vision_hidden_states&#x27;</span>]<br><br>    <span class="hljs-comment"># 获取输入token的嵌入</span><br>    inputs_embeds = <span class="hljs-variable language_">self</span>.embed_tokens(data[<span class="hljs-string">&#x27;input_ids&#x27;</span>])<br>    <span class="hljs-comment"># 将视觉特征的数据类型转换为与输入嵌入一致</span><br>    vision_hidden_states = [i.<span class="hljs-built_in">type</span>(inputs_embeds.dtype) <br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(i, torch.Tensor) <span class="hljs-keyword">else</span> i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> vision_hidden_states<br>    ]<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">HACK:</span> 替换回原始嵌入以支持LLaVA预训练</span><br>    orig_embeds_params = <span class="hljs-built_in">getattr</span>(<span class="hljs-variable language_">self</span>, <span class="hljs-string">&#x27;orig_embeds_params&#x27;</span>, <span class="hljs-literal">None</span>)<br><br>    new_input_embeds = []<br>    cur_image_idx = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> cur_input_ids, cur_input_embeds <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(data[<span class="hljs-string">&#x27;input_ids&#x27;</span>], inputs_embeds):<br>        <span class="hljs-comment"># 如果当前输入中没有图像patch token，则直接使用原始嵌入</span><br>        <span class="hljs-keyword">if</span> (cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_patch_token).<span class="hljs-built_in">sum</span>() == <span class="hljs-number">0</span>:<br>            cur_input_embeds = cur_input_embeds + (<span class="hljs-number">0.</span> * dummy_image_features).<span class="hljs-built_in">sum</span>()<br>            new_input_embeds.append(cur_input_embeds)<br>            <span class="hljs-keyword">continue</span><br><br>        <span class="hljs-comment"># 如果使用图像开始和结束token</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.vision_config.use_im_start_end:<br>            cur_image_features = vision_hidden_states[cur_image_idx]<br>            num_patches = cur_image_features.shape[<span class="hljs-number">0</span>]<br>            <span class="hljs-comment"># 检查图像开始和结束token的数量是否一致</span><br>            <span class="hljs-keyword">if</span> (cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_start_token).<span class="hljs-built_in">sum</span>() != (cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_end_token).<span class="hljs-built_in">sum</span>():<br>                <span class="hljs-keyword">raise</span> ValueError(<br>                    <span class="hljs-string">&quot;The number of image start tokens and image end tokens should be the same.&quot;</span>)<br>            <span class="hljs-comment"># 找到图像开始token的位置</span><br>            image_start_tokens = torch.where(<br>                cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_start_token)[<span class="hljs-number">0</span>]<br>            <span class="hljs-keyword">for</span> image_start_token_pos <span class="hljs-keyword">in</span> image_start_tokens:<br>                cur_image_features = vision_hidden_states[cur_image_idx].to(<br>                    device=cur_input_embeds.device)<br>                num_patches = cur_image_features.shape[<span class="hljs-number">0</span>]<br>                <span class="hljs-comment"># 检查图像结束token是否紧随图像开始token</span><br>                <span class="hljs-keyword">if</span> cur_input_ids[image_start_token_pos + num_patches + <span class="hljs-number">1</span>] != <span class="hljs-variable language_">self</span>.vision_config.im_end_token:<br>                    <span class="hljs-keyword">raise</span> ValueError(<br>                        <span class="hljs-string">&quot;The image end token should follow the image start token.&quot;</span>)<br>                <span class="hljs-comment"># 如果存在原始嵌入参数，则拼接嵌入</span><br>                <span class="hljs-keyword">if</span> orig_embeds_params <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos].detach(), cur_input_embeds[image_start_token_pos:image_start_token_pos+<span class="hljs-number">1</span>], cur_image_features,<br>                                                     cur_input_embeds[image_start_token_pos + num_patches + <span class="hljs-number">1</span>:image_start_token_pos + num_patches + <span class="hljs-number">2</span>], cur_input_embeds[image_start_token_pos + num_patches + <span class="hljs-number">2</span>:].detach()), dim=<span class="hljs-number">0</span>)<br>                <span class="hljs-keyword">else</span>:<br>                    cur_new_input_embeds = torch.cat(<br>                        (cur_input_embeds[:image_start_token_pos+<span class="hljs-number">1</span>], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + <span class="hljs-number">1</span>:]), dim=<span class="hljs-number">0</span>)<br>                cur_image_idx += <span class="hljs-number">1</span><br>            new_input_embeds.append(cur_new_input_embeds)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> NotImplementedError<br>    inputs_embeds = torch.stack(new_input_embeds, dim=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">return</span> inputs_embeds, vision_hidden_states<br></code></pre></td></tr></table></figure><ul><li>该方法用于从输入数据中提取视觉特征，并将其与文本嵌入结合，生成多模态输入嵌入。</li></ul><hr><h4 id="6-4-forward-方法"><a href="#6-4-forward-方法" class="headerlink" title="6.4 forward 方法"></a><strong>6.4 forward 方法</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    input_ids: torch.LongTensor = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    past_key_values: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[torch.FloatTensor]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inputs_embeds: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    use_cache: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    output_attentions: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    output_hidden_states: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    images: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    return_dict: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    **kwargs</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Union</span>[<span class="hljs-type">Tuple</span>, BaseModelOutputWithPast]:<br>    <span class="hljs-comment"># <span class="hljs-doctag">HACK:</span> 替换回原始嵌入以支持LLaVA预训练</span><br>    orig_embeds_params = <span class="hljs-built_in">getattr</span>(<span class="hljs-variable language_">self</span>, <span class="hljs-string">&#x27;orig_embeds_params&#x27;</span>, <span class="hljs-literal">None</span>)<br><br>    <span class="hljs-keyword">if</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> past_key_values <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        inputs_embeds = <span class="hljs-variable language_">self</span>.embed_tokens(input_ids)  <span class="hljs-comment"># 获取输入token的嵌入</span><br><br>        vision_tower = <span class="hljs-built_in">getattr</span>(<span class="hljs-variable language_">self</span>, <span class="hljs-string">&#x27;vision_tower&#x27;</span>, <span class="hljs-literal">None</span>)<br>        <span class="hljs-comment"># 如果存在视觉模型且输入不是单token或处于训练模式，并且有图像输入</span><br>        <span class="hljs-keyword">if</span> vision_tower <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> (input_ids.shape[<span class="hljs-number">1</span>] != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> <span class="hljs-variable language_">self</span>.training) <span class="hljs-keyword">and</span> images <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(images) <span class="hljs-keyword">is</span> <span class="hljs-built_in">list</span>:<br>                image_features = []<br>                <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> images:<br>                    image_forward_out = <span class="hljs-variable language_">self</span>.get_vision_embedding(image.unsqueeze(<span class="hljs-number">0</span>))[<span class="hljs-number">0</span>]<br>                    image_features.append(image_forward_out)<br>            <span class="hljs-keyword">else</span>:<br>                image_features = <span class="hljs-variable language_">self</span>.get_vision_embedding(images)<br><br>            <span class="hljs-comment"># 创建虚拟图像特征</span><br>            dummy_image_features = torch.zeros(<br>                <span class="hljs-variable language_">self</span>.config.num_query,<br>                <span class="hljs-variable language_">self</span>.config.hidden_size,<br>                device=inputs_embeds.device,<br>                dtype=inputs_embeds.dtype)<br><br>            new_input_embeds = []<br>            cur_image_idx = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> cur_input_ids, cur_input_embeds <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(input_ids, inputs_embeds):<br>                <span class="hljs-comment"># 如果当前输入中没有图像patch token，则直接使用原始嵌入</span><br>                <span class="hljs-keyword">if</span> (cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_patch_token).<span class="hljs-built_in">sum</span>() == <span class="hljs-number">0</span>:<br>                    cur_input_embeds = cur_input_embeds + (<span class="hljs-number">0.</span> * dummy_image_features).<span class="hljs-built_in">sum</span>()<br>                    new_input_embeds.append(cur_input_embeds)<br>                    <span class="hljs-keyword">continue</span><br><br>                <span class="hljs-comment"># 如果使用图像开始和结束token</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.vision_config.use_im_start_end:<br>                    cur_image_features = image_features[cur_image_idx]<br>                    num_patches = cur_image_features.shape[<span class="hljs-number">0</span>]<br>                    <span class="hljs-comment"># 检查图像开始和结束token的数量是否一致</span><br>                    <span class="hljs-keyword">if</span> (cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_start_token).<span class="hljs-built_in">sum</span>() != (cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_end_token).<span class="hljs-built_in">sum</span>():<br>                        <span class="hljs-keyword">raise</span> ValueError(<br>                            <span class="hljs-string">&quot;The number of image start tokens and image end tokens should be the same.&quot;</span>)<br>                    <span class="hljs-comment"># 找到图像开始token的位置</span><br>                    image_start_tokens = torch.where(<br>                        cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_start_token)[<span class="hljs-number">0</span>]<br>                    <span class="hljs-keyword">for</span> image_start_token_pos <span class="hljs-keyword">in</span> image_start_tokens:<br>                        cur_image_features = image_features[cur_image_idx].to(<br>                            device=cur_input_embeds.device)<br>                        num_patches = cur_image_features.shape[<span class="hljs-number">0</span>]<br>                        <span class="hljs-comment"># 检查图像结束token是否紧随图像开始token</span><br>                        <span class="hljs-keyword">if</span> cur_input_ids[image_start_token_pos + num_patches + <span class="hljs-number">1</span>] != <span class="hljs-variable language_">self</span>.vision_config.im_end_token:<br>                            <span class="hljs-keyword">raise</span> ValueError(<br>                                <span class="hljs-string">&quot;The image end token should follow the image start token.&quot;</span>)<br>                        <span class="hljs-comment"># 如果存在原始嵌入参数，则拼接嵌入</span><br>                        <span class="hljs-keyword">if</span> orig_embeds_params <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                            cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos].detach(), cur_input_embeds[image_start_token_pos:image_start_token_pos+<span class="hljs-number">1</span>], cur_image_features,<br>                                                             cur_input_embeds[image_start_token_pos + num_patches + <span class="hljs-number">1</span>:image_start_token_pos + num_patches + <span class="hljs-number">2</span>], cur_input_embeds[image_start_token_pos + num_patches + <span class="hljs-number">2</span>:].detach()), dim=<span class="hljs-number">0</span>)<br>                        <span class="hljs-keyword">else</span>:<br>                            cur_new_input_embeds = torch.cat(<br>                                (cur_input_embeds[:image_start_token_pos+<span class="hljs-number">1</span>], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + <span class="hljs-number">1</span>:]), dim=<span class="hljs-number">0</span>)<br>                        cur_image_idx += <span class="hljs-number">1</span><br>                    new_input_embeds.append(cur_new_input_embeds)<br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">raise</span> NotImplementedError<br>            inputs_embeds = torch.stack(new_input_embeds, dim=<span class="hljs-number">0</span>)<br>            input_ids = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># 调用父类的forward方法</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>(OmniLMMModel, <span class="hljs-variable language_">self</span>).forward(<br>        input_ids=input_ids, attention_mask=attention_mask, past_key_values=past_key_values,<br>        inputs_embeds=inputs_embeds, use_cache=use_cache,<br>        output_attentions=output_attentions, output_hidden_states=output_hidden_states,<br>        return_dict=return_dict,<br>        **kwargs<br>    )<br></code></pre></td></tr></table></figure><ul><li>该方法用于模型的前向传播，处理多模态输入（文本和图像），并调用父类的 <code>forward</code> 方法。</li></ul><hr><h3 id="7-OmniLMMForCausalLM-类"><a href="#7-OmniLMMForCausalLM-类" class="headerlink" title="7. OmniLMMForCausalLM 类"></a><strong>7. OmniLMMForCausalLM 类</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">OmniLMMForCausalLM</span>(<span class="hljs-title class_ inherited__">MistralForCausalLM</span>):<br>    config_class = OmniLMMConfig  <span class="hljs-comment"># 配置类为OmniLMMConfig</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, mm_vision_tower=<span class="hljs-literal">None</span>, tune_clip=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>(MistralForCausalLM, <span class="hljs-variable language_">self</span>).__init__(config)  <span class="hljs-comment"># 调用父类构造函数</span><br>        <span class="hljs-variable language_">self</span>.model = OmniLMMModel(<br>            config, mm_vision_tower=mm_vision_tower, tune_clip=tune_clip)  <span class="hljs-comment"># 初始化OmniLMM模型</span><br><br>        <span class="hljs-variable language_">self</span>.lm_head = nn.Linear(<br>            config.hidden_size, config.vocab_size, bias=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># 定义语言模型头</span><br><br>        <span class="hljs-comment"># 初始化权重并应用最终处理</span><br>        <span class="hljs-variable language_">self</span>.post_init()<br></code></pre></td></tr></table></figure><ul><li>继承自<code>MistralForCausalLM</code>，用于实现因果语言模型。</li><li>初始化时创建OmniLMM模型，并定义语言模型头。</li></ul><hr><h4 id="7-1-forward-方法"><a href="#7-1-forward-方法" class="headerlink" title="7.1 forward 方法"></a><strong>7.1 forward 方法</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    input_ids: torch.LongTensor = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    past_key_values: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[torch.FloatTensor]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inputs_embeds: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    labels: <span class="hljs-type">Optional</span>[torch.LongTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    use_cache: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    output_attentions: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    output_hidden_states: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    images: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    return_dict: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    **kwargs</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Union</span>[<span class="hljs-type">Tuple</span>, CausalLMOutputWithPast]:<br>    output_attentions = output_attentions <span class="hljs-keyword">if</span> output_attentions <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.config.output_attentions<br>    output_hidden_states = (<br>        output_hidden_states <span class="hljs-keyword">if</span> output_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.config.output_hidden_states<br>    )<br>    return_dict = return_dict <span class="hljs-keyword">if</span> return_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.config.use_return_dict<br><br>    <span class="hljs-comment"># 调用模型进行前向传播</span><br>    outputs = <span class="hljs-variable language_">self</span>.model(<br>        input_ids=input_ids,<br>        attention_mask=attention_mask,<br>        past_key_values=past_key_values,<br>        inputs_embeds=inputs_embeds,<br>        use_cache=use_cache,<br>        output_attentions=output_attentions,<br>        output_hidden_states=output_hidden_states,<br>        return_dict=return_dict,<br>        images=images,<br>        **kwargs<br>    )<br><br>    hidden_states = outputs[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 获取隐藏状态</span><br>    logits = <span class="hljs-variable language_">self</span>.lm_head(hidden_states)  <span class="hljs-comment"># 计算logits</span><br><br>    loss = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># 计算损失</span><br>        shift_logits = logits[..., :-<span class="hljs-number">1</span>, :].contiguous()<br>        shift_labels = labels[..., <span class="hljs-number">1</span>:].contiguous()<br>        loss_fct = CrossEntropyLoss()<br>        shift_logits = shift_logits.view(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.config.vocab_size)<br>        shift_labels = shift_labels.view(-<span class="hljs-number">1</span>)<br>        shift_labels = shift_labels.to(shift_logits.device)<br>        loss = loss_fct(shift_logits, shift_labels)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_dict:<br>        output = (logits,) + outputs[<span class="hljs-number">1</span>:]<br>        <span class="hljs-keyword">return</span> (loss,) + output <span class="hljs-keyword">if</span> loss <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> output<br><br>    <span class="hljs-keyword">return</span> CausalLMOutputWithPast(<br>        loss=loss,<br>        logits=logits,<br>        past_key_values=outputs.past_key_values,<br>        hidden_states=outputs.hidden_states,<br>        attentions=outputs.attentions,<br>    )<br></code></pre></td></tr></table></figure><ul><li>该方法用于模型的前向传播，计算logits和损失。</li></ul><hr><h4 id="7-2-prepare-inputs-for-generation-方法"><a href="#7-2-prepare-inputs-for-generation-方法" class="headerlink" title="7.2 prepare_inputs_for_generation 方法"></a><strong>7.2 prepare_inputs_for_generation 方法</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_inputs_for_generation</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self, input_ids, past_key_values=<span class="hljs-literal">None</span>, attention_mask=<span class="hljs-literal">None</span>, inputs_embeds=<span class="hljs-literal">None</span>, **kwargs</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-keyword">if</span> past_key_values:<br>        input_ids = input_ids[:, -<span class="hljs-number">1</span>:]  <span class="hljs-comment"># 如果存在过去的键值对，则只使用最后一个token</span><br><br>    <span class="hljs-comment"># 如果传递了inputs_embeds，则只在第一步生成时使用</span><br>    <span class="hljs-keyword">if</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> past_key_values <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        model_inputs = &#123;<span class="hljs-string">&quot;inputs_embeds&quot;</span>: inputs_embeds&#125;<br>    <span class="hljs-keyword">else</span>:<br>        model_inputs = &#123;<span class="hljs-string">&quot;input_ids&quot;</span>: input_ids&#125;<br><br>    <span class="hljs-comment"># 更新模型输入</span><br>    model_inputs.update(<br>        &#123;<br>            <span class="hljs-string">&quot;past_key_values&quot;</span>: past_key_values,<br>            <span class="hljs-string">&quot;use_cache&quot;</span>: kwargs.get(<span class="hljs-string">&quot;use_cache&quot;</span>),<br>            <span class="hljs-string">&quot;attention_mask&quot;</span>: attention_mask,<br>            <span class="hljs-string">&quot;images&quot;</span>: kwargs.get(<span class="hljs-string">&quot;images&quot;</span>, <span class="hljs-literal">None</span>),<br>        &#125;<br>    )<br>    <span class="hljs-keyword">return</span> model_inputs<br></code></pre></td></tr></table></figure><ul><li>该方法用于为生成任务准备输入数据。</li></ul><hr><h4 id="7-3-generate-vllm-方法"><a href="#7-3-generate-vllm-方法" class="headerlink" title="7.3 generate_vllm 方法"></a><strong>7.3 generate_vllm 方法</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_vllm</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    input_ids: torch.LongTensor = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    images: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    vision_hidden_states=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    return_vision_hidden_states=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    **kwargs</span><br><span class="hljs-params"></span>):<br>    model_inputs = &#123;<span class="hljs-string">&#x27;input_ids&#x27;</span>: input_ids&#125;<br>    <span class="hljs-keyword">if</span> vision_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        model_inputs[<span class="hljs-string">&#x27;pixel_values&#x27;</span>] = images  <span class="hljs-comment"># 如果没有视觉隐藏状态，则传递像素值</span><br>    <span class="hljs-keyword">else</span>:<br>        model_inputs[<span class="hljs-string">&#x27;vision_hidden_states&#x27;</span>] = vision_hidden_states  <span class="hljs-comment"># 否则传递视觉隐藏状态</span><br><br>    <span class="hljs-keyword">with</span> torch.inference_mode():<br>        <span class="hljs-comment"># 获取输入嵌入和视觉隐藏状态</span><br>        inputs_embeds, vision_hidden_states = <span class="hljs-variable language_">self</span>.model.get_vllm_embedding(model_inputs)<br><br>        <span class="hljs-comment"># 生成文本</span><br>        result = <span class="hljs-variable language_">self</span>.generate(<br>            inputs_embeds=inputs_embeds,<br>            **kwargs<br>        )<br><br>    <span class="hljs-keyword">if</span> return_vision_hidden_states:<br>        <span class="hljs-keyword">return</span> result, vision_hidden_states  <span class="hljs-comment"># 如果需要返回视觉隐藏状态，则一起返回</span><br><br>    <span class="hljs-keyword">return</span> result  <span class="hljs-comment"># 否则只返回生成结果</span><br></code></pre></td></tr></table></figure><ul><li>该方法用于生成文本，支持视觉输入，并可以选择返回视觉隐藏状态。</li></ul><hr><h4 id="7-4-initialize-vision-tokenizer-方法"><a href="#7-4-initialize-vision-tokenizer-方法" class="headerlink" title="7.4 initialize_vision_tokenizer 方法"></a><strong>7.4 initialize_vision_tokenizer 方法</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_vision_tokenizer</span>(<span class="hljs-params">self, mm_use_im_start_end, tokenizer, device,</span><br><span class="hljs-params">                                tune_mm_mlp_adapter=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-variable language_">self</span>.model.vision_config.use_im_start_end = mm_use_im_start_end  <span class="hljs-comment"># 设置是否使用图像开始和结束token</span><br>    tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 添加图像patch token</span><br>    <span class="hljs-variable language_">self</span>.resize_token_embeddings(<span class="hljs-built_in">len</span>(tokenizer))  <span class="hljs-comment"># 调整token嵌入的大小</span><br><br>    <span class="hljs-keyword">if</span> mm_use_im_start_end:<br>        <span class="hljs-comment"># 添加图像开始和结束token</span><br>        num_new_tokens = tokenizer.add_tokens(<br>            [DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.resize_token_embeddings(<span class="hljs-built_in">len</span>(tokenizer))<br>        <span class="hljs-variable language_">self</span>.model.vision_config.im_start_token, <span class="hljs-variable language_">self</span>.model.vision_config.im_end_token = tokenizer.convert_tokens_to_ids(<br>            [DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])<br><br>        <span class="hljs-keyword">if</span> num_new_tokens &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-comment"># 对新token的嵌入进行平均初始化</span><br>            input_embeddings = <span class="hljs-variable language_">self</span>.get_input_embeddings().weight.data<br>            output_embeddings = <span class="hljs-variable language_">self</span>.get_output_embeddings().weight.data<br><br>            input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(<br>                dim=<span class="hljs-number">0</span>, keepdim=<span class="hljs-literal">True</span>)<br>            output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(<br>                dim=<span class="hljs-number">0</span>, keepdim=<span class="hljs-literal">True</span>)<br><br>            input_embeddings[-num_new_tokens:] = input_embeddings_avg<br>            output_embeddings[-num_new_tokens:] = output_embeddings_avg<br><br>        <span class="hljs-comment"># 添加其他特殊token</span><br>        num_new_tokens = tokenizer.add_tokens(<br>            [<span class="hljs-string">&#x27;&lt;box&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;/box&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;ref&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;/ref&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;quad&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;/quad&gt;&#x27;</span>], special_tokens=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.resize_token_embeddings(<span class="hljs-built_in">len</span>(tokenizer))<br><br>        <span class="hljs-keyword">if</span> num_new_tokens &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-comment"># 对新token的嵌入进行平均初始化</span><br>            input_embeddings = <span class="hljs-variable language_">self</span>.get_input_embeddings().weight.data<br>            output_embeddings = <span class="hljs-variable language_">self</span>.get_output_embeddings().weight.data<br><br>            input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(<br>                dim=<span class="hljs-number">0</span>, keepdim=<span class="hljs-literal">True</span>)<br>            output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(<br>                dim=<span class="hljs-number">0</span>, keepdim=<span class="hljs-literal">True</span>)<br><br>            input_embeddings[-num_new_tokens:] = input_embeddings_avg<br>            output_embeddings[-num_new_tokens:] = output_embeddings_avg<br><br>        <span class="hljs-keyword">if</span> tune_mm_mlp_adapter:<br>            <span class="hljs-comment"># 如果调优MLP适配器，则设置原始嵌入参数</span><br>            <span class="hljs-variable language_">self</span>.model.orig_embeds_params = [<br>                <span class="hljs-variable language_">self</span>.get_input_embeddings().weight.data.clone().to(device=device)]<br>            <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.get_input_embeddings().parameters():<br>                p.requires_grad = <span class="hljs-literal">True</span><br>            <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.get_output_embeddings().parameters():<br>                p.requires_grad = <span class="hljs-literal">False</span><br><br>    <span class="hljs-comment"># 设置图像patch token的ID</span><br>    <span class="hljs-variable language_">self</span>.model.vision_config.im_patch_token = tokenizer.convert_tokens_to_ids(<br>        [DEFAULT_IMAGE_PATCH_TOKEN])[<span class="hljs-number">0</span>]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Tokenizer: <span class="hljs-subst">&#123;tokenizer&#125;</span>\n patch_token_id: <span class="hljs-subst">&#123;self.model.vision_config.im_patch_token&#125;</span>, visoin_config: <span class="hljs-subst">&#123;self.model.vision_config&#125;</span>&#x27;</span>, flush=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><ul><li>该方法用于初始化视觉tokenizer，添加特殊token并调整嵌入。</li></ul><hr><h3 id="8-注册模型"><a href="#8-注册模型" class="headerlink" title="8. 注册模型"></a><strong>8. 注册模型</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">AutoConfig.register(<span class="hljs-string">&quot;omnilmm&quot;</span>, OmniLMMConfig)  <span class="hljs-comment"># 注册OmniLMMConfig</span><br>AutoModelForCausalLM.register(OmniLMMConfig, OmniLMMForCausalLM)  <span class="hljs-comment"># 注册OmniLMMForCausalLM</span><br></code></pre></td></tr></table></figure><ul><li>将OmniLMM模型注册到Hugging Face的自动配置和模型工厂中。</li></ul><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>multi-modal</category>
      
    </categories>
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>llm</tag>
      
      <tag>源码解析</tag>
      
      <tag>MiniCPM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MiniCPM-V多模态模型源码解析-03：模型推理详细流程</title>
    <link href="/2025/01/14/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9AMiniCPM-V%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-03%EF%BC%9A%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B/"/>
    <url>/2025/01/14/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9AMiniCPM-V%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-03%EF%BC%9A%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="1-MiniCPM-V-推理流程图"><a href="#1-MiniCPM-V-推理流程图" class="headerlink" title="1. MiniCPM-V 推理流程图"></a>1. <strong>MiniCPM-V 推理流程图</strong></h2><pre><code class=" mermaid">graph TD    A[输入: 图像和文本] --&gt; B[图像预处理]    A --&gt; C[文本预处理]    B --&gt; D[图像编码器 Vision Tower]    C --&gt; E[文本编码器 Tokenizer]    D --&gt; F[图像特征重采样 Resampler]    E --&gt; G[文本特征编码]    F --&gt; H[多模态特征融合 OmniLMMModel]    G --&gt; H    H --&gt; I[生成文本输出 OmniLMMForCausalLM]    I --&gt; J[输出: 生成的文本]    subgraph 图像处理模块        B --&gt; D        D --&gt; F    end    subgraph 文本处理模块        C --&gt; E        E --&gt; G    end    subgraph 多模态融合模块        F --&gt; H        G --&gt; H    end    subgraph 模型核心模块        H --&gt; I    end    subgraph 输出模块        I --&gt; J    end    %% 详细步骤    B --&gt; B1[图像缩放]    B --&gt; B2[图像裁剪]    B --&gt; B3[图像归一化]    B1 --&gt; D    B2 --&gt; D    B3 --&gt; D    C --&gt; C1[文本分词]    C --&gt; C2[文本编码]    C1 --&gt; E    C2 --&gt; E    D --&gt; D1[提取图像特征]    D1 --&gt; F    E --&gt; E1[生成 input_ids]    E1 --&gt; G    F --&gt; F1[重采样图像特征]    F1 --&gt; H    G --&gt; G1[生成文本特征]    G1 --&gt; H    H --&gt; H1[拼接图像和文本特征]    H1 --&gt; H2[Transformer 编码]    H2 --&gt; I    I --&gt; I1[生成文本 token]    I1 --&gt; I2[自回归生成文本]    I2 --&gt; J</code></pre><hr><h2 id="2-推理流程的详细说明"><a href="#2-推理流程的详细说明" class="headerlink" title="2. 推理流程的详细说明"></a>2. <strong>推理流程的详细说明</strong></h2><h3 id="2-1-输入-图像和文本"><a href="#2-1-输入-图像和文本" class="headerlink" title="2.1 输入: 图像和文本"></a>2.1 <strong>输入: 图像和文本</strong></h3><ul><li><strong>输入</strong>：<ul><li>图像：RGB 图像文件（如 JPEG 或 PNG）。</li><li>文本：对话格式的列表，包含用户和助手的对话轮次。</li></ul></li></ul><h3 id="2-2-图像预处理"><a href="#2-2-图像预处理" class="headerlink" title="2.2 图像预处理"></a>2.2 <strong>图像预处理</strong></h3><ul><li><p><strong>步骤</strong>：</p><ol><li><strong>图像缩放</strong>：将图像调整到模型指定的输入尺寸（如 224x224）。</li><li><strong>图像裁剪</strong>：对图像进行中心裁剪。</li><li><strong>图像归一化</strong>：使用预定义的均值和标准差对图像进行归一化。</li></ol></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">image = Image.<span class="hljs-built_in">open</span>(io.BytesIO(base64.b64decode(<span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;image&#x27;</span>]))).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>image = <span class="hljs-variable language_">self</span>.image_transform(image)<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-3-图像编码器-Vision-Tower"><a href="#2-3-图像编码器-Vision-Tower" class="headerlink" title="2.3 图像编码器 (Vision Tower)"></a>2.3 <strong>图像编码器 (Vision Tower)</strong></h3><ul><li><p><strong>作用</strong>：将预处理后的图像转换为图像特征。</p></li><li><p><strong>实现</strong>：</p><ul><li>使用预训练的视觉模型（如 EVA02）提取图像特征。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">vision_embedding = vision_tower.forward_features(pixel_values.<span class="hljs-built_in">type</span>(dtype))<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-4-图像特征重采样-Resampler"><a href="#2-4-图像特征重采样-Resampler" class="headerlink" title="2.4 图像特征重采样 (Resampler)"></a>2.4 <strong>图像特征重采样 (Resampler)</strong></h3><ul><li><p><strong>作用</strong>：对图像特征进行重采样，使其与文本特征的维度匹配。</p></li><li><p><strong>实现</strong>：</p><ul><li>使用多头注意力机制对图像特征进行重采样。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">res = <span class="hljs-variable language_">self</span>.resampler(vision_embedding)<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-5-文本预处理"><a href="#2-5-文本预处理" class="headerlink" title="2.5 文本预处理"></a>2.5 <strong>文本预处理</strong></h3><ul><li><p><strong>步骤</strong>：</p><ol><li><strong>文本分词</strong>：将文本分割为 token。</li><li><strong>文本编码</strong>：将 token 转换为 <code>input_ids</code>。</li></ol></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">msgs = json.loads(<span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;question&#x27;</span>])<br>input_ids = wrap_question_for_omni_lmm(msgs, <span class="hljs-variable language_">self</span>.image_token_len, <span class="hljs-variable language_">self</span>.tokenizer)[<span class="hljs-string">&#x27;input_ids&#x27;</span>]<br>input_ids = torch.as_tensor(input_ids)<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-6-文本编码器-Tokenizer"><a href="#2-6-文本编码器-Tokenizer" class="headerlink" title="2.6 文本编码器 (Tokenizer)"></a>2.6 <strong>文本编码器 (Tokenizer)</strong></h3><ul><li><p><strong>作用</strong>：将预处理后的文本转换为文本特征。</p></li><li><p><strong>实现</strong>：</p><ul><li>使用 Transformer 模型（如 Mistral）对 <code>input_ids</code> 进行编码，生成文本特征。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs_embeds = <span class="hljs-variable language_">self</span>.embed_tokens(data[<span class="hljs-string">&#x27;input_ids&#x27;</span>])<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-7-多模态特征融合-OmniLMMModel"><a href="#2-7-多模态特征融合-OmniLMMModel" class="headerlink" title="2.7 多模态特征融合 (OmniLMMModel)"></a>2.7 <strong>多模态特征融合 (OmniLMMModel)</strong></h3><ul><li><p><strong>作用</strong>：将图像特征和文本特征拼接在一起，形成联合特征。</p></li><li><p><strong>实现</strong>：</p><ul><li>如果文本中包含 <code>&lt;image&gt;</code> 标记，系统会将图像特征插入到对应的位置。</li><li>拼接后的联合特征形状为 <code>(batch_size, sequence_length + num_queries, embed_dim)</code>。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python">new_input_embeds = []<br>cur_image_idx = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> cur_input_ids, cur_input_embeds <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(input_ids, inputs_embeds):<br>    <span class="hljs-keyword">if</span> (cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_patch_token).<span class="hljs-built_in">sum</span>() == <span class="hljs-number">0</span>:<br>        cur_input_embeds = cur_input_embeds + (<span class="hljs-number">0.</span> * dummy_image_features).<span class="hljs-built_in">sum</span>()<br>        new_input_embeds.append(cur_input_embeds)<br>        <span class="hljs-keyword">continue</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.vision_config.use_im_start_end:<br>        cur_image_features = vision_hidden_states[cur_image_idx]<br>        num_patches = cur_image_features.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">if</span> (cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_start_token).<span class="hljs-built_in">sum</span>() != (cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_end_token).<span class="hljs-built_in">sum</span>():<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;The number of image start tokens and image end tokens should be the same.&quot;</span>)<br>        image_start_tokens = torch.where(cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_start_token)[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">for</span> image_start_token_pos <span class="hljs-keyword">in</span> image_start_tokens:<br>            cur_image_features = vision_hidden_states[cur_image_idx].to(device=cur_input_embeds.device)<br>            num_patches = cur_image_features.shape[<span class="hljs-number">0</span>]<br>            <span class="hljs-keyword">if</span> cur_input_ids[image_start_token_pos + num_patches + <span class="hljs-number">1</span>] != <span class="hljs-variable language_">self</span>.vision_config.im_end_token:<br>                <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;The image end token should follow the image start token.&quot;</span>)<br>            <span class="hljs-keyword">if</span> orig_embeds_params <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos].detach(), cur_input_embeds[image_start_token_pos:image_start_token_pos+<span class="hljs-number">1</span>], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + <span class="hljs-number">1</span>:image_start_token_pos + num_patches + <span class="hljs-number">2</span>], cur_input_embeds[image_start_token_pos + num_patches + <span class="hljs-number">2</span>:].detach()), dim=<span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">else</span>:<br>                cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos+<span class="hljs-number">1</span>], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + <span class="hljs-number">1</span>:]), dim=<span class="hljs-number">0</span>)<br>            cur_image_idx += <span class="hljs-number">1</span><br>        new_input_embeds.append(cur_new_input_embeds)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br>inputs_embeds = torch.stack(new_input_embeds, dim=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-8-生成文本输出-OmniLMMForCausalLM"><a href="#2-8-生成文本输出-OmniLMMForCausalLM" class="headerlink" title="2.8 生成文本输出 (OmniLMMForCausalLM)"></a>2.8 <strong>生成文本输出 (OmniLMMForCausalLM)</strong></h3><ul><li><p><strong>作用</strong>：根据联合特征生成文本输出。</p></li><li><p><strong>实现</strong>：</p><ul><li>使用 <code>lm_head</code> 对联合特征进行解码，生成文本 token。</li><li>通过自回归生成逐步生成文本输出。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">output = <span class="hljs-variable language_">self</span>.model.generate_vllm(<br>    input_ids=input_ids.unsqueeze(<span class="hljs-number">0</span>).cuda(),<br>    images=image.unsqueeze(<span class="hljs-number">0</span>).half().cuda(),<br>    temperature=<span class="hljs-number">0.6</span>,<br>    max_new_tokens=<span class="hljs-number">1024</span>,<br>    do_sample=<span class="hljs-literal">True</span>,<br>    output_scores=<span class="hljs-literal">True</span>,<br>    return_dict_in_generate=<span class="hljs-literal">True</span>,<br>    repetition_penalty=<span class="hljs-number">1.1</span>,<br>    top_k=<span class="hljs-number">30</span>,<br>    top_p=<span class="hljs-number">0.9</span>,<br>)<br>response = <span class="hljs-variable language_">self</span>.tokenizer.decode(output.sequences[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)<br>response = response.strip()<br></code></pre></td></tr></table></figure></li></ul><h3 id="2-9-输出-生成的文本"><a href="#2-9-输出-生成的文本" class="headerlink" title="2.9 输出: 生成的文本"></a>2.9 <strong>输出: 生成的文本</strong></h3><ul><li><strong>输出</strong>：生成的文本被返回给用户。</li></ul><hr><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. <strong>总结</strong></h2><ul><li><strong>推理流程</strong>：<ol><li><strong>图像预处理</strong>：对图像进行缩放、裁剪和归一化。</li><li><strong>图像编码</strong>：使用 Vision Tower 提取图像特征。</li><li><strong>图像特征重采样</strong>：使用 Resampler 对图像特征进行重采样。</li><li><strong>文本预处理</strong>：对文本进行分词和编码。</li><li><strong>文本编码</strong>：使用 Tokenizer 生成文本特征。</li><li><strong>多模态特征融合</strong>：将图像特征和文本特征拼接在一起。</li><li><strong>生成文本输出</strong>：使用 OmniLMMForCausalLM 生成文本输出。</li><li><strong>输出</strong>：生成的文本返回给用户。</li></ol></li></ul><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>multi-modal</category>
      
    </categories>
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>llm</tag>
      
      <tag>源码解析</tag>
      
      <tag>MiniCPM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MiniCPM-V多模态模型源码解析-02：模型训练详细流程</title>
    <link href="/2025/01/13/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9AMiniCPM-V%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-02%EF%BC%9A%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B/"/>
    <url>/2025/01/13/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9AMiniCPM-V%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-02%EF%BC%9A%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="1-详细训练流程"><a href="#1-详细训练流程" class="headerlink" title="1. 详细训练流程"></a>1. 详细训练流程</h2><pre><code class=" mermaid">graph TD    A[输入: 图像和文本] --&gt; B[图像预处理]    A --&gt; C[文本预处理]    B --&gt; D[图像编码器 Vision Tower]    C --&gt; E[文本编码器 Tokenizer]    D --&gt; F[图像特征重采样 Resampler]    E --&gt; G[文本特征编码]    F --&gt; H[多模态特征融合 OmniLMMModel]    G --&gt; H    H --&gt; I[生成文本输出 OmniLMMForCausalLM]    I --&gt; J[输出: 生成的文本]    subgraph 图像处理模块        B[图像预处理] --&gt; D[图像编码器 Vision Tower]        D --&gt; F[图像特征重采样 Resampler]    end    subgraph 文本处理模块        C[文本预处理] --&gt; E[文本编码器 Tokenizer]        E --&gt; G[文本特征编码]    end    subgraph 多模态融合模块        F[图像特征重采样 Resampler] --&gt; H[多模态特征融合 OmniLMMModel]        G[文本特征编码] --&gt; H    end    subgraph 模型核心模块        H[多模态特征融合 OmniLMMModel] --&gt; I[生成文本输出 OmniLMMForCausalLM]    end    subgraph 输出模块        I[生成文本输出 OmniLMMForCausalLM] --&gt; J[输出: 生成的文本]    end    %% 详细步骤    B --&gt; B1[图像缩放]    B --&gt; B2[图像裁剪]    B --&gt; B3[图像归一化]    B1 --&gt; D    B2 --&gt; D    B3 --&gt; D    C --&gt; C1[文本分词]    C --&gt; C2[文本编码]    C1 --&gt; E    C2 --&gt; E    D --&gt; D1[提取图像特征]    D1 --&gt; F    E --&gt; E1[生成 input_ids]    E1 --&gt; G    F --&gt; F1[重采样图像特征]    F1 --&gt; H    G --&gt; G1[生成文本特征]    G1 --&gt; H    H --&gt; H1[拼接图像和文本特征]    H1 --&gt; H2[Transformer 编码]    H2 --&gt; I    I --&gt; I1[生成文本 token]    I1 --&gt; I2[自回归生成文本]    I2 --&gt; J</code></pre><h3 id="流程图说明"><a href="#流程图说明" class="headerlink" title="流程图说明"></a>流程图说明</h3><h4 id="1-输入-图像和文本"><a href="#1-输入-图像和文本" class="headerlink" title="1. 输入: 图像和文本"></a>1. <strong>输入: 图像和文本</strong></h4><ul><li>输入包括图像和文本数据，分别进入图像处理模块和文本处理模块。</li></ul><h4 id="2-图像处理模块"><a href="#2-图像处理模块" class="headerlink" title="2. 图像处理模块"></a>2. <strong>图像处理模块</strong></h4><ul><li><strong>图像预处理</strong>：<ul><li><strong>图像缩放</strong>：将图像调整到模型指定的输入尺寸（如 224x224）。</li><li><strong>图像裁剪</strong>：对图像进行中心裁剪。</li><li><strong>图像归一化</strong>：使用预定义的均值和标准差对图像进行归一化。</li></ul></li><li>**图像编码器 (Vision Tower)**：<ul><li>使用预训练的视觉模型（如 EVA02）提取图像特征。</li></ul></li><li>**图像特征重采样 (Resampler)**：<ul><li>使用多头注意力机制对图像特征进行重采样，使其与文本特征的维度匹配。</li></ul></li></ul><h4 id="3-文本处理模块"><a href="#3-文本处理模块" class="headerlink" title="3. 文本处理模块"></a>3. <strong>文本处理模块</strong></h4><ul><li><strong>文本预处理</strong>：<ul><li><strong>文本分词</strong>：将文本分割为 token。</li><li><strong>文本编码</strong>：将 token 转换为 <code>input_ids</code>。</li></ul></li><li>**文本编码器 (Tokenizer)**：<ul><li>使用 Transformer 模型（如 Mistral）对 <code>input_ids</code> 进行编码，生成文本特征。</li></ul></li></ul><h4 id="4-多模态融合模块"><a href="#4-多模态融合模块" class="headerlink" title="4. 多模态融合模块"></a>4. <strong>多模态融合模块</strong></h4><ul><li>**多模态特征融合 (OmniLMMModel)**：<ul><li>将图像特征和文本特征拼接在一起，形成联合特征。</li><li>使用 Transformer 的自注意力机制对联合特征进行编码。</li></ul></li></ul><h4 id="5-模型核心模块"><a href="#5-模型核心模块" class="headerlink" title="5. 模型核心模块"></a>5. <strong>模型核心模块</strong></h4><ul><li>**生成文本输出 (OmniLMMForCausalLM)**：<ul><li>使用 <code>lm_head</code> 对联合特征进行解码，生成文本 token。</li><li>通过自回归生成逐步生成文本输出。</li></ul></li></ul><h4 id="6-输出模块"><a href="#6-输出模块" class="headerlink" title="6. 输出模块"></a>6. <strong>输出模块</strong></h4><ul><li><strong>输出: 生成的文本</strong>：<ul><li>生成的文本被解码为自然语言，并返回给用户。</li></ul></li></ul><h2 id="2-图像处理"><a href="#2-图像处理" class="headerlink" title="2. 图像处理"></a>2. 图像处理</h2><p>图像处理模块的代码主要分布在以下几个文件中：</p><ol><li>**<code>omnilmm.py</code>**：定义了图像编码器（Vision Tower）和图像特征重采样器（Resampler）。</li><li>**<code>utils.py</code>**：定义了图像预处理函数 <code>build_transform</code>，用于对图像进行预处理。</li><li>**<code>resampler.py</code>**：定义了图像特征重采样器（Resampler）的实现。</li></ol><h3 id="2-1-训练中对图像的处理"><a href="#2-1-训练中对图像的处理" class="headerlink" title="2.1 训练中对图像的处理"></a>2.1 <strong>训练中对图像的处理</strong></h3><h4 id="2-1-1-图像预处理"><a href="#2-1-1-图像预处理" class="headerlink" title="2.1.1 图像预处理"></a>2.1.1 <strong>图像预处理</strong></h4><ul><li><p><strong>步骤</strong>：</p><ol><li><strong>图像缩放</strong>：将图像调整到模型指定的输入尺寸（如 224x224）。</li><li><strong>图像裁剪</strong>：对图像进行中心裁剪。</li><li><strong>图像归一化</strong>：使用预定义的均值和标准差对图像进行归一化。</li><li><strong>数据增强</strong>（可选）：在训练过程中，可能会使用随机数据增强（如随机裁剪、旋转、颜色调整等）。</li></ol></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">t = transforms.Compose([<br>    transforms.Resize((input_size, input_size), interpolation=transforms.InterpolationMode.BICUBIC),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=mean, std=std)<br>])<br></code></pre></td></tr></table></figure></li></ul><h4 id="2-1-2-图像编码"><a href="#2-1-2-图像编码" class="headerlink" title="2.1.2 图像编码"></a>2.1.2 <strong>图像编码</strong></h4><ul><li><p><strong>步骤</strong>：</p><ol><li>使用预训练的视觉模型（如 EVA02）提取图像特征。</li><li>提取的图像特征被传递给图像特征重采样器。</li></ol></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">vision_tower = timm.create_model(<span class="hljs-string">&#x27;eva02_enormous_patch14_clip_224.laion2b_plus&#x27;</span>,<br>                                 pretrained=<span class="hljs-literal">False</span>,<br>                                 num_classes=<span class="hljs-number">0</span>,<br>                                 dynamic_img_size=<span class="hljs-literal">True</span>,<br>                                 dynamic_img_pad=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure></li></ul><h4 id="2-1-3-图像特征重采样"><a href="#2-1-3-图像特征重采样" class="headerlink" title="2.1.3 图像特征重采样"></a>2.1.3 <strong>图像特征重采样</strong></h4><ul><li><p><strong>步骤</strong>：</p><ol><li>使用多头注意力机制对图像特征进行重采样。</li><li>重采样后的图像特征与文本特征进行融合。</li></ol></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">resampler = Resampler(<br>    grid_size=<span class="hljs-built_in">int</span>(math.sqrt(config.num_query)),<br>    embed_dim=embed_dim,<br>    num_heads=embed_dim // <span class="hljs-number">128</span>,<br>    kv_dim=vision_tower.embed_dim,<br>)<br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="2-2-使用的图像编码器"><a href="#2-2-使用的图像编码器" class="headerlink" title="2.2 使用的图像编码器"></a>2.2 <strong>使用的图像编码器</strong></h3><h4 id="2-2-1-EVA02-模型"><a href="#2-2-1-EVA02-模型" class="headerlink" title="2.2.1 EVA02 模型"></a>2.2.1 <strong>EVA02 模型</strong></h4><ul><li><strong>模型名称</strong>：<code>eva02_enormous_patch14_clip_224.laion2b_plus</code></li><li><strong>特点</strong>：<ul><li>基于 Vision Transformer（ViT）架构。</li><li>使用 CLIP 预训练权重，适合多模态任务。</li><li>支持动态图像尺寸和填充。</li></ul></li></ul><h4 id="2-2-2-图像编码器的作用"><a href="#2-2-2-图像编码器的作用" class="headerlink" title="2.2.2 图像编码器的作用"></a>2.2.2 <strong>图像编码器的作用</strong></h4><ul><li>将输入的图像转换为高维特征表示。</li><li>提取的图像特征用于与文本特征进行融合。</li></ul><hr><h3 id="2-3-图像特征重采样详解-Resampler类"><a href="#2-3-图像特征重采样详解-Resampler类" class="headerlink" title="2.3 图像特征重采样详解(Resampler类)"></a>2.3 图像特征重采样详解(Resampler类)</h3><p>在 MiniCPM-V 项目中，图像特征重采样器（Resampler）的作用是将图像编码器提取的图像特征进行重采样，使其与文本特征的维度匹配，从而便于后续的多模态特征融合。<code>resampler.py</code> 文件中定义了 <code>Resampler</code> 类的实现。以下是对图像特征重采样的详细介绍：</p><hr><h4 id="2-3-1-Resampler-的作用"><a href="#2-3-1-Resampler-的作用" class="headerlink" title="2.3.1 Resampler 的作用"></a>2.3.1 <strong>Resampler 的作用</strong></h4><p>图像特征重采样器的主要作用是对图像特征进行降维或调整，使其与文本特征的维度一致。具体来说：</p><ul><li>图像编码器（如 EVA02）提取的图像特征通常是高维的（例如，224x224 的图像可能生成 196 个 patch 特征，每个 patch 特征维度为 768）。</li><li>文本特征通常是一个序列（如 256 个 token，每个 token 维度为 768）。</li><li>为了将图像特征与文本特征融合，需要对图像特征进行重采样，使其维度与文本特征匹配。</li></ul><hr><h4 id="2-3-2-Resampler-的实现"><a href="#2-3-2-Resampler-的实现" class="headerlink" title="2.3.2 Resampler 的实现"></a>2.3.2 <strong>Resampler 的实现</strong></h4><p><code>Resampler</code> 类的实现基于多头注意力机制（Multihead Attention），以下是其核心部分的详细介绍：</p><h5 id="初始化-init"><a href="#初始化-init" class="headerlink" title="初始化 (__init__)"></a><strong>初始化 (<code>__init__</code>)</strong></h5><ul><li><p><strong>输入参数</strong>：</p><ul><li><code>grid_size</code>：重采样后的特征网格大小（如 14x14）。</li><li><code>embed_dim</code>：重采样后的特征维度（如 768）。</li><li><code>num_heads</code>：注意力头的数量（如 6）。</li><li><code>kv_dim</code>：键值对的维度（通常与 <code>embed_dim</code> 相同）。</li><li><code>norm_layer</code>：归一化层（如 LayerNorm）。</li></ul></li><li><p><strong>初始化步骤</strong>：</p><ol><li>定义可学习的查询向量（<code>query</code>），用于生成重采样后的特征。</li><li>定义位置编码（<code>pos_embed</code>），用于为图像特征添加位置信息。</li><li>定义多头注意力机制（<code>attn</code>），用于对图像特征进行重采样。</li><li>定义投影矩阵（<code>proj</code>），用于将重采样后的特征映射到目标维度。</li></ol></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Resampler</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, grid_size, embed_dim, num_heads, kv_dim=<span class="hljs-literal">None</span>, norm_layer=partial(<span class="hljs-params">nn.LayerNorm, eps=<span class="hljs-number">1e-6</span></span>)</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.num_queries = grid_size ** <span class="hljs-number">2</span><br>        <span class="hljs-variable language_">self</span>.embed_dim = embed_dim<br>        <span class="hljs-variable language_">self</span>.num_heads = num_heads<br><br>        <span class="hljs-variable language_">self</span>.pos_embed = nn.Parameter(<br>            torch.from_numpy(get_2d_sincos_pos_embed(embed_dim, grid_size)).<span class="hljs-built_in">float</span>()<br>        ).requires_grad_(<span class="hljs-literal">False</span>)<br><br>        <span class="hljs-variable language_">self</span>.query = nn.Parameter(torch.zeros(<span class="hljs-variable language_">self</span>.num_queries, embed_dim))<br>        trunc_normal_(<span class="hljs-variable language_">self</span>.query, std=<span class="hljs-number">.02</span>)<br><br>        <span class="hljs-keyword">if</span> kv_dim <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> kv_dim != embed_dim:<br>            <span class="hljs-variable language_">self</span>.kv_proj = nn.Linear(kv_dim, embed_dim, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.kv_proj = nn.Identity()<br><br>        <span class="hljs-variable language_">self</span>.attn = nn.MultiheadAttention(embed_dim, num_heads)<br>        <span class="hljs-variable language_">self</span>.ln_q = norm_layer(embed_dim)<br>        <span class="hljs-variable language_">self</span>.ln_kv = norm_layer(embed_dim)<br><br>        <span class="hljs-variable language_">self</span>.ln_post = norm_layer(embed_dim)<br>        <span class="hljs-variable language_">self</span>.proj = nn.Parameter(<br>            (embed_dim ** -<span class="hljs-number">0.5</span>) * torch.randn(embed_dim, embed_dim))<br><br>        <span class="hljs-variable language_">self</span>.apply(<span class="hljs-variable language_">self</span>._init_weights)<br></code></pre></td></tr></table></figure></li></ul><h5 id="前向传播-forward"><a href="#前向传播-forward" class="headerlink" title="前向传播 (forward)"></a><strong>前向传播 (<code>forward</code>)</strong></h5><ul><li><p><strong>输入</strong>：</p><ul><li><code>x</code>：图像特征，形状为 <code>(batch_size, num_patches, feature_dim)</code>。</li><li><code>attn_mask</code>：注意力掩码（可选）。</li></ul></li><li><p><strong>处理步骤</strong>：</p><ol><li><strong>位置编码</strong>：为图像特征添加位置信息。</li><li><strong>特征投影</strong>：将图像特征投影到目标维度。</li><li><strong>多头注意力</strong>：使用可学习的查询向量对图像特征进行重采样。</li><li><strong>特征映射</strong>：将重采样后的特征映射到目标维度。</li></ol></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, attn_mask=<span class="hljs-literal">None</span></span>):<br>    pos_embed = get_abs_pos(<span class="hljs-variable language_">self</span>.pos_embed, x.size(<span class="hljs-number">1</span>))<br><br>    x = <span class="hljs-variable language_">self</span>.kv_proj(x)<br>    x = <span class="hljs-variable language_">self</span>.ln_kv(x).permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br><br>    N = x.shape[<span class="hljs-number">1</span>]<br>    q = <span class="hljs-variable language_">self</span>.ln_q(<span class="hljs-variable language_">self</span>.query)<br>    out = <span class="hljs-variable language_">self</span>.attn(<br>        <span class="hljs-variable language_">self</span>._repeat(q, N) + <span class="hljs-variable language_">self</span>.pos_embed.unsqueeze(<span class="hljs-number">1</span>),<br>        x + pos_embed.unsqueeze(<span class="hljs-number">1</span>),<br>        x,<br>        attn_mask=attn_mask)[<span class="hljs-number">0</span>]<br>    x = out.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br><br>    x = <span class="hljs-variable language_">self</span>.ln_post(x)<br>    x = x @ <span class="hljs-variable language_">self</span>.proj<br>    <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure></li></ul><hr><h4 id="2-3-3-重采样的详细过程"><a href="#2-3-3-重采样的详细过程" class="headerlink" title="2.3.3 重采样的详细过程"></a>2.3.3 <strong>重采样的详细过程</strong></h4><pre><code class=" mermaid">graph TD    A[输入: 图像特征] --&gt; B[位置编码]    A --&gt; C[特征投影]    B --&gt; D[添加位置信息]    C --&gt; E[归一化]    E --&gt; F[多头注意力机制]    D --&gt; F    F --&gt; G[重采样特征]    G --&gt; H[归一化]    H --&gt; I[特征映射]    I --&gt; J[输出: 重采样后的图像特征]    subgraph 位置编码模块        B --&gt; D    end    subgraph 特征投影模块        C --&gt; E    end    subgraph 多头注意力模块        D --&gt; F        E --&gt; F        F --&gt; G    end    subgraph 特征映射模块        G --&gt; H        H --&gt; I    end    subgraph 输出模块        I --&gt; J    end    %% 详细步骤    B --&gt; B1[生成 2D 正弦余弦位置编码]    B1 --&gt; D    C --&gt; C1[线性投影]    C1 --&gt; E    F --&gt; F1[计算注意力权重]    F1 --&gt; F2[加权求和]    F2 --&gt; G    I --&gt; I1[线性变换]    I1 --&gt; J</code></pre><h5 id="流程图说明-1"><a href="#流程图说明-1" class="headerlink" title="流程图说明"></a>流程图说明</h5><h6 id="1-输入-图像特征"><a href="#1-输入-图像特征" class="headerlink" title="1. 输入: 图像特征"></a>1. <strong>输入: 图像特征</strong></h6><ul><li>输入是图像编码器（如 EVA02）提取的图像特征，形状为 <code>(batch_size, num_patches, feature_dim)</code>。</li></ul><h6 id="2-位置编码模块"><a href="#2-位置编码模块" class="headerlink" title="2. 位置编码模块"></a>2. <strong>位置编码模块</strong></h6><ul><li><strong>生成 2D 正弦余弦位置编码</strong>：<ul><li>使用 <code>get_2d_sincos_pos_embed</code> 函数生成 2D 正弦余弦位置编码。</li></ul></li><li><strong>添加位置信息</strong>：<ul><li>将位置编码添加到图像特征中，使模型能够区分不同位置的特征。</li></ul></li></ul><h6 id="3-特征投影模块"><a href="#3-特征投影模块" class="headerlink" title="3. 特征投影模块"></a>3. <strong>特征投影模块</strong></h6><ul><li><strong>线性投影</strong>：<ul><li>使用线性层将图像特征投影到目标维度。</li></ul></li><li><strong>归一化</strong>：<ul><li>对投影后的特征进行归一化（LayerNorm）。</li></ul></li></ul><h6 id="4-多头注意力模块"><a href="#4-多头注意力模块" class="headerlink" title="4. 多头注意力模块"></a>4. <strong>多头注意力模块</strong></h6><ul><li><strong>计算注意力权重</strong>：<ul><li>使用可学习的查询向量（<code>query</code>）对图像特征进行注意力计算。</li></ul></li><li><strong>加权求和</strong>：<ul><li>根据注意力权重对图像特征进行加权求和，生成重采样后的特征。</li></ul></li></ul><h6 id="5-特征映射模块"><a href="#5-特征映射模块" class="headerlink" title="5. 特征映射模块"></a>5. <strong>特征映射模块</strong></h6><ul><li><strong>归一化</strong>：<ul><li>对重采样后的特征进行归一化（LayerNorm）。</li></ul></li><li><strong>线性变换</strong>：<ul><li>使用投影矩阵对特征进行线性变换，映射到目标维度。</li></ul></li></ul><h6 id="6-输出-重采样后的图像特征"><a href="#6-输出-重采样后的图像特征" class="headerlink" title="6. 输出: 重采样后的图像特征"></a>6. <strong>输出: 重采样后的图像特征</strong></h6><ul><li>输出是重采样后的图像特征，形状为 <code>(batch_size, num_queries, embed_dim)</code>。</li></ul><hr><h5 id="关键组件的作用总结"><a href="#关键组件的作用总结" class="headerlink" title="关键组件的作用总结"></a>关键组件的作用总结</h5><table><thead><tr><th align="left">组件</th><th align="left">作用</th></tr></thead><tbody><tr><td align="left"><strong>位置编码模块</strong></td><td align="left">为图像特征添加位置信息，使模型能够区分不同位置的特征。</td></tr><tr><td align="left"><strong>特征投影模块</strong></td><td align="left">将图像特征投影到目标维度，并进行归一化。</td></tr><tr><td align="left"><strong>多头注意力模块</strong></td><td align="left">使用可学习的查询向量对图像特征进行重采样。</td></tr><tr><td align="left"><strong>特征映射模块</strong></td><td align="left">对重采样后的特征进行归一化和线性变换，映射到目标维度。</td></tr><tr><td align="left"><strong>输出模块</strong></td><td align="left">输出重采样后的图像特征，用于与文本特征融合。</td></tr></tbody></table><hr><h5 id="详细步骤"><a href="#详细步骤" class="headerlink" title="详细步骤"></a>详细步骤</h5><ol><li><strong>输入: 图像特征</strong><ul><li>输入是图像编码器提取的图像特征，形状为 <code>(batch_size, num_patches, feature_dim)</code>。</li></ul></li><li><strong>位置编码模块</strong><ul><li>生成 2D 正弦余弦位置编码。</li><li>将位置编码添加到图像特征中。</li></ul></li><li><strong>特征投影模块</strong><ul><li>使用线性层将图像特征投影到目标维度。</li><li>对投影后的特征进行归一化。</li></ul></li><li><strong>多头注意力模块</strong><ul><li>使用可学习的查询向量对图像特征进行注意力计算。</li><li>根据注意力权重对图像特征进行加权求和，生成重采样后的特征。</li></ul></li><li><strong>特征映射模块</strong><ul><li>对重采样后的特征进行归一化。</li><li>使用投影矩阵对特征进行线性变换，映射到目标维度。</li></ul></li><li><strong>输出: 重采样后的图像特征</strong><ul><li>输出是重采样后的图像特征，形状为 <code>(batch_size, num_queries, embed_dim)</code></li></ul></li></ol><h4 id="2-3-4-重采样的输出"><a href="#2-3-4-重采样的输出" class="headerlink" title="2.3.4 重采样的输出"></a>2.3.4 <strong>重采样的输出</strong></h4><ul><li><p><strong>输出形状</strong>：<code>(batch_size, num_queries, embed_dim)</code>。</p><ul><li><code>num_queries</code>：重采样后的特征数量（如 196）。</li><li><code>embed_dim</code>：重采样后的特征维度（如 768）。</li></ul></li><li><p><strong>输出用途</strong>：</p><ul><li>重采样后的图像特征与文本特征拼接，输入到多模态融合模块（<code>OmniLMMModel</code>）中进行进一步处理。</li></ul></li></ul><hr><h4 id="2-3-5-特征重采样总结"><a href="#2-3-5-特征重采样总结" class="headerlink" title="2.3.5 特征重采样总结"></a>2.3.5 <strong>特征重采样总结</strong></h4><ul><li><strong>图像特征重采样器</strong>：将图像编码器提取的高维图像特征重采样为与文本特征匹配的维度。</li><li><strong>核心组件</strong>：<ul><li>位置编码：为图像特征添加位置信息。</li><li>多头注意力机制：对图像特征进行重采样。</li><li>特征映射：将重采样后的特征映射到目标维度。</li></ul></li><li><strong>输出</strong>：重采样后的图像特征用于与文本特征融合，生成多模态联合特征。</li></ul><p>通过图像特征重采样器，MiniCPM-V 能够有效地将图像特征与文本特征融合，从而实现高质量的多模态任务处理。</p><h3 id="2-4-图像处理总结"><a href="#2-4-图像处理总结" class="headerlink" title="2.4 图像处理总结"></a>2.4 <strong>图像处理总结</strong></h3><ul><li><strong>图像处理模块</strong>：负责对图像进行预处理、编码和重采样。</li><li><strong>图像预处理</strong>：包括缩放、裁剪、归一化和数据增强。</li><li><strong>图像编码器</strong>：使用 EVA02 模型提取图像特征。</li><li><strong>图像特征重采样</strong>：使用多头注意力机制对图像特征进行重采样，使其与文本特征匹配。</li></ul><h2 id="3-输入文本处理"><a href="#3-输入文本处理" class="headerlink" title="3. 输入文本处理"></a>3. 输入文本处理</h2><h3 id="3-1-文本处理流程图"><a href="#3-1-文本处理流程图" class="headerlink" title="3.1 文本处理流程图"></a>3.1 <strong>文本处理流程图</strong></h3><pre><code class=" mermaid">graph TD    A[输入: 文本] --&gt; B[文本预处理]    B --&gt; C[文本分词]    C --&gt; D[文本编码]    D --&gt; E[生成 input_ids]    E --&gt; F[文本特征编码]    F --&gt; G[输出: 文本特征]    subgraph 文本预处理模块        B --&gt; C        C --&gt; D        D --&gt; E    end    subgraph 文本编码模块        E --&gt; F        F --&gt; G    end    %% 详细步骤    B --&gt; B1[插入特殊标记]    B1 --&gt; C    C --&gt; C1[使用 Tokenizer 分词]    C1 --&gt; D    D --&gt; D1[生成 token ID 序列]    D1 --&gt; E    E --&gt; E1[生成 input_ids]    E1 --&gt; F    F --&gt; F1[使用 Transformer 编码]    F1 --&gt; G</code></pre><hr><h3 id="3-2-文本处理的详细梳理"><a href="#3-2-文本处理的详细梳理" class="headerlink" title="3.2 文本处理的详细梳理"></a>3.2 <strong>文本处理的详细梳理</strong></h3><h4 id="3-2-1-输入-文本"><a href="#3-2-1-输入-文本" class="headerlink" title="3.2.1 输入: 文本"></a>3.2.1 <strong>输入: 文本</strong></h4><ul><li>输入是一个对话格式的列表，包含用户和助手的对话轮次。例如：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">[<br>    &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;What is in the image?&quot;</span>&#125;,<br>    &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;The image shows a cat.&quot;</span>&#125;<br>]<br></code></pre></td></tr></table></figure></li></ul><h4 id="3-2-2-文本预处理"><a href="#3-2-2-文本预处理" class="headerlink" title="3.2.2 文本预处理"></a>3.2.2 <strong>文本预处理</strong></h4><ul><li><p><strong>插入特殊标记</strong>：</p><ul><li>如果文本中包含 <code>&lt;image&gt;</code> 标记，系统会将其替换为图像的特征表示（如 <code>&lt;im_start&gt;&lt;im_patch&gt;*N&lt;im_end&gt;</code>，其中 <code>N</code> 是图像特征的长度）。</li><li>插入特殊标记（如 <code>&lt;im_start&gt;</code> 和 <code>&lt;im_end&gt;</code>）来表示图像的位置。</li></ul></li><li><p><strong>生成对话模板</strong>：</p><ul><li>使用 <code>tokenizer.apply_chat_template</code> 函数将对话格式的文本转换为模型可以处理的格式。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">omni_preprocess</span>(<span class="hljs-params">sources, tokenizer: transformers.PreTrainedTokenizer, generation=<span class="hljs-literal">False</span></span>):<br>    system_content = <span class="hljs-string">&#x27;You are an artificial intelligence assistant, which gives helpful, detailed, and polite answers to the human\&#x27;s questions.&#x27;</span><br>    ignore_index = -<span class="hljs-number">100</span><br><br>    response_template = <span class="hljs-string">&#x27;\n&lt;|assistant|&gt;\n&#x27;</span><br>    instruction_template = <span class="hljs-string">&#x27;\n&lt;|user|&gt;\n&#x27;</span><br>    response_token_ids = tokenizer.encode(response_template, add_special_tokens=<span class="hljs-literal">False</span>)<br>    instruction_token_ids = tokenizer.encode(instruction_template, add_special_tokens=<span class="hljs-literal">False</span>)<br><br>    batch_input_ids = []<br>    batch_labels = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sources)):<br>        new_source = []<br>        prev_role = <span class="hljs-string">&#x27;unexpect&#x27;</span><br>        <span class="hljs-keyword">for</span> conv_turn <span class="hljs-keyword">in</span> sources[i]:<br>            role = conv_turn[<span class="hljs-string">&#x27;from&#x27;</span>] <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;from&#x27;</span> <span class="hljs-keyword">in</span> conv_turn <span class="hljs-keyword">else</span> conv_turn[<span class="hljs-string">&#x27;role&#x27;</span>]<br>            content = conv_turn[<span class="hljs-string">&#x27;value&#x27;</span>] <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;value&#x27;</span> <span class="hljs-keyword">in</span> conv_turn <span class="hljs-keyword">else</span> conv_turn[<span class="hljs-string">&#x27;content&#x27;</span>]<br><br>            role = <span class="hljs-string">&#x27;user&#x27;</span> <span class="hljs-keyword">if</span> role == <span class="hljs-string">&#x27;human&#x27;</span> <span class="hljs-keyword">else</span> role<br>            role = <span class="hljs-string">&#x27;assistant&#x27;</span> <span class="hljs-keyword">if</span> role == <span class="hljs-string">&#x27;gpt&#x27;</span> <span class="hljs-keyword">else</span> role<br><br>            <span class="hljs-keyword">assert</span> role <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;assistant&#x27;</span>]<br>            <span class="hljs-keyword">assert</span> role != prev_role, <span class="hljs-string">f&#x27;role=<span class="hljs-subst">&#123;role&#125;</span>, prev_role=<span class="hljs-subst">&#123;prev_role&#125;</span>&#x27;</span><br>            prev_role = role<br><br>            new_turn = &#123;<br>                <span class="hljs-string">&#x27;role&#x27;</span>: role,<br>                <span class="hljs-string">&#x27;content&#x27;</span>: content<br>            &#125;<br>            new_source.append(new_turn)<br>        <span class="hljs-keyword">if</span> new_source[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;role&#x27;</span>] != <span class="hljs-string">&#x27;system&#x27;</span>:<br>            new_source.insert(<span class="hljs-number">0</span>, &#123;<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;system&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: system_content&#125;)<br><br>        res_text = tokenizer.apply_chat_template(new_source, tokenize=<span class="hljs-literal">False</span>, add_generation_prompt=generation)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> generation:<br>            res_text = res_text.strip()<br><br>        conversations_tokenized = _tokenize_fn([res_text], tokenizer)<br>        res_input_ids = conversations_tokenized[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]<br>        res_labels = copy.deepcopy(conversations_tokenized[<span class="hljs-string">&quot;labels&quot;</span>][<span class="hljs-number">0</span>])<br><br>        response_token_ids_idxs = []<br>        human_token_ids_idxs = []<br><br>        <span class="hljs-keyword">for</span> assistant_idx <span class="hljs-keyword">in</span> np.where(res_labels == response_token_ids[<span class="hljs-number">0</span>])[<span class="hljs-number">0</span>]:<br>            <span class="hljs-keyword">if</span> (response_token_ids == res_labels[assistant_idx: assistant_idx + <span class="hljs-built_in">len</span>(response_token_ids)].tolist()):<br>                response_token_ids_idxs.append(assistant_idx + <span class="hljs-built_in">len</span>(response_token_ids))<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(response_token_ids_idxs) == <span class="hljs-number">0</span>:<br>            warnings.warn(<br>                <span class="hljs-string">f&quot;Could not find response key `<span class="hljs-subst">&#123;response_template&#125;</span>` in the following instance: @===&gt;<span class="hljs-subst">&#123;tokenizer.decode(res_input_ids)&#125;</span>&lt;===@ &quot;</span><br>                <span class="hljs-string">f&#x27;Raw text is @===&gt;<span class="hljs-subst">&#123;res_text&#125;</span>&lt;===@ Raw source is @===&gt;<span class="hljs-subst">&#123;new_source&#125;</span>&lt;===@ &#x27;</span><br>                <span class="hljs-string">f&quot;This instance will be ignored in loss calculation. &quot;</span><br>                <span class="hljs-string">f&quot;Note, if this happens often, consider increasing the `max_seq_length`.&quot;</span><br>            )<br>            res_labels[:] = ignore_index<br><br>        human_token_ids = instruction_token_ids<br>        <span class="hljs-keyword">for</span> human_idx <span class="hljs-keyword">in</span> np.where(res_labels == human_token_ids[<span class="hljs-number">0</span>])[<span class="hljs-number">0</span>]:<br>            <span class="hljs-keyword">if</span> human_token_ids == res_labels[human_idx: human_idx + <span class="hljs-built_in">len</span>(human_token_ids)].tolist():<br>                human_token_ids_idxs.append(human_idx)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(human_token_ids_idxs) == <span class="hljs-number">0</span>:<br>            warnings.warn(<br>                <span class="hljs-string">f&quot;Could not find instruction key `<span class="hljs-subst">&#123;instruction_template&#125;</span>` in the following instance: @===&gt;<span class="hljs-subst">&#123;tokenizer.decode(res_input_ids)&#125;</span>&lt;===@ &quot;</span><br>                <span class="hljs-string">f&#x27;Raw text is @===&gt;<span class="hljs-subst">&#123;res_text&#125;</span>&lt;===@ Raw source is @===&gt;<span class="hljs-subst">&#123;new_source&#125;</span>&lt;===@ &#x27;</span><br>                <span class="hljs-string">f&quot;This instance will be ignored in loss calculation. &quot;</span><br>                <span class="hljs-string">f&quot;Note, if this happens often, consider increasing the `max_seq_length`.&quot;</span><br>            )<br>            res_labels[:] = ignore_index<br><br>        <span class="hljs-keyword">for</span> idx, (start, end) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(human_token_ids_idxs, response_token_ids_idxs)):<br>            <span class="hljs-keyword">if</span> idx != <span class="hljs-number">0</span>:<br>                res_labels[start:end] = ignore_index<br>            <span class="hljs-keyword">else</span>:<br>                res_labels[:end] = ignore_index<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(response_token_ids_idxs) &lt; <span class="hljs-built_in">len</span>(human_token_ids_idxs):<br>            res_labels[human_token_ids_idxs[-<span class="hljs-number">1</span>]:] = ignore_index<br><br>        batch_input_ids.append(res_input_ids)<br>        batch_labels.append(res_labels)<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">dict</span>(input_ids=batch_input_ids, labels=batch_labels)<br></code></pre></td></tr></table></figure></li></ul><h4 id="3-2-3-文本分词"><a href="#3-2-3-文本分词" class="headerlink" title="3.2.3 文本分词"></a>3.2.3 <strong>文本分词</strong></h4><ul><li><p><strong>使用 Tokenizer 分词</strong>：</p><ul><li>使用 <code>tokenizer</code> 对文本进行分词，生成 token 序列。</li><li>分词过程中会插入特殊标记（如 <code>&lt;im_start&gt;</code> 和 <code>&lt;im_end&gt;</code>）。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">conversations_tokenized = _tokenize_fn([res_text], tokenizer)<br>res_input_ids = conversations_tokenized[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure></li></ul><h4 id="3-2-4-文本编码"><a href="#3-2-4-文本编码" class="headerlink" title="3.2.4 文本编码"></a>3.2.4 <strong>文本编码</strong></h4><ul><li><p><strong>生成 token ID 序列</strong>：</p><ul><li>将分词后的 token 转换为 token ID 序列（<code>input_ids</code>）。</li><li>生成 <code>labels</code>，用于模型的损失计算。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">res_input_ids = conversations_tokenized[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]<br>res_labels = copy.deepcopy(conversations_tokenized[<span class="hljs-string">&quot;labels&quot;</span>][<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure></li></ul><h4 id="3-2-5-文本特征编码"><a href="#3-2-5-文本特征编码" class="headerlink" title="3.2.5 文本特征编码"></a>3.2.5 <strong>文本特征编码</strong></h4><ul><li><p><strong>使用 Transformer 编码</strong>：</p><ul><li>将 <code>input_ids</code> 输入到 Transformer 模型（如 Mistral）中，生成文本特征。</li><li>文本特征是一个高维向量，形状为 <code>(batch_size, sequence_length, hidden_dim)</code>。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs_embeds = <span class="hljs-variable language_">self</span>.embed_tokens(data[<span class="hljs-string">&#x27;input_ids&#x27;</span>])<br></code></pre></td></tr></table></figure></li></ul><hr><h3 id="3-3-输出-文本特征"><a href="#3-3-输出-文本特征" class="headerlink" title="3.3 输出: 文本特征"></a>3.3 <strong>输出: 文本特征</strong></h3><ul><li><p><strong>输出形状</strong>：<code>(batch_size, sequence_length, hidden_dim)</code>。</p><ul><li><code>sequence_length</code>：文本序列的长度。</li><li><code>hidden_dim</code>：文本特征的维度（如 768）。</li></ul></li><li><p><strong>输出用途</strong>：</p><ul><li>文本特征与图像特征拼接，输入到多模态融合模块（<code>OmniLMMModel</code>）中进行进一步处理。</li></ul></li></ul><hr><h3 id="3-4-总结"><a href="#3-4-总结" class="headerlink" title="3.4 总结"></a>3.4 <strong>总结</strong></h3><ul><li><strong>文本处理流程</strong>：<ol><li><strong>文本预处理</strong>：插入特殊标记，生成对话模板。</li><li><strong>文本分词</strong>：使用 <code>tokenizer</code> 对文本进行分词。</li><li><strong>文本编码</strong>：将分词后的 token 转换为 <code>input_ids</code>。</li><li><strong>文本特征编码</strong>：使用 Transformer 模型生成文本特征。</li></ol></li><li><strong>输出</strong>：文本特征用于与图像特征融合，生成多模态联合特征。</li></ul><h2 id="4-图像特征和文本特征的融合"><a href="#4-图像特征和文本特征的融合" class="headerlink" title="4. 图像特征和文本特征的融合"></a>4. 图像特征和文本特征的融合</h2><p>在 MiniCPM-V 模型中，图像特征和文本特征的融合是多模态任务的核心部分。融合过程发生在 <code>OmniLMMModel</code> 中，通过将图像特征和文本特征拼接在一起，并使用 Transformer 模型进行联合编码。以下是结合源码和流程图的详细说明：</p><hr><h3 id="4-1-图像特征和文本特征的融合流程图"><a href="#4-1-图像特征和文本特征的融合流程图" class="headerlink" title="4.1 图像特征和文本特征的融合流程图"></a>4.1 <strong>图像特征和文本特征的融合流程图</strong></h3><pre><code class=" mermaid">graph TD    A[输入: 图像特征] --&gt; B[图像特征重采样]    C[输入: 文本特征] --&gt; D[文本特征编码]    B --&gt; E[多模态特征拼接]    D --&gt; E    E --&gt; F[Transformer 编码]    F --&gt; G[输出: 联合特征]    subgraph 图像处理模块        A --&gt; B    end    subgraph 文本处理模块        C --&gt; D    end    subgraph 多模态融合模块        B --&gt; E        D --&gt; E        E --&gt; F        F --&gt; G    end    %% 详细步骤    B --&gt; B1[重采样图像特征]    B1 --&gt; E    D --&gt; D1[生成文本特征]    D1 --&gt; E    E --&gt; E1[拼接图像和文本特征]    E1 --&gt; F    F --&gt; F1[自注意力机制编码]    F1 --&gt; G</code></pre><hr><h3 id="4-2-融合过程的详细说明"><a href="#4-2-融合过程的详细说明" class="headerlink" title="4.2 融合过程的详细说明"></a>4.2 <strong>融合过程的详细说明</strong></h3><h4 id="4-2-1-输入-图像特征和文本特征"><a href="#4-2-1-输入-图像特征和文本特征" class="headerlink" title="4.2.1 输入: 图像特征和文本特征"></a>4.2.1 <strong>输入: 图像特征和文本特征</strong></h4><ul><li><strong>图像特征</strong>：<ul><li>来自图像特征重采样器（<code>Resampler</code>），形状为 <code>(batch_size, num_queries, embed_dim)</code>。</li><li>例如：<code>(batch_size, 196, 768)</code>。</li></ul></li><li><strong>文本特征</strong>：<ul><li>来自文本编码器（<code>Tokenizer</code> 和 Transformer），形状为 <code>(batch_size, sequence_length, embed_dim)</code>。</li><li>例如：<code>(batch_size, 256, 768)</code>。</li></ul></li></ul><h4 id="4-2-2-图像特征重采样"><a href="#4-2-2-图像特征重采样" class="headerlink" title="4.2.2 图像特征重采样"></a>4.2.2 <strong>图像特征重采样</strong></h4><ul><li><p><strong>作用</strong>：将图像特征重采样为与文本特征匹配的维度。</p></li><li><p><strong>实现</strong>：</p><ul><li>使用 <code>Resampler</code> 对图像特征进行重采样。</li><li>重采样后的图像特征形状为 <code>(batch_size, num_queries, embed_dim)</code>。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">vision_hidden_states = <span class="hljs-variable language_">self</span>.resampler(vision_embedding)<br></code></pre></td></tr></table></figure></li></ul><h4 id="4-2-3-文本特征编码"><a href="#4-2-3-文本特征编码" class="headerlink" title="4.2.3 文本特征编码"></a>4.2.3 <strong>文本特征编码</strong></h4><ul><li><p><strong>作用</strong>：将文本输入编码为文本特征。</p></li><li><p><strong>实现</strong>：</p><ul><li>使用 <code>Tokenizer</code> 对文本进行分词和编码，生成 <code>input_ids</code>。</li><li>使用 Transformer 模型对 <code>input_ids</code> 进行编码，生成文本特征。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs_embeds = <span class="hljs-variable language_">self</span>.embed_tokens(data[<span class="hljs-string">&#x27;input_ids&#x27;</span>])<br></code></pre></td></tr></table></figure></li></ul><h4 id="4-2-4-多模态特征拼接"><a href="#4-2-4-多模态特征拼接" class="headerlink" title="4.2.4 多模态特征拼接"></a>4.2.4 <strong>多模态特征拼接</strong></h4><ul><li><p><strong>作用</strong>：将图像特征和文本特征拼接在一起，形成联合特征。</p></li><li><p><strong>实现</strong>：</p><ul><li>如果文本中包含 <code>&lt;image&gt;</code> 标记，系统会将图像特征插入到对应的位置。</li><li>拼接后的联合特征形状为 <code>(batch_size, sequence_length + num_queries, embed_dim)</code>。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python">new_input_embeds = []<br>cur_image_idx = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> cur_input_ids, cur_input_embeds <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(input_ids, inputs_embeds):<br>    <span class="hljs-keyword">if</span> (cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_patch_token).<span class="hljs-built_in">sum</span>() == <span class="hljs-number">0</span>:<br>        <span class="hljs-comment"># multimodal LLM, but the current sample is not multimodal</span><br>        cur_input_embeds = cur_input_embeds + (<span class="hljs-number">0.</span> * dummy_image_features).<span class="hljs-built_in">sum</span>()<br>        new_input_embeds.append(cur_input_embeds)<br>        <span class="hljs-keyword">continue</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.vision_config.use_im_start_end:<br>        cur_image_features = vision_hidden_states[cur_image_idx]<br>        num_patches = cur_image_features.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">if</span> (cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_start_token).<span class="hljs-built_in">sum</span>() != (cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_end_token).<span class="hljs-built_in">sum</span>():<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;The number of image start tokens and image end tokens should be the same.&quot;</span>)<br>        image_start_tokens = torch.where(cur_input_ids == <span class="hljs-variable language_">self</span>.vision_config.im_start_token)[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">for</span> image_start_token_pos <span class="hljs-keyword">in</span> image_start_tokens:<br>            cur_image_features = vision_hidden_states[cur_image_idx].to(device=cur_input_embeds.device)<br>            num_patches = cur_image_features.shape[<span class="hljs-number">0</span>]<br>            <span class="hljs-keyword">if</span> cur_input_ids[image_start_token_pos + num_patches + <span class="hljs-number">1</span>] != <span class="hljs-variable language_">self</span>.vision_config.im_end_token:<br>                <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;The image end token should follow the image start token.&quot;</span>)<br>            <span class="hljs-keyword">if</span> orig_embeds_params <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos].detach(), cur_input_embeds[image_start_token_pos:image_start_token_pos+<span class="hljs-number">1</span>], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + <span class="hljs-number">1</span>:image_start_token_pos + num_patches + <span class="hljs-number">2</span>], cur_input_embeds[image_start_token_pos + num_patches + <span class="hljs-number">2</span>:].detach()), dim=<span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">else</span>:<br>                cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos+<span class="hljs-number">1</span>], cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + <span class="hljs-number">1</span>:]), dim=<span class="hljs-number">0</span>)<br>            cur_image_idx += <span class="hljs-number">1</span><br>        new_input_embeds.append(cur_new_input_embeds)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br>inputs_embeds = torch.stack(new_input_embeds, dim=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure></li></ul><h4 id="4-2-5-Transformer-编码"><a href="#4-2-5-Transformer-编码" class="headerlink" title="4.2.5 Transformer 编码"></a>4.2.5 <strong>Transformer 编码</strong></h4><ul><li><p><strong>作用</strong>：对拼接后的联合特征进行编码，生成最终的联合特征。</p></li><li><p><strong>实现</strong>：</p><ul><li>使用 Transformer 的自注意力机制（Self-Attention）对联合特征进行编码。</li><li>编码后的联合特征形状为 <code>(batch_size, sequence_length + num_queries, embed_dim)</code>。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">outputs = <span class="hljs-variable language_">self</span>.model(<br>    input_ids=input_ids, attention_mask=attention_mask, past_key_values=past_key_values,<br>    inputs_embeds=inputs_embeds, use_cache=use_cache,<br>    output_attentions=output_attentions, output_hidden_states=output_hidden_states,<br>    return_dict=return_dict,<br>    **kwargs<br>)<br></code></pre></td></tr></table></figure></li></ul><h4 id="4-2-6-输出-联合特征"><a href="#4-2-6-输出-联合特征" class="headerlink" title="4.2.6 输出: 联合特征"></a>4.2.6 <strong>输出: 联合特征</strong></h4><ul><li><p><strong>输出形状</strong>：<code>(batch_size, sequence_length + num_queries, embed_dim)</code>。</p><ul><li><code>sequence_length</code>：文本序列的长度。</li><li><code>num_queries</code>：图像特征的数量。</li><li><code>embed_dim</code>：特征的维度（如 768）。</li></ul></li><li><p><strong>输出用途</strong>：</p><ul><li>联合特征用于生成文本输出（如通过 <code>OmniLMMForCausalLM</code> 生成回答）。</li></ul></li></ul><hr><h3 id="4-3-总结"><a href="#4-3-总结" class="headerlink" title="4.3 总结"></a>4.3 <strong>总结</strong></h3><ul><li><strong>图像特征和文本特征的融合</strong>：<ol><li><strong>图像特征重采样</strong>：将图像特征重采样为与文本特征匹配的维度。</li><li><strong>文本特征编码</strong>：将文本输入编码为文本特征。</li><li><strong>多模态特征拼接</strong>：将图像特征和文本特征拼接在一起，形成联合特征。</li><li><strong>Transformer 编码</strong>：对联合特征进行编码，生成最终的联合特征。</li></ol></li><li><strong>输出</strong>：联合特征用于生成文本输出。</li></ul><h2 id="5-模型核心模块-1"><a href="#5-模型核心模块-1" class="headerlink" title="5. 模型核心模块"></a>5. 模型核心模块</h2><p>除了文本处理和图像处理模块外，模型核心模块（<code>OmniLMMModel</code> 和 <code>OmniLMMForCausalLM</code>）负责将多模态特征融合并生成最终的输出。</p><hr><h3 id="5-1-模型核心模块的流程图"><a href="#5-1-模型核心模块的流程图" class="headerlink" title="5.1 模型核心模块的流程图"></a>5.1 <strong>模型核心模块的流程图</strong></h3><pre><code class=" mermaid">graph TD    A[输入: 联合特征] --&gt; B[Transformer 编码]    B --&gt; C[生成文本输出]    C --&gt; D[计算损失]    D --&gt; E[反向传播]    E --&gt; F[更新模型参数]    F --&gt; G[输出: 训练后的模型]    subgraph 模型核心模块        A --&gt; B        B --&gt; C        C --&gt; D        D --&gt; E        E --&gt; F        F --&gt; G    end    %% 详细步骤    B --&gt; B1[自注意力机制编码]    B1 --&gt; C    C --&gt; C1[生成文本 token]    C1 --&gt; C2[自回归生成文本]    C2 --&gt; D    D --&gt; D1[计算交叉熵损失]    D1 --&gt; E    E --&gt; E1[计算梯度]    E1 --&gt; F    F --&gt; F1[优化器更新参数]    F1 --&gt; G</code></pre><hr><h3 id="5-2-模型核心模块的详细说明"><a href="#5-2-模型核心模块的详细说明" class="headerlink" title="5.2 模型核心模块的详细说明"></a>5.2 <strong>模型核心模块的详细说明</strong></h3><h4 id="5-2-1-输入-联合特征"><a href="#5-2-1-输入-联合特征" class="headerlink" title="5.2.1 输入: 联合特征"></a>5.2.1 <strong>输入: 联合特征</strong></h4><ul><li><strong>联合特征</strong>：<ul><li>来自多模态融合模块，形状为 <code>(batch_size, sequence_length + num_queries, embed_dim)</code>。</li><li>例如：<code>(batch_size, 256 + 196, 768)</code>。</li></ul></li></ul><h4 id="5-2-2-Transformer-编码"><a href="#5-2-2-Transformer-编码" class="headerlink" title="5.2.2 Transformer 编码"></a>5.2.2 <strong>Transformer 编码</strong></h4><ul><li><p><strong>作用</strong>：对联合特征进行编码，生成隐藏状态。</p></li><li><p><strong>实现</strong>：</p><ul><li>使用 Transformer 的自注意力机制（Self-Attention）对联合特征进行编码。</li><li>编码后的隐藏状态形状为 <code>(batch_size, sequence_length + num_queries, embed_dim)</code>。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">outputs = <span class="hljs-variable language_">self</span>.model(<br>    input_ids=input_ids, attention_mask=attention_mask, past_key_values=past_key_values,<br>    inputs_embeds=inputs_embeds, use_cache=use_cache,<br>    output_attentions=output_attentions, output_hidden_states=output_hidden_states,<br>    return_dict=return_dict,<br>    **kwargs<br>)<br></code></pre></td></tr></table></figure></li></ul><h4 id="5-2-3-生成文本输出"><a href="#5-2-3-生成文本输出" class="headerlink" title="5.2.3 生成文本输出"></a>5.2.3 <strong>生成文本输出</strong></h4><ul><li><p><strong>作用</strong>：根据隐藏状态生成文本输出。</p></li><li><p><strong>实现</strong>：</p><ul><li>使用 <code>lm_head</code> 对隐藏状态进行解码，生成文本 token。</li><li>通过自回归生成逐步生成文本输出。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">hidden_states = outputs[<span class="hljs-number">0</span>]<br>logits = <span class="hljs-variable language_">self</span>.lm_head(hidden_states)<br></code></pre></td></tr></table></figure></li></ul><h4 id="5-2-4-计算损失"><a href="#5-2-4-计算损失" class="headerlink" title="5.2.4 计算损失"></a>5.2.4 <strong>计算损失</strong></h4><ul><li><p><strong>作用</strong>：计算模型输出与真实标签之间的损失。</p></li><li><p><strong>实现</strong>：</p><ul><li>使用交叉熵损失函数（CrossEntropyLoss）计算损失。</li><li>损失函数的目标是最小化模型输出与真实标签之间的差异。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">loss = <span class="hljs-literal">None</span><br><span class="hljs-keyword">if</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    shift_logits = logits[..., :-<span class="hljs-number">1</span>, :].contiguous()<br>    shift_labels = labels[..., <span class="hljs-number">1</span>:].contiguous()<br>    loss_fct = CrossEntropyLoss()<br>    shift_logits = shift_logits.view(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.config.vocab_size)<br>    shift_labels = shift_labels.view(-<span class="hljs-number">1</span>)<br>    shift_labels = shift_labels.to(shift_logits.device)<br>    loss = loss_fct(shift_logits, shift_labels)<br></code></pre></td></tr></table></figure></li></ul><h4 id="5-2-5-反向传播"><a href="#5-2-5-反向传播" class="headerlink" title="5.2.5 反向传播"></a>5.2.5 <strong>反向传播</strong></h4><ul><li><p><strong>作用</strong>：计算梯度并更新模型参数。</p></li><li><p><strong>实现</strong>：</p><ul><li>使用反向传播算法计算损失函数对模型参数的梯度。</li><li>通过优化器（如 AdamW）更新模型参数。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">loss.backward()<br>optimizer.step()<br></code></pre></td></tr></table></figure></li></ul><h4 id="5-2-6-更新模型参数"><a href="#5-2-6-更新模型参数" class="headerlink" title="5.2.6 更新模型参数"></a>5.2.6 <strong>更新模型参数</strong></h4><ul><li><p><strong>作用</strong>：根据梯度更新模型参数。</p></li><li><p><strong>实现</strong>：</p><ul><li>使用优化器更新模型参数。</li><li>更新后的模型参数用于下一轮训练。</li></ul></li><li><p><strong>代码</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer.step()<br></code></pre></td></tr></table></figure></li></ul><h4 id="5-2-7-输出-训练后的模型"><a href="#5-2-7-输出-训练后的模型" class="headerlink" title="5.2.7 输出: 训练后的模型"></a>5.2.7 <strong>输出: 训练后的模型</strong></h4><ul><li><strong>输出</strong>：训练后的模型，可以用于推理任务。</li></ul><hr><h3 id="5-3-总结"><a href="#5-3-总结" class="headerlink" title="5.3 总结"></a>5.3 <strong>总结</strong></h3><ul><li><strong>模型核心模块的训练流程</strong>：<ol><li><strong>Transformer 编码</strong>：对联合特征进行编码，生成隐藏状态。</li><li><strong>生成文本输出</strong>：根据隐藏状态生成文本输出。</li><li><strong>计算损失</strong>：计算模型输出与真实标签之间的损失。</li><li><strong>反向传播</strong>：计算梯度并更新模型参数。</li><li><strong>更新模型参数</strong>：根据梯度更新模型参数。</li><li><strong>输出</strong>：训练后的模型。</li></ol></li></ul><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>multi-modal</category>
      
    </categories>
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>llm</tag>
      
      <tag>源码解析</tag>
      
      <tag>MiniCPM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MiniCPM-V多模态模型源码解析-01：整体代码结构及模块功能</title>
    <link href="/2025/01/13/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9AMiniCPM-V%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-01%EF%BC%9A%E6%95%B4%E4%BD%93%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84%E5%8F%8A%E6%A8%A1%E5%9D%97%E5%8A%9F%E8%83%BD/"/>
    <url>/2025/01/13/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9AMiniCPM-V%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-01%EF%BC%9A%E6%95%B4%E4%BD%93%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84%E5%8F%8A%E6%A8%A1%E5%9D%97%E5%8A%9F%E8%83%BD/</url>
    
    <content type="html"><![CDATA[<p>MiniCPM-V 是一个多模态大模型项目，旨在处理图像和文本的联合任务。</p><h2 id="1-项目整体架构"><a href="#1-项目整体架构" class="headerlink" title="1. 项目整体架构"></a>1. <strong>项目整体架构</strong></h2><p>MiniCPM-V 项目主要由以下几个部分组成：</p><ul><li><strong>模型核心模块</strong>：负责处理多模态输入（图像和文本），并生成相应的输出。</li><li><strong>数据处理模块</strong>：负责预处理图像和文本数据，将其转换为模型可以处理的格式。</li><li><strong>训练模块</strong>：负责模型的训练和优化。</li><li><strong>推理模块</strong>：负责模型的推理和生成。</li><li><strong>工具模块</strong>：提供一些辅助功能，如日志记录、图像处理等。</li></ul><h2 id="2-功能模块及其作用"><a href="#2-功能模块及其作用" class="headerlink" title="2. 功能模块及其作用"></a>2. <strong>功能模块及其作用</strong></h2><h3 id="2-1-模型核心模块"><a href="#2-1-模型核心模块" class="headerlink" title="2.1 模型核心模块"></a>2.1 <strong>模型核心模块</strong></h3><ul><li><p><strong><code>omnilmm.py</code></strong>:</p><ul><li>这是模型的核心实现文件，定义了 <code>OmniLMMForCausalLM</code> 和 <code>OmniLMMModel</code> 类。</li><li><code>OmniLMMModel</code> 继承自 <code>MistralModel</code>，负责处理多模态输入（图像和文本），并生成相应的隐藏状态。</li><li><code>OmniLMMForCausalLM</code> 继承自 <code>MistralForCausalLM</code>，负责生成文本输出。</li><li>该模块还包含了图像编码器（<code>vision_tower</code>）和重采样器（<code>Resampler</code>），用于处理图像输入。</li></ul></li><li><p><strong><code>resampler.py</code></strong>:</p><ul><li>定义了 <code>Resampler</code> 类，用于对图像特征进行重采样，以便与文本特征进行融合。</li><li>该模块使用了多头注意力机制（<code>MultiheadAttention</code>）来处理图像特征。</li></ul></li></ul><h3 id="2-2-数据处理模块"><a href="#2-2-数据处理模块" class="headerlink" title="2.2 数据处理模块"></a>2.2 <strong>数据处理模块</strong></h3><ul><li><p><strong><code>train_utils.py</code></strong>:</p><ul><li>包含了数据预处理函数 <code>omni_preprocess</code>，用于将对话数据转换为模型可以处理的格式。</li><li>该模块还定义了如何处理图像和文本的联合输入，并生成相应的 <code>input_ids</code> 和 <code>labels</code>。</li></ul></li><li><p><strong><code>utils.py</code></strong>:</p><ul><li>提供了一些图像处理的工具函数，如 <code>build_transform</code>，用于构建图像预处理管道。</li><li>还包含了一些辅助函数，如 <code>img2b64</code> 用于将图像转换为 base64 编码。</li></ul></li></ul><h3 id="2-3-训练模块"><a href="#2-3-训练模块" class="headerlink" title="2.3 训练模块"></a>2.3 <strong>训练模块</strong></h3><ul><li><strong><code>train_utils.py</code></strong>:<ul><li>包含了训练过程中使用的工具函数，如 <code>omni_preprocess</code>，用于处理训练数据。</li><li>该模块还定义了如何处理多模态输入，并生成相应的损失函数。</li></ul></li></ul><h3 id="2-4-推理模块"><a href="#2-4-推理模块" class="headerlink" title="2.4 推理模块"></a>2.4 <strong>推理模块</strong></h3><ul><li><strong><code>chat.py</code></strong>:<ul><li>这是项目的推理模块，负责加载模型并进行推理。</li><li>定义了 <code>OmniLMM12B</code> 类，用于处理图像和文本的联合输入，并生成相应的输出。</li><li>该模块还包含了 <code>MiniCPMV</code>、<code>MiniCPMV2_5</code> 和 <code>MiniCPMV2_6</code> 类，用于不同版本的模型推理。</li></ul></li></ul><h3 id="2-5-工具模块"><a href="#2-5-工具模块" class="headerlink" title="2.5 工具模块"></a>2.5 <strong>工具模块</strong></h3><ul><li><p><strong><code>constants.py</code></strong>:</p><ul><li>定义了一些常量，如心跳间隔时间、日志目录等。</li></ul></li><li><p><strong><code>conversation.py</code></strong>:</p><ul><li>定义了 <code>Conversation</code> 类，用于管理对话历史记录。</li><li>该模块还定义了一些预定义的对话模板，如 <code>conv_v1</code>、<code>conv_v1_2</code> 等。</li></ul></li><li><p><strong><code>utils.py</code></strong>:</p><ul><li>提供了一些通用的工具函数，如日志记录、图像处理、数据转换等。</li></ul></li></ul><h2 id="3-模块之间的交互"><a href="#3-模块之间的交互" class="headerlink" title="3. 模块之间的交互"></a>3. <strong>模块之间的交互</strong></h2><ul><li><strong>模型核心模块</strong>（<code>omnilmm.py</code> 和 <code>resampler.py</code>）负责处理多模态输入，并生成相应的输出。</li><li><strong>数据处理模块</strong>（<code>train_utils.py</code> 和 <code>utils.py</code>）负责将原始数据（图像和文本）转换为模型可以处理的格式。</li><li><strong>训练模块</strong>（<code>train_utils.py</code>）使用数据处理模块的输出进行模型训练。</li><li><strong>推理模块</strong>（<code>chat.py</code>）使用模型核心模块进行推理，并生成最终的输出。</li><li><strong>工具模块</strong>（<code>constants.py</code> 和 <code>conversation.py</code>）提供了一些辅助功能，如日志记录、对话管理等。</li></ul><h2 id="4-主要功能"><a href="#4-主要功能" class="headerlink" title="4. 主要功能"></a>4. <strong>主要功能</strong></h2><ul><li><strong>多模态输入处理</strong>：项目能够处理图像和文本的联合输入，并生成相应的输出。</li><li><strong>对话管理</strong>：项目能够管理对话历史记录，并根据对话历史生成相应的输出。</li><li><strong>模型训练</strong>：项目提供了训练多模态模型的工具和函数。</li><li><strong>模型推理</strong>：项目提供了推理接口，能够根据输入的图像和文本生成相应的输出。</li></ul><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. <strong>总结</strong></h3><p>MiniCPM-V 是一个多模态大模型项目，能够处理图像和文本的联合任务。项目的主要模块包括模型核心模块、数据处理模块、训练模块、推理模块和工具模块。每个模块都有其特定的功能，模块之间通过数据流进行交互，共同完成多模态任务的处理。</p><h2 id="5-整体结构"><a href="#5-整体结构" class="headerlink" title="5. 整体结构"></a>5. <strong>整体结构</strong></h2><pre><code class=" mermaid">graph TD    A[输入: 图像和文本] --&gt; B[图像预处理]    A --&gt; C[文本预处理]    B --&gt; D[图像编码器 Vision Tower]    C --&gt; E[文本编码器 Tokenizer]    D --&gt; F[图像特征重采样 Resampler]    E --&gt; G[文本特征编码]    F --&gt; H[多模态特征融合 OmniLMMModel]    G --&gt; H    H --&gt; I[生成文本输出 OmniLMMForCausalLM]    I --&gt; J[输出: 生成的文本]    subgraph 图像处理模块        B --&gt; D        D --&gt; F    end    subgraph 文本处理模块        C --&gt; E        E --&gt; G    end    subgraph 多模态融合模块        F --&gt; H        G --&gt; H    end    subgraph 模型核心模块        H --&gt; I    end    subgraph 输出模块        I --&gt; J    end</code></pre><h3 id="流程图说明"><a href="#流程图说明" class="headerlink" title="流程图说明"></a>流程图说明</h3><ol><li><p><strong>输入: 图像和文本</strong></p><ul><li>输入包括图像和文本数据，图像通过图像预处理模块处理，文本通过文本预处理模块处理。</li></ul></li><li><p><strong>图像预处理</strong></p><ul><li>图像预处理模块对输入的图像进行预处理，包括缩放、裁剪、归一化等操作。</li><li>处理后的图像传递给图像编码器（Vision Tower）。</li></ul></li><li><p><strong>图像编码器 (Vision Tower)</strong></p><ul><li>图像编码器将预处理后的图像转换为图像特征。</li><li>图像特征传递给图像特征重采样器（Resampler）。</li></ul></li><li><p><strong>图像特征重采样 (Resampler)</strong></p><ul><li>图像特征重采样器对图像特征进行重采样，以便与文本特征进行融合。</li><li>重采样后的图像特征传递给多模态特征融合模块（OmniLMMModel）。</li></ul></li><li><p><strong>文本预处理</strong></p><ul><li>文本预处理模块对输入的文本进行分词、编码等操作。</li><li>处理后的文本传递给文本编码器（Tokenizer）。</li></ul></li><li><p><strong>文本编码器 (Tokenizer)</strong></p><ul><li>文本编码器将预处理后的文本转换为文本特征。</li><li>文本特征传递给多模态特征融合模块（OmniLMMModel）。</li></ul></li><li><p><strong>多模态特征融合 (OmniLMMModel)</strong></p><ul><li>多模态特征融合模块将图像特征和文本特征进行融合，生成联合特征。</li><li>联合特征传递给生成文本输出模块（OmniLMMForCausalLM）。</li></ul></li><li><p><strong>生成文本输出 (OmniLMMForCausalLM)</strong></p><ul><li>生成文本输出模块根据联合特征生成文本输出。</li><li>生成的文本传递给输出模块。</li></ul></li><li><p><strong>输出: 生成的文本</strong></p><ul><li>输出模块将生成的文本输出给用户。</li></ul></li></ol><h3 id="类及相关组件的作用"><a href="#类及相关组件的作用" class="headerlink" title="类及相关组件的作用"></a>类及相关组件的作用</h3><ul><li><strong>图像预处理模块</strong>：负责对输入的图像进行预处理，包括缩放、裁剪、归一化等操作。</li><li>**图像编码器 (Vision Tower)**：将预处理后的图像转换为图像特征。</li><li>**图像特征重采样器 (Resampler)**：对图像特征进行重采样，以便与文本特征进行融合。</li><li><strong>文本预处理模块</strong>：负责对输入的文本进行分词、编码等操作。</li><li>**文本编码器 (Tokenizer)**：将预处理后的文本转换为文本特征。</li><li>**多模态特征融合模块 (OmniLMMModel)**：将图像特征和文本特征进行融合，生成联合特征。</li><li>**生成文本输出模块 (OmniLMMForCausalLM)**：根据联合特征生成文本输出。</li><li><strong>输出模块</strong>：将生成的文本输出给用户。</li></ul><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>multi-modal</category>
      
    </categories>
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>llm</tag>
      
      <tag>源码解析</tag>
      
      <tag>MiniCPM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HTTP基础03：简单的HTTP协议</title>
    <link href="/2025/01/02/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP%E5%9F%BA%E7%A1%8003%EF%BC%9A%E7%AE%80%E5%8D%95%E7%9A%84%20HTTP%E5%8D%8F%E8%AE%AE/"/>
    <url>/2025/01/02/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP%E5%9F%BA%E7%A1%8003%EF%BC%9A%E7%AE%80%E5%8D%95%E7%9A%84%20HTTP%E5%8D%8F%E8%AE%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="简单的-HTTP协议"><a href="#简单的-HTTP协议" class="headerlink" title="简单的 HTTP协议"></a>简单的 HTTP协议</h2><h2 id="1-HTTP协议用于客户端和服务器端之间的通信"><a href="#1-HTTP协议用于客户端和服务器端之间的通信" class="headerlink" title="1. HTTP协议用于客户端和服务器端之间的通信"></a>1. HTTP协议用于客户端和服务器端之间的通信</h2><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102104311290.png" alt="image-20250102104311290"></p><ul><li><strong>定义</strong>：HTTP协议用于客户端和服务器之间的通信。</li><li><strong>角色</strong>：请求访问资源的一端称为客户端，提供资源响应的一端称为服务器端。</li><li><strong>通信线路</strong>：在一条通信线路上，必定有一端是客户端，另一端是服务器端。</li><li><strong>角色互换</strong>：在某些情况下，两台计算机可能会互换客户端和服务器端的角色，但在一条通信路线上，角色是确定的。</li></ul><h2 id="2-通过请求和响应的交换达成通信"><a href="#2-通过请求和响应的交换达成通信" class="headerlink" title="2. 通过请求和响应的交换达成通信"></a>2. 通过请求和响应的交换达成通信</h2><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102104434929.png" alt="请求和响应"></p><p><strong>请求和响应</strong>：HTTP协议规定请求从客户端发出，服务器端响应该请求并返回。</p><p><strong>请求报文</strong>：由请求方法、请求URI、协议版本、可选的请求首部字段和内容实体构成。</p><ul><li><strong>示例</strong>：<code>GET /index.htm HTTP/1.1 Host: hackr.jp</code></li></ul><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102104508513.png" alt="请求报文"></p><p><strong>响应报文</strong>：由协议版本、状态码、原因短语、可选的响应首部字段以及实体主体构成。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102104617041.png" alt="image-20250102104617041"></p><ul><li><p><strong>示例</strong>：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">HTTP</span>/<span class="hljs-number">1</span>.<span class="hljs-number">1</span> <span class="hljs-number">200</span> OK<br><span class="hljs-attribute">Date</span>: Tue, <span class="hljs-number">10</span> Jul <span class="hljs-number">2012</span> <span class="hljs-number">06</span>:<span class="hljs-number">50</span>:<span class="hljs-number">15</span> GMT<br><span class="hljs-attribute">Content</span>-Length: <span class="hljs-number">362</span><br><span class="hljs-attribute">Content</span>-Type: text/html<br></code></pre></td></tr></table></figure></li></ul><h2 id="3-HTTP是不保存状态的协议"><a href="#3-HTTP是不保存状态的协议" class="headerlink" title="3. HTTP是不保存状态的协议"></a>3. HTTP是不保存状态的协议</h2><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102104640175.png" alt="无状态协议"></p><ul><li><strong>无状态协议</strong>：HTTP协议自身不对请求和响应之间的通信状态进行保存。</li><li><strong>优点</strong>：为了更快地处理大量事务，确保协议的可伸缩性。</li><li><strong>缺点</strong>：随着Web的发展，无状态导致业务处理变得棘手。</li><li><strong>解决方案</strong>：引入Cookie技术来管理状态。</li></ul><h2 id="4-请求URI定位资源"><a href="#4-请求URI定位资源" class="headerlink" title="4. 请求URI定位资源"></a>4. 请求URI定位资源</h2><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102104835594.png" alt="URI定位资源"></p><ul><li><strong>URI功能</strong>：HTTP协议使用URI定位互联网上的资源。</li><li><strong>请求URI</strong>：客户端请求访问资源时，URI需要包含在请求报文中。<ul><li><strong>完整URI</strong>：<code>GET http://hackr.jp/index.htm HTTP/1.1</code></li><li><strong>Host字段</strong>：<code>GET /index.htm HTTP/1.1 Host: hackr.jp</code></li><li><strong>通配符</strong>：<code>OPTIONS * HTTP/1.1</code></li></ul></li></ul><h2 id="5-告知服务器意图的-HTTP方法"><a href="#5-告知服务器意图的-HTTP方法" class="headerlink" title="5. 告知服务器意图的 HTTP方法"></a>5. 告知服务器意图的 HTTP方法</h2><h3 id="5-1-GET-获取资源"><a href="#5-1-GET-获取资源" class="headerlink" title="5.1 GET: 获取资源"></a>5.1 GET: 获取资源</h3><ul><li><strong>用途</strong>：用于请求访问已被URI识别的资源。</li><li><strong>特点</strong>：请求的资源会被服务器解析后返回响应内容。如果是文本资源，则原样返回；如果是程序（如CGI），则返回执行后的输出结果。</li><li><strong>示例请求</strong>：<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs http"><span class="hljs-keyword">GET</span> <span class="hljs-string">/index.html</span> <span class="hljs-meta">HTTP/1.1</span><br><span class="hljs-attribute">Host</span><span class="hljs-punctuation">: </span>www.example.com<br></code></pre></td></tr></table></figure></li><li><strong>示例响应</strong>：<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs xquery">HTTP/<span class="hljs-number">1.1</span> <span class="hljs-number">200</span> OK<br>Content-Type: <span class="hljs-type">text</span>/html<br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">html</span>&gt;</span>...<span class="hljs-tag">&lt;/<span class="hljs-name">html</span>&gt;</span></span><br></code></pre></td></tr></table></figure></li></ul><h3 id="5-2-POST-传输实体主体"><a href="#5-2-POST-传输实体主体" class="headerlink" title="5.2 POST: 传输实体主体"></a>5.2 POST: 传输实体主体</h3><ul><li><strong>用途</strong>：用于传输实体的主体，通常用于提交表单数据或其他数据。</li><li><strong>特点</strong>：虽然GET方法也可以传输数据，但POST方法更适合传输大量数据或敏感信息，因为它不会将数据暴露在URL中。</li><li><strong>示例请求</strong>：<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">POST</span> /submit.cgi HTTP/<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">Host</span>: www.example.com<br><span class="hljs-attribute">Content</span>-Length: <span class="hljs-number">1560</span><br><span class="hljs-section">&lt;form data&gt;</span><br></code></pre></td></tr></table></figure></li><li><strong>示例响应</strong>：<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">HTTP</span>/<span class="hljs-number">1</span>.<span class="hljs-number">1</span> <span class="hljs-number">200</span> OK<br><span class="hljs-attribute">Content</span>-Type: text/html<br><span class="hljs-section">&lt;result of form submission&gt;</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="5-3-PUT-传输文件"><a href="#5-3-PUT-传输文件" class="headerlink" title="5.3 PUT: 传输文件"></a>5.3 PUT: 传输文件</h3><ul><li><strong>用途</strong>：用于传输文件，类似于FTP协议的文件上传。</li><li><strong>特点</strong>：请求报文的主体中包含文件内容，服务器将其保存到请求URI指定的位置。由于PUT方法不带验证机制，存在安全性问题，一般Web网站不使用该方法。</li><li><strong>示例请求</strong>：<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">PUT /example<span class="hljs-selector-class">.html</span> HTTP/<span class="hljs-number">1.1</span><br>Host: www<span class="hljs-selector-class">.example</span><span class="hljs-selector-class">.com</span><br>Content-Type: text/<span class="hljs-selector-tag">html</span><br>Content-Length: <span class="hljs-number">1560</span><br>&lt;file <span class="hljs-attribute">content</span>&gt;<br></code></pre></td></tr></table></figure></li><li><strong>示例响应</strong>：<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">HTTP</span>/<span class="hljs-number">1</span>.<span class="hljs-number">1</span> <span class="hljs-number">204</span> No Content<br></code></pre></td></tr></table></figure></li></ul><h3 id="5-4-HEAD-获得报文首部"><a href="#5-4-HEAD-获得报文首部" class="headerlink" title="5.4 HEAD: 获得报文首部"></a>5.4 HEAD: 获得报文首部</h3><ul><li><strong>用途</strong>：类似于GET方法，但不返回报文主体部分，仅用于确认URI的有效性及资源更新的日期时间等。</li><li><strong>特点</strong>：常用于检查资源是否存在或获取资源的元数据。</li><li><strong>示例请求</strong>：<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs http"><span class="hljs-keyword">HEAD</span> <span class="hljs-string">/index.html</span> <span class="hljs-meta">HTTP/1.1</span><br><span class="hljs-attribute">Host</span><span class="hljs-punctuation">: </span>www.example.com<br></code></pre></td></tr></table></figure></li><li><strong>示例响应</strong>：<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">HTTP</span>/<span class="hljs-number">1</span>.<span class="hljs-number">1</span> <span class="hljs-number">200</span> OK<br><span class="hljs-attribute">Date</span>: Mon, <span class="hljs-number">27</span> Jul <span class="hljs-number">2009</span> <span class="hljs-number">12</span>:<span class="hljs-number">28</span>:<span class="hljs-number">53</span> GMT<br><span class="hljs-attribute">Server</span>: Apache/<span class="hljs-number">2</span>.<span class="hljs-number">2</span>.<span class="hljs-number">14</span> (Win32)<br><span class="hljs-attribute">Last</span>-Modified: Wed, <span class="hljs-number">22</span> Jul <span class="hljs-number">2009</span> <span class="hljs-number">19</span>:<span class="hljs-number">15</span>:<span class="hljs-number">56</span> GMT<br><span class="hljs-attribute">Content</span>-Length: <span class="hljs-number">88</span><br><span class="hljs-attribute">Content</span>-Type: text/html<br></code></pre></td></tr></table></figure></li></ul><h3 id="5-5-DELETE-删除文件"><a href="#5-5-DELETE-删除文件" class="headerlink" title="5.5 DELETE: 删除文件"></a>5.5 DELETE: 删除文件</h3><ul><li><strong>用途</strong>：用于删除文件，是与PUT相反的操作。</li><li><strong>特点</strong>：请求URI指定的资源将被删除。由于DELETE方法不带验证机制，存在安全性问题，一般Web网站不使用该方法。</li><li><strong>示例请求</strong>：<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs http"><span class="hljs-keyword">DELETE</span> <span class="hljs-string">/example.html</span> <span class="hljs-meta">HTTP/1.1</span><br><span class="hljs-attribute">Host</span><span class="hljs-punctuation">: </span>www.example.com<br></code></pre></td></tr></table></figure></li><li><strong>示例响应</strong>：<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">HTTP</span>/<span class="hljs-number">1</span>.<span class="hljs-number">1</span> <span class="hljs-number">204</span> No Content<br></code></pre></td></tr></table></figure></li></ul><h3 id="5-6-OPTIONS-询问支持的方法"><a href="#5-6-OPTIONS-询问支持的方法" class="headerlink" title="5.6 OPTIONS: 询问支持的方法"></a>5.6 OPTIONS: 询问支持的方法</h3><ul><li><strong>用途</strong>：用于查询针对请求URI指定的资源支持的方法。</li><li><strong>特点</strong>：返回服务器支持的各种HTTP方法。</li><li><strong>示例请求</strong>：<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs http"><span class="hljs-keyword">OPTIONS</span> <span class="hljs-string">*</span> <span class="hljs-meta">HTTP/1.1</span><br><span class="hljs-attribute">Host</span><span class="hljs-punctuation">: </span>www.example.com<br></code></pre></td></tr></table></figure></li><li><strong>示例响应</strong>：<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs http"><span class="hljs-meta">HTTP/1.1</span> <span class="hljs-number">200</span> OK<br><span class="hljs-attribute">Allow</span><span class="hljs-punctuation">: </span>GET, POST, HEAD, OPTIONS<br></code></pre></td></tr></table></figure></li></ul><h3 id="5-7-TRACE-追踪路径"><a href="#5-7-TRACE-追踪路径" class="headerlink" title="5.7 TRACE: 追踪路径"></a>5.7 TRACE: 追踪路径</h3><ul><li><strong>用途</strong>：用于让Web服务器端将之前的请求通信环回给客户端。</li><li><strong>特点</strong>：通过Max-Forwards首部字段的值递减，最终返回状态码200 OK的响应，包含请求内容。常用于调试和诊断请求路径。</li><li><strong>示例请求</strong>：<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs http"><span class="hljs-keyword">TRACE</span> <span class="hljs-string">/</span> <span class="hljs-meta">HTTP/1.1</span><br><span class="hljs-attribute">Host</span><span class="hljs-punctuation">: </span>www.example.com<br><span class="hljs-attribute">Max-Forwards</span><span class="hljs-punctuation">: </span>2<br></code></pre></td></tr></table></figure></li><li><strong>示例响应</strong>：<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">HTTP</span>/<span class="hljs-number">1</span>.<span class="hljs-number">1</span> <span class="hljs-number">200</span> OK<br><span class="hljs-attribute">Content</span>-Type: message/http<br><span class="hljs-attribute">Content</span>-Length: <span class="hljs-number">1024</span><br><span class="hljs-attribute">TRACE</span> / HTTP/<span class="hljs-number">1</span>.<span class="hljs-number">1</span> Host: www.example.com Max-Forwards: <span class="hljs-number">2</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="5-8-CONNECT-要求用隧道协议连接代理"><a href="#5-8-CONNECT-要求用隧道协议连接代理" class="headerlink" title="5.8 CONNECT: 要求用隧道协议连接代理"></a>5.8 CONNECT: 要求用隧道协议连接代理</h3><ul><li><strong>用途</strong>：用于在与代理服务器通信时建立隧道，实现用隧道协议进行TCP通信，常用于SSL&#x2F;TLS加密通信。</li><li><strong>特点</strong>：主要用于HTTPS请求。</li><li><strong>示例请求</strong>：<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs http"><span class="hljs-keyword">CONNECT</span> <span class="hljs-string">www.example.com:443</span> <span class="hljs-meta">HTTP/1.1</span><br><span class="hljs-attribute">Host</span><span class="hljs-punctuation">: </span>www.example.com<br></code></pre></td></tr></table></figure></li><li><strong>示例响应</strong>：<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">HTTP</span>/<span class="hljs-number">1</span>.<span class="hljs-number">1</span> <span class="hljs-number">200</span> Connection Established<br></code></pre></td></tr></table></figure></li></ul><h2 id="6-使用方法下达命令"><a href="#6-使用方法下达命令" class="headerlink" title="6. 使用方法下达命令"></a>6. 使用方法下达命令</h2><ul><li><strong>方法</strong>：向请求URI指定的资源发送请求报文时，采用称为方法的命令。</li><li><strong>支持的方法</strong>：GET、POST、HEAD等。</li></ul><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102113459101.png" alt="image-20250102113459101"></p><h2 id="7-持久连接节省通信量"><a href="#7-持久连接节省通信量" class="headerlink" title="7. 持久连接节省通信量"></a>7. 持久连接节省通信量</h2><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102113558809.png" alt="初始版本"></p><p>HTTP协议的初始版本中，每次HTTP通信都需要断开TCP连接。这在传输小容量文本时问题不大，但随着HTTP的普及，文档中包含大量图片的情况增多，导致每次请求都会造成无谓的TCP连接建立和断开，增加了通信量的开销。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102113810021.png" alt="image-20250102113810021"></p><h3 id="7-1-持久连接"><a href="#7-1-持久连接" class="headerlink" title="7.1 持久连接"></a>7.1 持久连接</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102113835652.png" alt="image-20250102113835652"></p><p>为了解决上述TCP连接的问题，HTTP&#x2F;1.1和一部分HTTP&#x2F;1.0引入了持久连接（HTTP Persistent Connections，也称为HTTP keep-alive或HTTP connection reuse）的方法。持久连接的特点是，只要任意一端没有明确提出断开连接，则保持TCP连接状态。</p><p><strong>优点</strong>：</p><ul><li>减少了TCP连接的重复建立和断开所造成的额外开销。</li><li>减轻了服务器端的负载。</li><li>减少开销的那部分时间，使HTTP请求和响应能够更早地结束，从而提高了Web页面的显示速度。</li></ul><p>在HTTP&#x2F;1.1中，所有的连接默认都是持久连接，但在HTTP&#x2F;1.0内并未标准化。虽然有一部分服务器通过非标准的手段实现了持久连接，但服务器端不一定能够支持持久连接。毫无疑问，除了服务器端，客户端也需要支持持久连接。</p><h3 id="7-2-管线化"><a href="#7-2-管线化" class="headerlink" title="7.2 管线化"></a>7.2 管线化</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102113951113.png" alt="image-20250102113951113"></p><p>持久连接使得多数请求以管线化（pipelining）方式发送成为可能。以前发送请求后需等待并收到响应，才能发送下一个请求。管线化技术出现后，不用等待响应亦可直接发送下一个请求。</p><p><strong>优点</strong>：</p><ul><li>能够同时并行发送多个请求，而不需要一个接一个地等待响应。</li><li>当请求一个包含多张图片的HTML Web页面时，与挨个连接相比，用持久连接可以让请求更快结束。而管线化技术则比持久连接还要快。请求数越多，时间差就越明显。</li></ul><p>通过持久连接和管线化技术，HTTP协议大大提高了通信效率，减少了不必要的开销，提升了用户体验。</p><h2 id="8-使用-Cookie的状态管理"><a href="#8-使用-Cookie的状态管理" class="headerlink" title="8. 使用 Cookie的状态管理"></a>8. 使用 Cookie的状态管理</h2><p>HTTP协议是一种无状态协议，这意味着它不会保存请求和响应之间的通信状态。这种设计简化了协议，使其能够高效地处理大量事务，但也带来了一些挑战，特别是在需要保持用户状态的情况下。例如，用户在登录后访问网站的不同页面时，仍然需要保持登录状态。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102114136299.png" alt="没有Cookie"></p><h4 id="8-1-无状态协议的缺点"><a href="#8-1-无状态协议的缺点" class="headerlink" title="8.1 无状态协议的缺点"></a>8.1 无状态协议的缺点</h4><ul><li><strong>状态管理困难</strong>：由于HTTP不保存状态，服务器无法识别同一用户的多次请求，导致每次请求都需要重新认证。</li><li><strong>用户体验不佳</strong>：用户每次访问新页面时可能需要重新登录，影响用户体验。</li></ul><h4 id="8-2-Cookie技术的引入"><a href="#8-2-Cookie技术的引入" class="headerlink" title="8.2 Cookie技术的引入"></a>8.2 Cookie技术的引入</h4><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102114244688.png" alt="没有Cookie"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250102114256778.png" alt="有了Cookie"></p><p>为了解决无状态协议带来的问题，引入了Cookie技术。Cookie通过在请求和响应报文中写入Cookie信息来控制客户端的状态。</p><h4 id="8-3-Cookie的工作原理"><a href="#8-3-Cookie的工作原理" class="headerlink" title="8.3 Cookie的工作原理"></a>8.3 Cookie的工作原理</h4><ol><li><p><strong>服务器生成Cookie</strong>：服务器在响应报文中添加一个<code>Set-Cookie</code>首部字段，通知客户端保存Cookie。</p><figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs http"><span class="hljs-meta">HTTP/1.1</span> <span class="hljs-number">200</span> OK<br><span class="hljs-attribute">Set-Cookie</span><span class="hljs-punctuation">: </span>session_id=1234567890<br></code></pre></td></tr></table></figure></li><li><p><strong>客户端保存Cookie</strong>：客户端在收到响应后，会自动保存Cookie信息。</p></li><li><p><strong>客户端发送Cookie</strong>：下次客户端向同一服务器发送请求时，会自动在请求报文中加入Cookie值。</p><figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs http"><span class="hljs-keyword">GET</span> <span class="hljs-string">/page</span> <span class="hljs-meta">HTTP/1.1</span><br><span class="hljs-attribute">Host</span><span class="hljs-punctuation">: </span>example.com<br><span class="hljs-attribute">Cookie</span><span class="hljs-punctuation">: </span>session_id=1234567890<br></code></pre></td></tr></table></figure></li><li><p><strong>服务器读取Cookie</strong>：服务器端发现客户端发送的Cookie后，会检查并对比服务器上的记录，从而识别出请求的用户。</p></li></ol><h4 id="8-4-Cookie的应用场景"><a href="#8-4-Cookie的应用场景" class="headerlink" title="8.4 Cookie的应用场景"></a>8.4 Cookie的应用场景</h4><ul><li><strong>用户认证</strong>：通过Cookie保存用户的登录状态，避免每次请求都需要重新登录。</li><li><strong>个性化设置</strong>：保存用户的偏好设置，如主题、语言等。</li><li><strong>购物车</strong>：保存用户的购物车内容，方便用户继续购物。</li></ul><h4 id="8-5-Cookie的限制"><a href="#8-5-Cookie的限制" class="headerlink" title="8.5 Cookie的限制"></a>8.5 Cookie的限制</h4><ul><li><strong>安全性</strong>：Cookie可以被篡改，因此敏感信息不应存储在Cookie中。</li><li><strong>隐私</strong>：Cookie可以追踪用户的浏览行为，可能引发隐私问题。</li><li><strong>大小限制</strong>：每个域名下的Cookie数量和总大小有限制，通常每个Cookie不超过4KB。</li></ul><p>通过Cookie技术，HTTP协议能够在保持其简单高效的同时，实现状态管理功能，提升用户体验。</p><p>参考：《图解HTTP》第一章</p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>计算机基础</category>
      
      <category>计算机网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>计算机基础</tag>
      
      <tag>HTTP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HTTP基础02：了解Web及网络基础</title>
    <link href="/2024/12/31/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP%E5%9F%BA%E7%A1%8002%EF%BC%9A%E4%BA%86%E8%A7%A3Web%E5%8F%8A%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <url>/2024/12/31/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP%E5%9F%BA%E7%A1%8002%EF%BC%9A%E4%BA%86%E8%A7%A3Web%E5%8F%8A%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h2 id="了解Web及网络基础"><a href="#了解Web及网络基础" class="headerlink" title="了解Web及网络基础"></a>了解Web及网络基础</h2><h3 id="1-使用HTTP协议访问Web"><a href="#1-使用HTTP协议访问Web" class="headerlink" title="1. 使用HTTP协议访问Web"></a>1. 使用HTTP协议访问Web</h3><ul><li><p><strong>URL输入与页面显示</strong>：当用户在浏览器地址栏中输入URL时，浏览器会向指定的Web服务器发送请求，获取相应的资源（如HTML文件、图片等），并将这些资源渲染成用户可见的Web页面。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241231135527092.png" alt="image-20241231135527092"></p></li><li><p><strong>客户端与服务器</strong>：浏览器作为客户端，负责向服务器发送请求并接收响应。服务器则存储资源并响应客户端的请求。这种请求-响应模式是Web通信的基础。</p></li></ul><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241231135627239.png" alt="image-20241231135627239"></p><ul><li><strong>HTTP协议</strong>：HTTP（超文本传输协议）是Web通信的核心协议，定义了客户端和服务器之间的通信规则。它基于请求-响应模型，客户端发送请求，服务器返回响应。HTTP协议是无状态的，意味着每次请求都是独立的，服务器不会保留之前的请求信息。</li></ul><h3 id="2-HTTP的诞生"><a href="#2-HTTP的诞生" class="headerlink" title="2. HTTP的诞生"></a>2. HTTP的诞生</h3><ul><li><strong>背景</strong>：1989年，Tim Berners-Lee在CERN提出了WWW（万维网）的概念，旨在通过互联网实现全球知识共享。他提出了超文本（HyperText）的概念，即文档之间可以通过链接相互关联，形成一个巨大的信息网络。</li><li><strong>WWW构建技术</strong>：WWW的构建依赖于三大核心技术：<ul><li><strong>HTML</strong>（超文本标记语言）：用于创建和格式化Web页面。</li><li><strong>HTTP</strong>（超文本传输协议）：用于在客户端和服务器之间传输数据。</li><li><strong>URL</strong>（统一资源定位符）：用于定位互联网上的资源。</li></ul></li><li><strong>Web的成长</strong>：1990年，CERN成功研发了世界上第一台Web服务器和Web浏览器。1993年，NCSA Mosaic浏览器问世，它支持内联图像显示，迅速在全球流行。1994年，Netscape Navigator发布，成为当时最流行的浏览器。1995年，微软发布了Internet Explorer，开启了浏览器大战。</li><li><strong>HTTP版本</strong>：<ul><li><strong>HTTP&#x2F;0.9</strong>：1990年发布，功能极为简单，仅支持GET请求。</li><li><strong>HTTP&#x2F;1.0</strong>：1996年发布，引入了更多的请求方法（如POST、HEAD）和头部字段，支持多种内容类型。</li><li><strong>HTTP&#x2F;1.1</strong>：1997年发布，是目前最广泛使用的版本，支持持久连接、分块传输编码等特性。</li><li><strong>HTTP&#x2F;2.0</strong>：正在制定中，旨在提高性能，支持多路复用、头部压缩等新特性。</li></ul></li></ul><h3 id="3-网络基础TCP-IP"><a href="#3-网络基础TCP-IP" class="headerlink" title="3. 网络基础TCP&#x2F;IP"></a>3. 网络基础TCP&#x2F;IP</h3><ul><li><strong>TCP&#x2F;IP协议族</strong>：TCP&#x2F;IP是互联网的基础协议族，定义了计算机如何在网络上进行通信。HTTP协议是TCP&#x2F;IP协议族的一部分，依赖于TCP&#x2F;IP进行数据传输。</li></ul><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241231135723889.png" alt="image-20241231135723889"></p><ul><li><p>为什么需要 TCP&#x2F;IP？</p><ul><li><strong>背景</strong>：互联网是由全球数以亿计的计算机和网络设备组成的复杂网络。为了让这些设备能够相互通信，必须有一套统一的规则和协议。TCP&#x2F;IP 协议族就是为此而设计的，它定义了计算机和网络设备之间如何通信、如何传输数据、如何寻址等一系列规则。</li><li><strong>跨平台通信</strong>：不同的计算机可能使用不同的硬件、操作系统和网络技术。TCP&#x2F;IP 协议族提供了一种标准化的通信方式，使得这些异构系统能够无缝地进行数据交换。</li><li><strong>模块化设计</strong>：TCP&#x2F;IP 协议族采用分层设计，每一层负责特定的功能。这种模块化的设计使得协议族易于扩展和维护，某一层的改动不会影响其他层的功能。</li></ul></li><li><p>分层管理的优点</p><ul><li><p><strong>简化设计</strong>：分层设计将复杂的网络通信问题分解为多个相对简单的子问题，每一层只需关注自己的任务，无需了解其他层的细节。</p></li><li><p><strong>易于维护和扩展</strong>：如果某一层的协议需要更新或替换，只需修改该层的实现，而不会影响其他层。例如，传输层的 TCP 协议可以独立于应用层的 HTTP 协议进行优化。</p></li><li><p><strong>灵活性</strong>：分层设计允许不同的协议在同一层中并存。例如，传输层可以使用 TCP 或 UDP，具体选择取决于应用的需求。</p></li><li><p><strong>标准化接口</strong>：每一层之间通过标准化的接口进行通信，确保了不同厂商的设备能够互操作。</p></li></ul></li><li><p><strong>分层管理</strong>：TCP&#x2F;IP协议族分为四层：</p><ul><li><strong>应用层</strong>：提供应用程序之间的通信服务，如HTTP、FTP、DNS等。</li><li><strong>传输层</strong>：负责端到端的数据传输，主要协议有TCP和UDP。</li><li><strong>网络层</strong>：处理在网络上流动的数据包。数据包是网络传输的最小数 据单位。该层规定了通过怎样的路径（所谓的传输路线）到达对方计 算机，并把数据包传送给对方。</li><li><strong>链路层</strong>：硬件部分。包括控制操作系统、硬件的设备驱 动、NIC（Network Interface Card，网络适配器，即网卡），及光纤等 物理可见部分（还包括连接器等一切传输媒介）。硬件上的范畴均在 链路层的作用范围之内。</li></ul></li><li><p><strong>通信传输流</strong></p><ul><li>用 HTTP 举例来说明，首先作为发送端的客户端在应用层 （HTTP 协议）发出一个想看某个 Web 页面的 HTTP 请求。 </li><li>接着，为了传输方便，在传输层（TCP 协议）把从应用层处收到的数 据（HTTP 请求报文）进行分割，并在各个报文上打上标记序号及端 口号后转发给网络层。 </li><li>在网络层（IP 协议），增加作为通信目的地的 MAC 地址后转发给链 路层。这样一来，发往网络的通信请求就准备齐全了。 </li><li>接收端的服务器在链路层接收到数据，按序往上层发送，一直到应用 层。</li><li>当传输到应用层，才能算真正接收到由客户端发送过来的 HTTP 请求。</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241231140702101.png" alt="image-20241231140702101"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241231140906221.png" alt="image-20241231140906221"></p><ul><li><p>发送端在层与层之间传输数据时，每经过一层时必定会被打上一个该 层所属的首部信息。反之，接收端在层与层传输数据时，每经过一层 时会把对应的首部消去。</p></li><li><p>这种把数据信息包装起来的做法称为封装（encapsulate）。</p></li></ul><h3 id="4-与HTTP关系密切的协议：IP、TCP和DNS"><a href="#4-与HTTP关系密切的协议：IP、TCP和DNS" class="headerlink" title="4. 与HTTP关系密切的协议：IP、TCP和DNS"></a>4. 与HTTP关系密切的协议：IP、TCP和DNS</h3><ul><li><strong>IP协议</strong>：IP（网际协议）位于网络层，负责将数据包从源地址传输到目的地址。IP协议依赖于IP地址和MAC地址进行通信。IP地址是逻辑地址，可以变化，而MAC地址是物理地址，通常不变。IP协议使用ARP（地址解析协议）来根据IP地址查找对应的MAC地址。</li><li><strong>TCP协议</strong>：TCP（传输控制协议）位于传输层，提供可靠的字节流服务。它将大数据分割成小的报文段进行传输，并通过三次握手确保数据的可靠传输。TCP还提供了流量控制、拥塞控制等机制，确保数据传输的稳定性。</li><li><strong>DNS服务</strong>：DNS（域名系统）位于应用层，负责将域名解析为IP地址。用户通常使用域名访问网站，而不是直接使用IP地址，因为域名更易于记忆。DNS服务通过域名查找IP地址，或从IP地址反查域名。</li></ul><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241231141054269.png" alt="数据包发送"></p><h4 id="数据包发送的过程："><a href="#数据包发送的过程：" class="headerlink" title="数据包发送的过程："></a>数据包发送的过程：</h4><ol><li><p><strong>发送端准备发送数据包</strong>：</p><ul><li>发送端想要将数据包发送到IP地址为192.0.43.10的目的地。</li><li>发送端使用ARP（地址解析协议）来解析目标IP地址对应的MAC地址。</li></ul></li><li><p><strong>ARP解析过程</strong>：</p><ul><li>发送端通过ARP协议尝试解析目标IP地址192.0.43.10的MAC地址，但此时还未成功解析到具体的MAC地址。</li></ul></li><li><p><strong>先将数据包发送给路由器</strong>：</p><ul><li>由于ARP解析未完成，发送端先将数据包发送给一个中间路由器，该路由器的MAC地址为00-XX-C6-6B-XX-XX。</li></ul></li><li><p><strong>第一次转发</strong>：</p><ul><li>路由器接收到数据包后，进行第一次转发。路由器将数据包转发给下一个路由器，其MAC地址为00-XX-B5-A5-XX-XX。</li></ul></li><li><p><strong>第二次转发</strong>：</p><ul><li>第二个路由器接收到数据包后，进行第二次转发。此时，路由器已经知道目标IP地址192.0.43.10的具体MAC地址为00-XX-A6-6B-XX-XX。</li><li>路由器将数据包转发给最终的目标接收端。</li></ul></li><li><p><strong>数据包到达接收端</strong>：</p><ul><li>最终，数据包成功到达接收端，接收端的IP地址为192.0.43.10，MAC地址为00-XX-A6-6B-XX-XX。</li></ul></li></ol><p>总结：</p><ul><li>数据包从发送端出发，通过多个路由器的转发，最终到达目标接收端。</li><li>在这个过程中，ARP协议用于解析目标IP地址的MAC地址，确保数据包能够正确送达目的地。</li></ul><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241231141211177.png" alt="三次握手"></p><p>字节流服务：TCP 位于传输层，提供可靠的字节流服务。为了方便传输，将大 块数据分割成以报文段（segment）为单位的数据包进行管理。</p><h4 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a>三次握手</h4><ul><li>发送端首先发送一个带 SYN 标志的数据包给对方。</li><li>接收端收到后， 回传一个带有 SYN&#x2F;ACK 标志的数据包以示传达确认信息。</li><li>最后，发 送端再回传一个带 ACK 标志的数据包，代表“握手”结束。</li></ul><p>若在握手过程中某个阶段莫名中断，TCP 协议会再次以相同的顺序发 送相同的数据包。</p><h3 id="5-负责域名解析的DNS服务"><a href="#5-负责域名解析的DNS服务" class="headerlink" title="5. 负责域名解析的DNS服务"></a>5. 负责域名解析的DNS服务</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241231141657984.png" alt="image-20241231141657984"></p><ul><li><strong>DNS功能</strong>：DNS的主要功能是将人类可读的域名转换为计算机可识别的IP地址。例如，当用户输入<code>www.example.com</code>时，DNS会将其解析为对应的IP地址，如<code>192.0.2.1</code>，以便浏览器能够与服务器建立连接。</li><li><strong>用户习惯</strong>：用户通常使用域名访问网站，因为域名比IP地址更容易记忆。DNS服务使得用户无需记住复杂的IP地址，只需输入简单的域名即可访问资源。</li></ul><h3 id="6-各种协议与HTTP协议的关系"><a href="#6-各种协议与HTTP协议的关系" class="headerlink" title="6. 各种协议与HTTP协议的关系"></a>6. 各种协议与HTTP协议的关系</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241231141751459.png" alt="image-20241231141751459"></p><ul><li><strong>HTTP通信过程</strong>：当用户访问一个Web页面时，首先通过DNS解析域名获取IP地址，然后通过IP协议找到目标服务器，接着通过TCP协议建立可靠的连接，最后通过HTTP协议发送请求并接收响应。整个过程涉及多个协议的协同工作。</li><li><strong>协议协作</strong>：DNS负责域名解析，IP协议负责寻址，TCP协议确保数据传输的可靠性，HTTP协议则负责生成请求和处理响应。这些协议共同构成了Web通信的基础。</li></ul><h4 id="1-7-URI和URL"><a href="#1-7-URI和URL" class="headerlink" title="1.7 URI和URL"></a>1.7 URI和URL</h4><ul><li><strong>URI与URL</strong>：URI（统一资源标识符）是用于标识互联网上资源的字符串，而URL（统一资源定位符）是URI的子集，表示资源的具体位置。URL是用户在浏览器中输入的地址，如<code>http://www.example.com</code>。</li><li><strong>URI格式</strong>：URI的格式包括协议方案、登录信息、服务器地址、端口号、文件路径、查询字符串和片段标识符。例如，<code>http://user:password@www.example.com:80/path/to/resource?query=string#fragment</code>。</li><li><strong>RFC标准</strong>：HTTP协议通常遵循RFC（Request for Comments）标准，RFC是互联网技术标准文档。虽然大多数应用程序遵循RFC标准，但某些应用程序可能会扩展或偏离标准，导致兼容性问题。</li></ul><h3 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h3><p>本章详细介绍了Web的基础技术，特别是HTTP协议的诞生、发展及其与TCP&#x2F;IP协议族的关系。通过了解IP、TCP、DNS等协议，可以更好地理解HTTP在Web通信中的作用。URI和URL的概念及其格式也是Web开发中的重要基础知识。</p><p>参考：《图解HTTP》第一章</p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>计算机基础</category>
      
      <category>计算机网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>计算机基础</tag>
      
      <tag>HTTP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HTTP基础01：HTTP协议概述</title>
    <link href="/2024/12/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP%E5%9F%BA%E7%A1%8001%EF%BC%9AHTTP%E5%8D%8F%E8%AE%AE%E6%A6%82%E8%BF%B0/"/>
    <url>/2024/12/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP%E5%9F%BA%E7%A1%8001%EF%BC%9AHTTP%E5%8D%8F%E8%AE%AE%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="HTTP协议概述"><a href="#HTTP协议概述" class="headerlink" title="HTTP协议概述"></a>HTTP协议概述</h1><p>在当今数字化的时代，我们每天都在与网络进行着无数次的交互，而这背后离不开众多网络协议的支撑，其中 HTTP（超文本传输协议）起着至关重要的作用。无论是浏览网页、使用手机应用获取数据，还是各种智能设备之间的信息传输，HTTP 都在默默地为我们服务。今天，就让我们深入了解一下 HTTP 的奥秘。</p><h2 id="1-HTTP-的定义与作用"><a href="#1-HTTP-的定义与作用" class="headerlink" title="1. HTTP 的定义与作用"></a>1. HTTP 的定义与作用</h2><h3 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h3><p>HTTP 是一种用于分布式、协作式和超媒体信息系统的应用层协议。简单来说，它定义了客户端和服务器之间如何进行通信，规定了数据的格式、传输方式以及各种操作的方法。它工作在 <strong>TCP&#x2F;IP 协议栈的应用层</strong>，<strong>基于传输层的 TCP 协议</strong>来确保数据的可靠传输。</p><p>例如，当你在浏览器中输入一个网址并按下回车键时，浏览器就会作为客户端向服务器发送一个 HTTP 请求，请求获取特定的网页资源。服务器接收到请求后，根据 HTTP 协议的规范对请求进行处理，并将相应的网页内容以 HTTP 响应的形式返回给浏览器，浏览器再将这些内容解析并展示给用户，这就是 HTTP 在我们日常网络浏览中最基本的工作流程。</p><h3 id="1-2-作用"><a href="#1-2-作用" class="headerlink" title="1.2 作用"></a>1.2 作用</h3><p>HTTP 的主要作用是<strong>实现客户端和服务器之间的超文本数据传输</strong>，从而使得我们能够在互联网上获取和交换各种信息。它使得不同的系统和平台能够相互通信，无论是大型的服务器集群还是小型的嵌入式设备，只要遵循 HTTP 协议，就能够进行有效的数据交互。</p><p>比如，</p><p>电子商务网站依靠 HTTP 协议来实现商品信息的展示、用户订单的提交和处理；</p><p>社交媒体平台利用它来加载动态内容、上传和下载图片视频等；</p><p>在线教育平台借助 HTTP 让学生获取课程资料、参与直播互动等。</p><p>可以说，HTTP 是现代互联网应用的基石，没有它，我们所熟悉的丰富多彩的网络世界将不复存在。</p><h2 id="2-HTTP-的发展历史"><a href="#2-HTTP-的发展历史" class="headerlink" title="2. HTTP 的发展历史"></a>2. HTTP 的发展历史</h2><table><thead><tr><th>版本</th><th>主要特性</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>HTTP&#x2F;0.9</td><td>只支持 GET 方法，无请求和响应头部信息，服务器仅响应 HTML 文档，连接在响应后立即关闭</td><td>简单，满足基本网页浏览需求</td><td>功能单一，仅支持 HTML 文档获取，无连接复用</td></tr><tr><td>HTTP&#x2F;1.0</td><td>引入更多请求方法（如 POST、HEAD 等），增加请求和响应头部信息，支持多种文件类型传输</td><td>丰富了网页内容呈现形式，能传输多种类型资源</td><td>每个请求需建立新的 TCP 连接，性能较低</td></tr><tr><td>HTTP&#x2F;1.1</td><td>引入持久连接（Keep-Alive），增加更多请求方法（如 PUT、DELETE 等）和头部字段</td><td>减少连接建立和关闭开销，提高网络传输效率，增强协议功能和灵活性</td><td>仍存在性能瓶颈，如队头阻塞问题</td></tr><tr><td>HTTP&#x2F;2</td><td>采用二进制分帧层，实现多路复用，支持服务器推送</td><td>显著提高资源并行传输能力，降低延迟，优化网络性能</td><td>对服务器和客户端实现要求较高，部分老旧基础设施和设备支持不佳</td></tr><tr><td>HTTP&#x2F;3</td><td>基于 UDP 协议，引入 QUIC 协议，继承 HTTP&#x2F;2 特性</td><td>进一步降低延迟，具备更好连接迁移能力和拥塞控制，提升性能和可靠性</td><td>UDP 协议本身的可靠性相对 TCP 较弱，应用普及需要时间</td></tr></tbody></table><h3 id="2-1-HTTP-0-9"><a href="#2-1-HTTP-0-9" class="headerlink" title="2.1 HTTP&#x2F;0.9"></a>2.1 HTTP&#x2F;0.9</h3><p>HTTP 的最初版本 HTTP&#x2F;0.9 极其简单，它<strong>只支持 GET 方法</strong>，主要用于获取 HTML 文档。在这个版本中，请求和响应都没有头部信息，服务器只能响应 HTML 格式的内容，并且连接在响应后立即关闭，不支持其他类型的资源传输和复杂的交互操作。但正是这个简单的版本开启了 HTTP 协议的发展之路，满足了当时人们对网页浏览的基本需求，使得网络信息的获取变得更加便捷。</p><h3 id="2-2-HTTP-1-0"><a href="#2-2-HTTP-1-0" class="headerlink" title="2.2 HTTP&#x2F;1.0"></a>2.2 HTTP&#x2F;1.0</h3><p>随着网络应用的逐渐丰富，HTTP&#x2F;1.0 应运而生。它引入了<strong>更多的请求方法（如 POST、HEAD</strong> 等），并且增加了请求和响应的头部信息，使得客户端和服务器能够传递更多关于请求和响应的元数据，例如内容类型、编码方式等。这一版本还支持多种类型的文件传输，如图片、样式表等，大大丰富了网页的内容呈现形式。然而，HTTP&#x2F;1.0 存在一个显著的问题，即每个请求都需要建立一个新的 TCP 连接，这导致了在请求大量资源时效率较低，因为建立和关闭 TCP 连接的开销较大。</p><h3 id="2-3-HTTP-1-1"><a href="#2-3-HTTP-1-1" class="headerlink" title="2.3 HTTP&#x2F;1.1"></a>2.3 HTTP&#x2F;1.1</h3><p>为了解决 HTTP&#x2F;1.0 的性能问题，HTTP&#x2F;1.1 进行了一系列重要的改进。它引入了<strong>持久连接（Keep-Alive）</strong>，允许在一个 TCP 连接上进行多个 HTTP 请求和响应的交互，减少了建立和关闭连接的开销，提高了网络传输效率。同时，还增加了更多的请求方法（如 PUT、DELETE 等）和头部字段，进一步增强了协议的功能和灵活性。HTTP&#x2F;1.1 成为了应用最广泛且持续时间最长的 HTTP 版本，至今仍在许多场景中被大量使用，但随着网络技术的不断发展和应用需求的日益增长，它也逐渐暴露出一些性能瓶颈。</p><h3 id="2-4-HTTP-2"><a href="#2-4-HTTP-2" class="headerlink" title="2.4 HTTP&#x2F;2"></a>2.4 HTTP&#x2F;2</h3><p>HTTP&#x2F;2 在性能优化方面迈出了重大一步。它采用了二进制分帧层，将 HTTP 消息分解为更小的帧进行传输，这些帧可以在同一个 TCP 连接上交错发送和接收，从而实现了多路复用，大大提高了资源的并行传输能力，进一步提升了网络性能。同时，HTTP&#x2F;2 还支持服务器推送，服务器可以主动向客户端推送资源，减少了客户端请求的延迟。不过，HTTP&#x2F;2 的推广和应用也面临一些挑战，例如对服务器和客户端的实现要求较高，部分老旧的网络基础设施和设备可能无法很好地支持它。</p><h3 id="2-5-HTTP-3"><a href="#2-5-HTTP-3" class="headerlink" title="2.5 HTTP&#x2F;3"></a>2.5 HTTP&#x2F;3</h3><p>为了克服 HTTP&#x2F;2 在 TCP 协议上的一些性能限制，HTTP&#x2F;3 基于 UDP 协议进行了全新的设计，引入了 QUIC 协议。QUIC 协议在 UDP 之上实现了类似 TCP 的可靠传输功能，同时还具备更低的延迟、更好的连接迁移能力以及对网络拥塞的优化控制。HTTP&#x2F;3 继承了 HTTP&#x2F;2 的许多优秀特性，如二进制分帧、多路复用和服务器推送等，并通过 QUIC 协议进一步提升了性能和可靠性。目前，HTTP&#x2F;3 正在逐渐得到广泛的应用和支持，有望成为未来网络通信的主流协议之一。</p><h2 id="3-HTTP-的基本特点"><a href="#3-HTTP-的基本特点" class="headerlink" title="3. HTTP 的基本特点"></a>3. HTTP 的基本特点</h2><h3 id="3-1-无状态"><a href="#3-1-无状态" class="headerlink" title="3.1 无状态"></a>3.1 无状态</h3><p>HTTP 的无状态特性是指协议对于事务处理没有记忆能力。每一次的 HTTP 请求都是独立的，服务器不会记住之前的请求信息，也不会在不同的请求之间保留任何状态数据。</p><p>例如，当你在一个电商网站上先后将两件商品加入购物车，对于服务器来说，这是两个完全独立的 HTTP 请求，它不会自动关联这两个操作，因为 HTTP 协议本身并不知道这两个请求来自同一个用户的同一次购物行为。这种无状态性使得服务器的设计更加简单高效，能够轻松应对大量并发的请求。但在某些需要保持用户状态的应用场景中，如用户登录后的权限管理、购物车功能等，就需要通过其他技术手段（如使用 Cookie、Session 等）来在客户端和服务器之间维护状态信息。</p><h3 id="3-2-无连接"><a href="#3-2-无连接" class="headerlink" title="3.2 无连接"></a>3.2 无连接</h3><p>在 HTTP&#x2F;1.0 中，默认采用的是无连接的方式，即客户端与服务器在完成一次 HTTP 请求和响应后，会立即关闭连接。这种方式在每次请求都需要重新建立连接，虽然简单直接，但在请求频繁的情况下，会造成较大的性能开销，因为建立和关闭 TCP 连接需要消耗一定的时间和资源。</p><p>而在 HTTP&#x2F;1.1 及以后的版本中，引入了持久连接（Keep-Alive）来改善这一问题，使得在一个 TCP 连接上可以进行多次 HTTP 请求和响应的交互，减少了连接建立和关闭的次数，从而提高了网络传输效率。但从协议本身的设计初衷来看，HTTP 最初是基于无连接的理念构建的，这种理念在一定程度上简化了服务器的实现和资源管理，同时也使得 HTTP 能够更灵活地适应不同的网络环境和应用场景。</p><p>HTTP 作为网络通信领域的重要协议，从诞生之初到不断发展演变，始终在适应着互联网的发展需求。了解 HTTP 的定义、作用、发展历史和基本特点，不仅有助于我们深入理解网络通信的原理，更能为我们在开发高效、稳定的网络应用时提供有力的理论支持和实践指导。随着技术的不断进步，HTTP 协议也将继续进化，为构建更加智能、便捷的网络世界贡献力量。</p><h3 id="3-3-基于请求-响应模型"><a href="#3-3-基于请求-响应模型" class="headerlink" title="3.3 基于请求 - 响应模型"></a>3.3 <strong>基于请求 - 响应模型</strong></h3><ul><li><strong>工作方式</strong>：客户端发起请求，服务器接收请求后进行处理并返回响应。例如，当用户在浏览器中访问一个网页时，浏览器会构建一个HTTP请求，这个请求包含请求方法（如GET、POST等）、请求URL、请求头和请求体（如果是POST等方法可能有）等信息。服务器根据请求中的信息，查找相应的资源，然后构建一个HTTP响应，其中包含状态码、响应头和响应体（如网页内容、文件数据等），并将其发送回客户端。</li><li><strong>优点</strong>：这种模型简单直观，使得客户端和服务器之间的交互逻辑清晰。开发人员可以很容易地理解和实现通信过程，方便构建各种网络应用。而且它允许服务器对不同的请求进行不同的处理，具有很好的灵活性。</li><li><strong>应用场景</strong>：广泛应用于各种网络服务，如网页浏览、API调用等。在网页浏览中，浏览器根据用户输入的网址或者操作（如点击链接、提交表单）发送请求，服务器返回对应的网页或者处理结果。在API调用场景下，客户端（如移动应用、其他服务器等）通过HTTP请求向提供API的服务器获取或更新数据。</li></ul><h3 id="3-4-简单性和通用性"><a href="#3-4-简单性和通用性" class="headerlink" title="3.4 简单性和通用性"></a>3.4 <strong>简单性和通用性</strong></h3><ul><li><p><strong>协议格式简单</strong>：HTTP是一种基于文本的协议，它的请求和响应消息格式相对简单，易于理解和实现。例如，一个简单的HTTP GET请求可能如下所示：</p><figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs http"><span class="hljs-keyword">GET</span> <span class="hljs-string">/index.html</span> <span class="hljs-meta">HTTP/1.1</span><br><span class="hljs-attribute">Host</span><span class="hljs-punctuation">: </span>www.example.com<br></code></pre></td></tr></table></figure><p>这是一个请求获取<code>www.example.com</code>网站的<code>index.html</code>文件的HTTP请求。第一行是请求行，包含请求方法（GET）、请求的资源路径（<code>/index.html</code>）和协议版本（<code>HTTP/1.1</code>）。后面的行是请求头，这里的<code>Host</code>头指定了请求的目标主机。</p></li><li><p><strong>通用性高</strong>：几乎所有的网络编程语言都提供了对HTTP的支持，无论是Python（通过<code>requests</code>库等）、Java（通过<code>java.net.HttpURLConnection</code>等）还是JavaScript（在浏览器中通过<code>XMLHttpRequest</code>或者<code>fetch</code> API）。这使得不同平台和语言编写的客户端和服务器能够很容易地进行通信。</p></li><li><p><strong>优点</strong>：由于简单性，开发人员可以快速上手，降低了开发网络应用的难度。通用性使得HTTP能够在各种不同的设备和系统中广泛应用，促进了网络的互联互通。</p></li><li><p><strong>应用场景</strong>：在物联网设备中，简单的传感器设备可以使用HTTP将采集的数据发送到服务器，或者接收服务器的控制指令。小型的Web应用开发，尤其是初学者或者快速原型开发，利用HTTP的简单性可以快速搭建起基本的功能架构。</p></li></ul><h3 id="3-5-可扩展性"><a href="#3-5-可扩展性" class="headerlink" title="3.5 可扩展性"></a>3.5 <strong>可扩展性</strong></h3><ul><li><strong>协议扩展机制</strong>：HTTP允许通过添加新的请求方法、头部字段等来扩展其功能。例如，随着Web应用的发展，出现了新的请求方法如PATCH（用于部分更新资源）。同时，自定义的头部字段也可以用于在客户端和服务器之间传递特定的元数据。</li><li><strong>版本更新支持进步</strong>：从HTTP&#x2F;0.9到HTTP&#x2F;3的发展历程也体现了它的可扩展性。每个新版本都在保留基本的通信模式的基础上，对性能、功能等方面进行了扩展和优化。例如，HTTP&#x2F;2的二进制分帧和多路复用就是对原有协议传输机制的重大扩展，提高了协议的性能和效率。</li><li><strong>优点</strong>：能够适应不断变化的网络应用需求。随着技术的发展，如移动互联网、云计算、物联网等新兴领域的出现，HTTP可以通过扩展来满足这些新场景下的数据传输和交互需求。</li><li><strong>应用场景</strong>：在现代的微服务架构中，HTTP可以通过扩展来支持服务之间的复杂通信和协调。例如，在一个包含多个微服务的电商系统中，不同微服务之间（如用户服务、商品服务、订单服务等）可以通过自定义的HTTP请求方法和头部字段来实现高效的业务逻辑交互。</li></ul><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>计算机基础</category>
      
      <category>计算机网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>计算机基础</tag>
      
      <tag>HTTP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>llama3源码解析-03：model.py模块解析</title>
    <link href="/2024/12/30/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/nlp/llm/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9Allama3%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-03%EF%BC%9Amodel.py%E6%A8%A1%E5%9D%97%E8%A7%A3%E6%9E%90/"/>
    <url>/2024/12/30/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/nlp/llm/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9Allama3%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-03%EF%BC%9Amodel.py%E6%A8%A1%E5%9D%97%E8%A7%A3%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/Llama3_Repo.jpeg" alt="llama"></p><h2 id="整体"><a href="#整体" class="headerlink" title="整体"></a>整体</h2><p><code>model.py</code> 模块是 Llama 3 模型的核心实现部分，主要负责定义和实现 Transformer 模型的结构及其相关组件。</p><h3 id="1-模型参数定义-ModelArgs-类"><a href="#1-模型参数定义-ModelArgs-类" class="headerlink" title="1. 模型参数定义 (ModelArgs 类)"></a>1. <strong>模型参数定义 (<code>ModelArgs</code> 类)</strong></h3><ul><li><code>ModelArgs</code> 类是一个数据类，用于定义和存储模型的各种超参数，例如：<ul><li><code>dim</code>: 模型的维度。</li><li><code>n_layers</code>: Transformer 的层数。</li><li><code>n_heads</code>: 注意力机制中的头数。</li><li><code>vocab_size</code>: 词汇表大小。</li><li><code>max_batch_size</code>: 最大批处理大小。</li><li><code>max_seq_len</code>: 最大序列长度。</li></ul></li><li>这些参数在模型初始化时被使用，决定了模型的结构和行为。</li></ul><h3 id="2-RMSNorm-层-RMSNorm-类"><a href="#2-RMSNorm-层-RMSNorm-类" class="headerlink" title="2. RMSNorm 层 (RMSNorm 类)"></a>2. <strong>RMSNorm 层 (<code>RMSNorm</code> 类)</strong></h3><ul><li><code>RMSNorm</code> 是一个自定义的归一化层，用于替代传统的 LayerNorm。它通过对输入进行<strong>均方根归一化</strong>来稳定训练过程。</li><li>该层在 Transformer 的每个子层（如注意力机制和前馈网络）之后使用。</li></ul><h3 id="3-RoPE-Rotary-Positional-Embedding"><a href="#3-RoPE-Rotary-Positional-Embedding" class="headerlink" title="3. RoPE (Rotary Positional Embedding)"></a>3. <strong>RoPE (Rotary Positional Embedding)</strong></h3><ul><li>该模块实现了旋转位置编码（RoPE），用于为输入序列中的每个位置生成位置编码。RoPE 通过将位置信息嵌入到注意力机制中，帮助模型捕捉序列中的位置关系。</li><li><code>precompute_freqs_cis</code> 函数预计算了频率矩阵，<code>apply_rotary_emb</code> 函数将旋转<strong>位置编码应用到查询和键向量上。</strong></li></ul><h3 id="4-注意力机制-Attention-类"><a href="#4-注意力机制-Attention-类" class="headerlink" title="4. 注意力机制 (Attention 类)"></a>4. <strong>注意力机制 (<code>Attention</code> 类)</strong></h3><ul><li><code>Attention</code> 类实现了多头注意力机制（Multi-Head Attention），这是 Transformer 模型的核心组件之一。</li><li>它使用 <code>ColumnParallelLinear</code> 和 <code>RowParallelLinear</code> 来实现并行的线性变换，支持模型并行化。</li><li>该模块还实现了键值缓存（KV Cache），用于在生成过程中缓存先前的键和值，以减少重复计算。</li></ul><h3 id="5-前馈网络-FeedForward-类"><a href="#5-前馈网络-FeedForward-类" class="headerlink" title="5. 前馈网络 (FeedForward 类)"></a>5. <strong>前馈网络 (<code>FeedForward</code> 类)</strong></h3><ul><li><code>FeedForward</code> 类实现了 Transformer 中的前馈神经网络（FFN），通常由两个线性变换和一个激活函数（如 SiLU）组成。</li><li>该模块也支持模型并行化，使用 <code>ColumnParallelLinear</code> 和 <code>RowParallelLinear</code> 来实现并行的线性变换。</li></ul><h3 id="6-Transformer-块-TransformerBlock-类"><a href="#6-Transformer-块-TransformerBlock-类" class="headerlink" title="6. Transformer 块 (TransformerBlock 类)"></a>6. <strong>Transformer 块 (<code>TransformerBlock</code> 类)</strong></h3><ul><li><code>TransformerBlock</code> 类将注意力机制和前馈网络组合在一起，形成一个完整的 Transformer 层。</li><li>每个 Transformer 块包含一个注意力层和一个前馈网络层，并且在每个子层之后应用 RMSNorm 进行归一化。</li></ul><h3 id="7-Transformer-模型-Transformer-类"><a href="#7-Transformer-模型-Transformer-类" class="headerlink" title="7. Transformer 模型 (Transformer 类)"></a>7. <strong>Transformer 模型 (<code>Transformer</code> 类)</strong></h3><ul><li><code>Transformer</code> 类是整个模型的核心，它由多个 <code>TransformerBlock</code> 组成，形成一个深层的 Transformer 网络。</li><li>该模块还负责处理输入嵌入、位置编码、以及最终的输出线性变换。</li><li><code>forward</code> 方法实现了模型的前向传播过程，包括嵌入、位置编码、多层 Transformer 块的处理以及最终的输出生成。</li></ul><h3 id="8-模型并行化"><a href="#8-模型并行化" class="headerlink" title="8. 模型并行化"></a>8. <strong>模型并行化</strong></h3><ul><li>该模块使用了 <code>fairscale</code> 库中的 <code>ColumnParallelLinear</code> 和 <code>RowParallelLinear</code> 来实现模型并行化，允许模型在多个 GPU 上分布计算，从而提高训练和推理的效率。</li></ul><h3 id="9-推理模式"><a href="#9-推理模式" class="headerlink" title="9. 推理模式"></a>9. <strong>推理模式</strong></h3><ul><li>在推理模式下，模型使用 <code>torch.inference_mode()</code> 来禁用梯度计算，从而提高推理速度并减少内存占用。</li></ul><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p><code>model.py</code> 模块定义了 Llama 3 模型的核心架构，包括 Transformer 的各个组件（如注意力机制、前馈网络、归一化层等），并实现了模型并行化和推理优化。它是整个 Llama 3 模型的基础，负责处理输入数据并生成输出。</p><h2 id="模型详细流程图"><a href="#模型详细流程图" class="headerlink" title="模型详细流程图"></a>模型详细流程图</h2><pre><code class=" mermaid">graph TD    A[输入 tokens] --&gt; B[Token Embedding]    B --&gt; C[添加位置编码 freqs_cis]    C --&gt; D[初始化 mask]    D --&gt; E[进入 Transformer 层]    E --&gt; F[Transformer Block 1]    E --&gt; G[Transformer Block 2]    E --&gt; H[...]    E --&gt; I[Transformer Block N]    F --&gt; J[输出 logits]    G --&gt; J    H --&gt; J    I --&gt; J    subgraph Transformer Block        direction TB        K[输入] --&gt; L[RMSNorm]        L --&gt; M[Attention]        M --&gt; N[Add &amp; Norm]        N --&gt; O[FeedForward]        O --&gt; P[Add &amp; Norm]        P --&gt; Q[输出]    end    F --&gt; K    G --&gt; K    I --&gt; K</code></pre><ol><li><p><strong>输入 tokens</strong>  </p><ul><li>输入是一个批次的 token IDs，形状为 <code>(batch_size, seq_len)</code>。</li></ul></li><li><p><strong>Token Embedding</strong>  </p><ul><li>通过 <code>tok_embeddings</code> 将 token IDs 转换为嵌入向量，形状为 <code>(batch_size, seq_len, dim)</code>。</li></ul></li><li><p><strong>添加位置编码 freqs_cis</strong>  </p><ul><li>使用预计算的 <code>freqs_cis</code> 为嵌入向量添加旋转位置编码，帮助模型捕捉序列中的位置信息。</li></ul></li><li><p><strong>初始化 mask</strong>  </p><ul><li>根据 <code>seq_len</code> 和 <code>start_pos</code> 生成注意力掩码 <code>mask</code>，用于防止模型看到未来的 token。</li></ul></li><li><p><strong>进入 Transformer 层</strong>  </p><ul><li>嵌入向量和位置编码进入多层 Transformer 块进行处理。</li></ul></li><li><p><strong>Transformer Block 1 到 N</strong>  </p><ul><li>每个 Transformer 块包含以下步骤：<ul><li><strong>RMSNorm</strong>: 对输入进行归一化。</li><li><strong>Attention</strong>: 应用多头注意力机制，生成注意力输出。</li><li><strong>RMSNorm</strong>: 对注意力输出进行归一化。</li><li><strong>FeedForward</strong>: 应用前馈网络，生成最终的 Transformer 块输出。</li></ul></li></ul></li><li><p><strong>应用 RMSNorm</strong>  </p><ul><li>在所有 Transformer 块处理完成后，对最终输出应用 RMSNorm 进行归一化。</li></ul></li><li><p><strong>输出线性变换</strong>  </p><ul><li>通过 <code>output</code> 线性层将归一化后的输出映射到词汇表空间，形状为 <code>(batch_size, seq_len, vocab_size)</code>。</li></ul></li><li><p><strong>输出 logits</strong>  </p><ul><li>返回最终的 logits，表示每个 token 的概率分布。</li></ul></li></ol><h2 id="class-ModelArgs"><a href="#class-ModelArgs" class="headerlink" title="class ModelArgs"></a><code>class ModelArgs</code></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ModelArgs</span>:<br>    dim: <span class="hljs-built_in">int</span> = <span class="hljs-number">4096</span>  <span class="hljs-comment"># 模型的维度，即每个token的向量表示的大小</span><br>    n_layers: <span class="hljs-built_in">int</span> = <span class="hljs-number">32</span>  <span class="hljs-comment"># 模型的层数，即Transformer的层数</span><br>    n_heads: <span class="hljs-built_in">int</span> = <span class="hljs-number">32</span>  <span class="hljs-comment"># 注意力机制中的头数</span><br>    n_kv_heads: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>  <span class="hljs-comment"># 键值头的数量，如果为None，则与n_heads相同</span><br>    vocab_size: <span class="hljs-built_in">int</span> = -<span class="hljs-number">1</span>  <span class="hljs-comment"># 词汇表的大小，通常由tokenizer决定</span><br>    multiple_of: <span class="hljs-built_in">int</span> = <span class="hljs-number">256</span>  <span class="hljs-comment"># SwiGLU激活函数中隐藏层大小的倍数，确保是256的倍数</span><br>    ffn_dim_multiplier: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">float</span>] = <span class="hljs-literal">None</span>  <span class="hljs-comment"># 前馈网络维度的乘数，用于调整隐藏层大小</span><br>    norm_eps: <span class="hljs-built_in">float</span> = <span class="hljs-number">1e-5</span>  <span class="hljs-comment"># Layer Normalization中的epsilon值，用于数值稳定性</span><br>    rope_theta: <span class="hljs-built_in">float</span> = <span class="hljs-number">500000</span>  <span class="hljs-comment"># RoPE（Rotary Position Embedding）中的theta参数</span><br><br>    max_batch_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">32</span>  <span class="hljs-comment"># 最大批处理大小</span><br>    max_seq_len: <span class="hljs-built_in">int</span> = <span class="hljs-number">2048</span>  <span class="hljs-comment"># 最大序列长度</span><br></code></pre></td></tr></table></figure><h2 id="class-RMSNorm"><a href="#class-RMSNorm" class="headerlink" title="class RMSNorm"></a><code>class RMSNorm</code></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RMSNorm</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim: <span class="hljs-built_in">int</span>, eps: <span class="hljs-built_in">float</span> = <span class="hljs-number">1e-6</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.eps = eps  <span class="hljs-comment"># 用于数值稳定性的小值，防止除以零</span><br>        <span class="hljs-variable language_">self</span>.weight = nn.Parameter(torch.ones(dim))  <span class="hljs-comment"># 可学习的缩放参数，初始化为1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_norm</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 计算RMS（Root Mean Square）归一化，对输入x进行归一化处理</span><br>        <span class="hljs-keyword">return</span> x * torch.rsqrt(x.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mean(-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>) + <span class="hljs-variable language_">self</span>.eps)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 前向传播函数，先对输入x进行归一化，然后乘以可学习的缩放参数</span><br>        output = <span class="hljs-variable language_">self</span>._norm(x.<span class="hljs-built_in">float</span>()).type_as(x)  <span class="hljs-comment"># 归一化并保持与输入x相同的数据类型</span><br>        <span class="hljs-keyword">return</span> output * <span class="hljs-variable language_">self</span>.weight  <span class="hljs-comment"># 乘以缩放参数</span><br></code></pre></td></tr></table></figure><h3 id="解释："><a href="#解释：" class="headerlink" title="解释："></a>解释：</h3><ol><li><p><strong><code>ModelArgs</code></strong>:</p><ul><li>这是一个数据类，用于存储模型的配置参数。它定义了模型的结构和超参数，如模型的维度、层数、注意力头数等。</li><li><code>dim</code> 是每个token的向量表示的大小，<code>n_layers</code> 是Transformer的层数，<code>n_heads</code> 是注意力机制中的头数。</li><li><code>vocab_size</code> 是词汇表的大小，通常由tokenizer决定。</li><li><code>multiple_of</code> 和 <code>ffn_dim_multiplier</code> 用于调整前馈网络的隐藏层大小。</li><li><code>norm_eps</code> 是Layer Normalization中的epsilon值，用于数值稳定性。</li><li><code>rope_theta</code> 是RoPE（Rotary Position Embedding）中的theta参数，用于位置编码。</li><li><code>max_batch_size</code> 和 <code>max_seq_len</code> 分别定义了模型的最大批处理大小和最大序列长度。</li></ul></li><li><p><strong><code>RMSNorm</code></strong>:</p><ul><li>这是一个自定义的归一化层，类似于Layer Normalization，但使用了RMS（Root Mean Square）归一化。</li><li><code>_norm</code> 方法计算输入的RMS归一化值，<code>forward</code> 方法在前向传播时对输入进行归一化并乘以可学习的缩放参数。</li><li><code>eps</code> 是一个小值，用于防止除以零的情况，<code>weight</code> 是可学习的缩放参数，初始化为1。</li></ul></li></ol><p>这两个类在模型中分别用于定义模型的结构和实现归一化操作，是Transformer模型的重要组成部分。</p><hr><h2 id="旋转位置编码"><a href="#旋转位置编码" class="headerlink" title="旋转位置编码"></a>旋转位置编码</h2><h3 id="precompute-freqs-cis"><a href="#precompute-freqs-cis" class="headerlink" title="precompute_freqs_cis"></a><strong><code>precompute_freqs_cis</code></strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">precompute_freqs_cis</span>(<span class="hljs-params">dim: <span class="hljs-built_in">int</span>, end: <span class="hljs-built_in">int</span>, theta: <span class="hljs-built_in">float</span> = <span class="hljs-number">10000.0</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    预计算旋转位置编码的频率矩阵 (freqs_cis)。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    旋转位置编码 (Rotary Positional Embedding, RoPE) 是一种将位置信息嵌入到注意力机制中的方法。</span><br><span class="hljs-string">    它通过将位置信息编码为复数形式，应用到查询和键向量上。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        dim (int): 模型的维度（通常是注意力头的维度）。</span><br><span class="hljs-string">        end (int): 序列的最大长度。</span><br><span class="hljs-string">        theta (float): 控制频率的基数，默认为 10000.0。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        freqs_cis (torch.Tensor): 预计算的频率矩阵，形状为 (end, dim // 2)，数据类型为 complex64。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 计算频率向量</span><br>    freqs = <span class="hljs-number">1.0</span> / (theta ** (torch.arange(<span class="hljs-number">0</span>, dim, <span class="hljs-number">2</span>)[: (dim // <span class="hljs-number">2</span>)].<span class="hljs-built_in">float</span>() / dim))<br>    <span class="hljs-comment"># 生成位置向量</span><br>    t = torch.arange(end, device=freqs.device, dtype=torch.float32)<br>    <span class="hljs-comment"># 计算外积，得到频率矩阵</span><br>    freqs = torch.outer(t, freqs)<br>    <span class="hljs-comment"># 将频率矩阵转换为复数形式（极坐标表示）</span><br>    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  <span class="hljs-comment"># complex64</span><br>    <span class="hljs-keyword">return</span> freqs_cis<br></code></pre></td></tr></table></figure><h4 id="解释：-1"><a href="#解释：-1" class="headerlink" title="解释："></a>解释：</h4><ul><li><strong>作用</strong>: 预计算旋转位置编码的频率矩阵 <code>freqs_cis</code>，用于将位置信息嵌入到查询和键向量中。</li><li><strong>输入</strong>:<ul><li><code>dim</code>: 模型的维度，通常是注意力头的维度。</li><li><code>end</code>: 序列的最大长度。</li><li><code>theta</code>: 控制频率的基数，默认为 10000.0。</li></ul></li><li><strong>输出</strong>:<ul><li><code>freqs_cis</code>: 预计算的频率矩阵，形状为 <code>(end, dim // 2)</code>，数据类型为 <code>complex64</code>。</li></ul></li><li><strong>关键点</strong>:<ul><li>使用 <code>torch.outer</code> 计算位置向量和频率向量的外积，得到频率矩阵。</li><li>通过 <code>torch.polar</code> 将频率矩阵转换为复数形式，表示极坐标。</li></ul></li></ul><hr><h3 id="reshape-for-broadcast"><a href="#reshape-for-broadcast" class="headerlink" title="reshape_for_broadcast"></a><strong><code>reshape_for_broadcast</code></strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reshape_for_broadcast</span>(<span class="hljs-params">freqs_cis: torch.Tensor, x: torch.Tensor</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    将频率矩阵 `freqs_cis` 重塑为适合广播的形状，以便与查询或键向量进行逐元素操作。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        freqs_cis (torch.Tensor): 频率矩阵，形状为 (seq_len, dim // 2)。</span><br><span class="hljs-string">        x (torch.Tensor): 查询或键向量，形状为 (batch_size, seq_len, n_heads, head_dim)。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        freqs_cis (torch.Tensor): 重塑后的频率矩阵，形状为 (1, seq_len, 1, dim // 2)。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    ndim = x.ndim<br>    <span class="hljs-keyword">assert</span> <span class="hljs-number">0</span> &lt;= <span class="hljs-number">1</span> &lt; ndim<br>    <span class="hljs-keyword">assert</span> freqs_cis.shape == (x.shape[<span class="hljs-number">1</span>], x.shape[-<span class="hljs-number">1</span>])<br>    <span class="hljs-comment"># 重塑频率矩阵，使其形状为 (1, seq_len, 1, dim // 2)</span><br>    shape = [d <span class="hljs-keyword">if</span> i == <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> i == ndim - <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, d <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(x.shape)]<br>    <span class="hljs-keyword">return</span> freqs_cis.view(*shape)<br></code></pre></td></tr></table></figure><h4 id="解释：-2"><a href="#解释：-2" class="headerlink" title="解释："></a>解释：</h4><ul><li><strong>作用</strong>: 将频率矩阵 <code>freqs_cis</code> 重塑为适合广播的形状，以便与查询或键向量进行逐元素操作。</li><li><strong>输入</strong>:<ul><li><code>freqs_cis</code>: 频率矩阵，形状为 <code>(seq_len, dim // 2)</code>。</li><li><code>x</code>: 查询或键向量，形状为 <code>(batch_size, seq_len, n_heads, head_dim)</code>。</li></ul></li><li><strong>输出</strong>:<ul><li>重塑后的频率矩阵，形状为 <code>(1, seq_len, 1, dim // 2)</code>。</li></ul></li><li><strong>关键点</strong>:<ul><li>通过 <code>view</code> 方法将频率矩阵重塑为适合广播的形状，使其能够与查询或键向量进行逐元素操作。</li></ul></li></ul><hr><h3 id="apply-rotary-emb"><a href="#apply-rotary-emb" class="headerlink" title="apply_rotary_emb"></a><strong><code>apply_rotary_emb</code></strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">apply_rotary_emb</span>(<span class="hljs-params"></span><br><span class="hljs-params">    xq: torch.Tensor,</span><br><span class="hljs-params">    xk: torch.Tensor,</span><br><span class="hljs-params">    freqs_cis: torch.Tensor,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Tuple</span>[torch.Tensor, torch.Tensor]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    将旋转位置编码应用到查询和键向量上。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    旋转位置编码通过将位置信息嵌入到查询和键向量中，帮助模型捕捉序列中的位置关系。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        xq (torch.Tensor): 查询向量，形状为 (batch_size, seq_len, n_heads, head_dim)。</span><br><span class="hljs-string">        xk (torch.Tensor): 键向量，形状为 (batch_size, seq_len, n_heads, head_dim)。</span><br><span class="hljs-string">        freqs_cis (torch.Tensor): 频率矩阵，形状为 (1, seq_len, 1, head_dim // 2)。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        xq_out (torch.Tensor): 应用旋转位置编码后的查询向量。</span><br><span class="hljs-string">        xk_out (torch.Tensor): 应用旋转位置编码后的键向量。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 将查询和键向量转换为复数形式</span><br>    xq_ = torch.view_as_complex(xq.<span class="hljs-built_in">float</span>().reshape(*xq.shape[:-<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br>    xk_ = torch.view_as_complex(xk.<span class="hljs-built_in">float</span>().reshape(*xk.shape[:-<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br>    <span class="hljs-comment"># 重塑频率矩阵以匹配查询和键向量的形状</span><br>    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)<br>    <span class="hljs-comment"># 应用旋转位置编码</span><br>    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(<span class="hljs-number">3</span>)<br>    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(<span class="hljs-number">3</span>)<br>    <span class="hljs-keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)<br></code></pre></td></tr></table></figure><h4 id="解释：-3"><a href="#解释：-3" class="headerlink" title="解释："></a>解释：</h4><ul><li><p><strong>作用</strong>: 将旋转位置编码应用到查询和键向量上，帮助模型捕捉序列中的位置关系。</p></li><li><p><strong>输入</strong>:</p><ul><li><code>xq</code>: 查询向量，形状为 <code>(batch_size, seq_len, n_heads, head_dim)</code>。</li><li><code>xk</code>: 键向量，形状为 <code>(batch_size, seq_len, n_heads, head_dim)</code>。</li><li><code>freqs_cis</code>: 频率矩阵，形状为 <code>(1, seq_len, 1, head_dim // 2)</code>。</li></ul></li><li><p><strong>输出</strong>:</p><ul><li><code>xq_out</code>: 应用旋转位置编码后的查询向量。</li><li><code>xk_out</code>: 应用旋转位置编码后的键向量。</li></ul></li><li><p><strong>关键点</strong>:</p><ul><li>使用 <code>torch.view_as_complex</code> 将查询和键向量转换为复数形式。</li><li>通过逐元素乘法将频率矩阵应用到查询和键向量上。</li><li>使用 <code>torch.view_as_real</code> 将结果转换回实数形式。</li></ul><p>这些函数共同实现了旋转位置编码（RoPE）</p><h3 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h3><ul><li><strong><code>precompute_freqs_cis</code></strong>: 预计算旋转位置编码的频率矩阵。</li><li><strong><code>reshape_for_broadcast</code></strong>: 将频率矩阵重塑为适合广播的形状。</li><li><strong><code>apply_rotary_emb</code></strong>: 将旋转位置编码应用到查询和键向量上。</li></ul></li></ul><hr><h2 id="repeat-kv"><a href="#repeat-kv" class="headerlink" title="repeat_kv"></a><strong><code>repeat_kv</code></strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">repeat_kv</span>(<span class="hljs-params">x: torch.Tensor, n_rep: <span class="hljs-built_in">int</span></span>) -&gt; torch.Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    重复键或值向量，以匹配查询向量的头数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    在分组注意力机制中，键和值向量的头数可能少于查询向量的头数，因此需要重复键和值向量以匹配查询向量的头数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        x (torch.Tensor): 键或值向量，形状为 (batch_size, seq_len, n_kv_heads, head_dim)。</span><br><span class="hljs-string">        n_rep (int): 重复次数，通常为查询头数与键值头数的比值。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        torch.Tensor: 重复后的键或值向量，形状为 (batch_size, seq_len, n_kv_heads * n_rep, head_dim)。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    bs, slen, n_kv_heads, head_dim = x.shape<br>    <span class="hljs-keyword">if</span> n_rep == <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> x<br>    <span class="hljs-keyword">return</span> (<br>        x[:, :, :, <span class="hljs-literal">None</span>, :]<br>        .expand(bs, slen, n_kv_heads, n_rep, head_dim)<br>        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)<br>    )<br></code></pre></td></tr></table></figure><h4 id="解释：-4"><a href="#解释：-4" class="headerlink" title="解释："></a>解释：</h4><ul><li><strong>作用</strong>: 重复键或值向量，以匹配查询向量的头数。</li><li><strong>输入</strong>:<ul><li><code>x</code>: 键或值向量，形状为 <code>(batch_size, seq_len, n_kv_heads, head_dim)</code>。</li><li><code>n_rep</code>: 重复次数，通常为查询头数与键值头数的比值。</li></ul></li><li><strong>输出</strong>:<ul><li>重复后的键或值向量，形状为 <code>(batch_size, seq_len, n_kv_heads * n_rep, head_dim)</code>。</li></ul></li><li><strong>关键点</strong>:<ul><li>使用 <code>expand</code> 和 <code>reshape</code> 方法重复键或值向量，使其头数与查询向量匹配。</li></ul></li></ul><h2 id="class-Attention"><a href="#class-Attention" class="headerlink" title="class Attention"></a><code>class Attention</code></h2><p><code>class Attention</code> 实现了 Transformer 中的 <strong>多头注意力机制（Multi-Head Attention）</strong>，它是 Transformer 模型的核心组件之一。以下是该类的详细解释：</p><h4 id="主要功能："><a href="#主要功能：" class="headerlink" title="主要功能："></a><strong>主要功能</strong>：</h4><ol><li><p><strong>多头注意力机制</strong>：</p><ul><li>将输入向量拆分为多个头，每个头独立计算注意力分数。</li><li>通过并行计算，捕捉输入序列中不同位置之间的关系。</li></ul></li><li><p><strong>键值缓存（KV Cache）</strong>：</p><ul><li>在生成任务中，缓存先前的键和值，避免重复计算，提高效率。</li></ul></li><li><p><strong>模型并行化</strong>：</p><ul><li>使用 <code>ColumnParallelLinear</code> 和 <code>RowParallelLinear</code> 实现并行的线性变换，支持多 GPU 计算。</li></ul></li></ol><h4 id="关键组件："><a href="#关键组件：" class="headerlink" title="关键组件："></a><strong>关键组件</strong>：</h4><ol><li><p><strong>线性变换</strong>：</p><ul><li><code>wq</code>、<code>wk</code>、<code>wv</code>：分别对输入进行线性变换，生成查询（Query）、键（Key）和值（Value）向量。</li><li><code>wo</code>：将多头注意力的输出进行线性变换，合并为最终输出。</li></ul></li><li><p><strong>键值缓存</strong>：</p><ul><li><code>cache_k</code> 和 <code>cache_v</code>：用于缓存先前的键和值，形状为 <code>(batch_size, max_seq_len, n_local_kv_heads, head_dim)</code>。</li></ul></li><li><p><strong>旋转位置编码（RoPE）</strong>：</p><ul><li>通过 <code>apply_rotary_emb</code> 将位置信息嵌入到查询和键向量中。</li></ul></li><li><p><strong>注意力分数计算</strong>：</p><ul><li>计算查询和键的点积，除以 <code>sqrt(head_dim)</code> 进行缩放，然后应用 Softmax 得到注意力分数。</li></ul></li><li><p><strong>输出计算</strong>：</p><ul><li>使用注意力分数对值向量进行加权求和，得到多头注意力的输出。</li></ul></li></ol><hr><h3 id="流程图"><a href="#流程图" class="headerlink" title="&#96;流程图"></a><strong>&#96;流程图</strong></h3><pre><code class=" mermaid">graph TD    A[输入 x] --&gt; B[线性变换]    B --&gt; C[生成 Query]    B --&gt; D[生成 Key]    B --&gt; E[生成 Value]    C --&gt; F[应用旋转位置编码]    D --&gt; F    F --&gt; G[更新键值缓存]    G --&gt; H[计算注意力分数]    H --&gt; I[应用 Softmax]    I --&gt; J[加权求和]    J --&gt; K[线性变换]    K --&gt; L[输出]</code></pre><h3 id="详细步骤说明："><a href="#详细步骤说明：" class="headerlink" title="详细步骤说明："></a><strong>详细步骤说明</strong>：</h3><ol><li><p><strong>输入 x</strong>  </p><ul><li>输入是一个批次的嵌入向量，形状为 <code>(batch_size, seq_len, dim)</code>。</li></ul></li><li><p><strong>线性变换</strong>  </p><ul><li>通过 <code>wq</code>、<code>wk</code>、<code>wv</code> 分别对输入进行线性变换，生成查询（Query）、键（Key）和值（Value）向量。</li></ul></li><li><p><strong>生成 Query、Key、Value</strong>  </p><ul><li>查询向量形状为 <code>(batch_size, seq_len, n_local_heads, head_dim)</code>。</li><li>键和值向量形状为 <code>(batch_size, seq_len, n_local_kv_heads, head_dim)</code>。</li></ul></li><li><p><strong>应用旋转位置编码</strong>  </p><ul><li>使用 <code>apply_rotary_emb</code> 将旋转位置编码应用到查询和键向量上。</li></ul></li><li><p><strong>更新键值缓存</strong>  </p><ul><li>将当前的键和值向量缓存到 <code>cache_k</code> 和 <code>cache_v</code> 中。</li></ul></li><li><p><strong>计算注意力分数</strong>  </p><ul><li>计算查询和键的点积，除以 <code>sqrt(head_dim)</code> 进行缩放，得到注意力分数。</li></ul></li><li><p><strong>应用 Softmax</strong>  </p><ul><li>对注意力分数应用 Softmax，得到归一化的注意力权重。</li></ul></li><li><p><strong>加权求和</strong>  </p><ul><li>使用注意力权重对值向量进行加权求和，得到多头注意力的输出。</li></ul></li><li><p><strong>线性变换</strong>  </p><ul><li>通过 <code>wo</code> 对多头注意力的输出进行线性变换，合并为最终输出。</li></ul></li><li><p><strong>输出</strong>  </p><ul><li>返回最终的输出，形状为 <code>(batch_size, seq_len, dim)</code>。</li></ul></li></ol><hr><h3 id="代码实现的关键点："><a href="#代码实现的关键点：" class="headerlink" title="代码实现的关键点："></a><strong>代码实现的关键点</strong>：</h3><ol><li><p><strong>并行化</strong>：</p><ul><li>使用 <code>ColumnParallelLinear</code> 和 <code>RowParallelLinear</code> 实现并行的线性变换，支持多 GPU 计算。</li></ul></li><li><p><strong>键值缓存</strong>：</p><ul><li>在生成任务中，缓存先前的键和值，避免重复计算，提高效率。</li></ul></li><li><p><strong>旋转位置编码</strong>：</p><ul><li>通过 <code>apply_rotary_emb</code> 将位置信息嵌入到查询和键向量中，帮助模型捕捉序列中的位置关系。</li></ul></li><li><p><strong>注意力分数计算</strong>：</p><ul><li>使用点积计算注意力分数，并通过 Softmax 进行归一化。</li></ul></li><li><p><strong>输出计算</strong>：</p><ul><li>使用注意力权重对值向量进行加权求和，得到多头注意力的输出。</li></ul></li></ol><hr><h3 id="总结：-2"><a href="#总结：-2" class="headerlink" title="总结："></a><strong>总结</strong>：</h3><p><code>class Attention</code> 实现了 Transformer 中的多头注意力机制，通过并行化、键值缓存和旋转位置编码等技术，高效地捕捉输入序列中的关系。</p><h2 id="FeedForward"><a href="#FeedForward" class="headerlink" title="FeedForward"></a><code>FeedForward</code></h2><pre><code class=" mermaid">graph TD    A[输入 x] --&gt; B[线性变换 W1]    B --&gt; C[激活函数 SiLU]    C --&gt; D[线性变换 W3]    D --&gt; E[逐元素乘法]    E --&gt; F[线性变换 W2]    F --&gt; G[输出]</code></pre><h3 id="详细步骤说明：-1"><a href="#详细步骤说明：-1" class="headerlink" title="详细步骤说明："></a><strong>详细步骤说明</strong>：</h3><ol><li><p><strong>输入 x</strong>  </p><ul><li>输入是一个批次的向量，形状为 <code>(batch_size, seq_len, dim)</code>。</li></ul></li><li><p><strong>线性变换 W1</strong>  </p><ul><li>通过 <code>w1</code> 对输入进行线性变换，生成中间向量，形状为 <code>(batch_size, seq_len, hidden_dim)</code>。</li></ul></li><li><p><strong>激活函数 SiLU</strong>  </p><ul><li><p>对线性变换后的结果应用 SiLU（Sigmoid Linear Unit）激活函数，公式为：  </p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241230114851133.png"></p></li></ul></li><li><p><strong>线性变换 W3</strong>  </p><ul><li>通过 <code>w3</code> 对输入进行另一个线性变换，生成中间向量，形状为 <code>(batch_size, seq_len, hidden_dim)</code>。</li></ul></li><li><p><strong>逐元素乘法</strong>  </p><ul><li>将 <code>SiLU(W1(x))</code> 和 <code>W3(x)</code> 进行逐元素乘法，生成加权后的中间向量。</li></ul></li><li><p><strong>线性变换 W2</strong>  </p><ul><li>通过 <code>w2</code> 对加权后的中间向量进行线性变换，生成最终输出，形状为 <code>(batch_size, seq_len, dim)</code>。</li></ul></li><li><p><strong>输出</strong>  </p><ul><li>返回最终的输出，作为前馈网络的结果。</li></ul></li></ol><hr><h3 id="代码实现的关键点：-1"><a href="#代码实现的关键点：-1" class="headerlink" title="代码实现的关键点："></a><strong>代码实现的关键点</strong>：</h3><ol><li><p><strong>并行化</strong>：</p><ul><li>使用 <code>ColumnParallelLinear</code> 和 <code>RowParallelLinear</code> 实现并行的线性变换，支持多 GPU 计算。</li></ul></li><li><p><strong>激活函数</strong>：</p><ul><li>使用 SiLU 激活函数，结合了 Sigmoid 和线性变换的优点，增强了模型的非线性表达能力。</li></ul></li><li><p><strong>逐元素乘法</strong>：</p><ul><li>将两个线性变换的结果进行逐元素乘法，生成加权后的中间向量。</li></ul></li><li><p><strong>输出计算</strong>：</p><ul><li>通过 <code>w2</code> 对加权后的中间向量进行线性变换，生成最终输出。</li></ul></li></ol><hr><h3 id="总结：-3"><a href="#总结：-3" class="headerlink" title="总结："></a><strong>总结</strong>：</h3><p><code>FeedForward</code> 类实现了 Transformer 中的前馈网络，通过线性变换、激活函数和逐元素乘法等技术，增强了模型的非线性表达能力。</p><h2 id="TransformerBlock"><a href="#TransformerBlock" class="headerlink" title="TransformerBlock"></a><code>TransformerBlock</code></h2><pre><code class=" mermaid">graph TD    A[输入 x] --&gt; B[RMSNorm]    B --&gt; C[Attention]    C --&gt; D[Add &amp; Norm]    D --&gt; E[FeedForward]    E --&gt; F[Add &amp; Norm]    F --&gt; G[输出]</code></pre><h3 id="详细步骤说明：-2"><a href="#详细步骤说明：-2" class="headerlink" title="详细步骤说明："></a><strong>详细步骤说明</strong>：</h3><ol><li><p><strong>输入 x</strong>  </p><ul><li>输入是一个批次的向量，形状为 <code>(batch_size, seq_len, dim)</code>。</li></ul></li><li><p><strong>RMSNorm</strong>  </p><ul><li>对输入进行 RMSNorm 归一化，公式为：<br>[<br>\text{RMSNorm}(x) &#x3D; \frac{x}{\sqrt{\text{mean}(x^2) + \epsilon}} \cdot \gamma<br>]<br>其中，(\gamma) 是可学习的缩放参数，(\epsilon) 是防止除零的小常数。</li></ul></li><li><p><strong>Attention</strong>  </p><ul><li>将归一化后的输入传递给 <code>Attention</code> 模块，计算多头注意力机制的输出。</li></ul></li><li><p><strong>Add &amp; Norm</strong>  </p><ul><li>将注意力输出与输入进行残差连接，然后再次应用 RMSNorm 归一化。</li></ul></li><li><p><strong>FeedForward</strong>  </p><ul><li>将归一化后的结果传递给 <code>FeedForward</code> 模块，计算前馈网络的输出。</li></ul></li><li><p><strong>Add &amp; Norm</strong>  </p><ul><li>将前馈网络输出与上一层的输出进行残差连接，然后再次应用 RMSNorm 归一化。</li></ul></li><li><p><strong>输出</strong>  </p><ul><li>返回最终的输出，作为 Transformer 块的结果。</li></ul></li></ol><hr><h3 id="代码实现的关键点：-2"><a href="#代码实现的关键点：-2" class="headerlink" title="代码实现的关键点："></a><strong>代码实现的关键点</strong>：</h3><ol><li><p><strong>残差连接</strong>：</p><ul><li>在注意力机制和前馈网络之后，分别使用残差连接，将输入与输出相加，缓解梯度消失问题。</li></ul></li><li><p><strong>归一化</strong>：</p><ul><li>使用 RMSNorm 对输入和输出进行归一化，稳定训练过程。</li></ul></li><li><p><strong>注意力机制</strong>：</p><ul><li>通过 <code>Attention</code> 模块计算多头注意力机制的输出，捕捉输入序列中的关系。</li></ul></li><li><p><strong>前馈网络</strong>：</p><ul><li>通过 <code>FeedForward</code> 模块增强模型的非线性表达能力。</li></ul></li></ol><hr><h3 id="总结：-4"><a href="#总结：-4" class="headerlink" title="总结："></a><strong>总结</strong>：</h3><p><code>TransformerBlock</code> 类实现了 Transformer 中的一个完整块，包括注意力机制、前馈网络、残差连接和归一化操作。</p><h2 id="class-Transformer"><a href="#class-Transformer" class="headerlink" title="class Transformer"></a><code>class Transformer</code></h2><pre><code class=" mermaid">graph TD    A[输入 tokens] --&gt; B[Token Embedding]    B --&gt; C[添加位置编码 freqs_cis]    C --&gt; D[初始化 mask]    D --&gt; E[进入 Transformer 层]    E --&gt; F[Transformer Block 1]    E --&gt; G[Transformer Block 2]    E --&gt; H[...]    E --&gt; I[Transformer Block N]    F --&gt; J[RMSNorm]    G --&gt; J    H --&gt; J    I --&gt; J    J --&gt; K[输出线性变换]    K --&gt; L[输出 logits]    subgraph Transformer Block        direction TB        M[输入] --&gt; N[RMSNorm]        N --&gt; O[Attention]        O --&gt; P[Add &amp; Norm]        P --&gt; Q[FeedForward]        Q --&gt; R[Add &amp; Norm]        R --&gt; S[输出]    end    F --&gt; M    G --&gt; M    I --&gt; M</code></pre><h3 id="详细步骤说明：-3"><a href="#详细步骤说明：-3" class="headerlink" title="详细步骤说明："></a><strong>详细步骤说明</strong>：</h3><h4 id="整体流程："><a href="#整体流程：" class="headerlink" title="整体流程："></a><strong>整体流程</strong>：</h4><ol><li><p><strong>输入 tokens</strong>  </p><ul><li>输入是一个批次的 token IDs，形状为 <code>(batch_size, seq_len)</code>。</li></ul></li><li><p><strong>Token Embedding</strong>  </p><ul><li>通过 <code>tok_embeddings</code> 将 token IDs 转换为嵌入向量，形状为 <code>(batch_size, seq_len, dim)</code>。</li></ul></li><li><p><strong>添加位置编码 freqs_cis</strong>  </p><ul><li>使用预计算的 <code>freqs_cis</code> 为嵌入向量添加旋转位置编码，帮助模型捕捉序列中的位置信息。</li></ul></li><li><p><strong>初始化 mask</strong>  </p><ul><li>根据 <code>seq_len</code> 和 <code>start_pos</code> 生成注意力掩码 <code>mask</code>，用于防止模型看到未来的 token。</li></ul></li><li><p><strong>进入 Transformer 层</strong>  </p><ul><li>嵌入向量和位置编码进入多层 Transformer 块进行处理。</li></ul></li><li><p><strong>Transformer Block 1 到 N</strong>  </p><ul><li>每个 Transformer 块内部执行子图中的流程。</li></ul></li><li><p><strong>RMSNorm</strong>  </p><ul><li>在所有 Transformer 块处理完成后，对最终输出应用 RMSNorm 进行归一化。</li></ul></li><li><p><strong>输出线性变换</strong>  </p><ul><li>通过 <code>output</code> 线性层将归一化后的输出映射到词汇表空间，形状为 <code>(batch_size, seq_len, vocab_size)</code>。</li></ul></li><li><p><strong>输出 logits</strong>  </p><ul><li>返回最终的 logits，表示每个 token 的概率分布。</li></ul></li></ol><h4 id="Transformer-Block-子流程："><a href="#Transformer-Block-子流程：" class="headerlink" title="Transformer Block 子流程："></a><strong>Transformer Block 子流程</strong>：</h4><ol><li><p><strong>输入</strong>  </p><ul><li>接收来自上一层的输入。</li></ul></li><li><p><strong>RMSNorm</strong>  </p><ul><li>对输入进行归一化。</li></ul></li><li><p><strong>Attention</strong>  </p><ul><li>应用多头注意力机制，生成注意力输出。</li></ul></li><li><p><strong>Add &amp; Norm</strong>  </p><ul><li>将注意力输出与输入进行残差连接，并再次应用 RMSNorm 进行归一化。</li></ul></li><li><p><strong>FeedForward</strong>  </p><ul><li>应用前馈网络，生成前馈输出。</li></ul></li><li><p><strong>Add &amp; Norm</strong>  </p><ul><li>将前馈输出与上一层的输出进行残差连接，并再次应用 RMSNorm 进行归一化。</li></ul></li><li><p><strong>输出</strong>  </p><ul><li>返回当前 Transformer 块的输出，作为下一层的输入。</li></ul></li></ol><hr><h3 id="代码实现的关键点：-3"><a href="#代码实现的关键点：-3" class="headerlink" title="代码实现的关键点："></a><strong>代码实现的关键点</strong>：</h3><ol><li><p><strong>嵌入和位置编码</strong>：</p><ul><li>使用 <code>tok_embeddings</code> 将 token IDs 转换为嵌入向量，并通过 <code>freqs_cis</code> 添加位置信息。</li></ul></li><li><p><strong>注意力掩码</strong>：</p><ul><li>生成注意力掩码 <code>mask</code>，防止模型看到未来的 token。</li></ul></li><li><p><strong>多层 Transformer 块</strong>：</p><ul><li>通过多个 Transformer 块处理输入，每个块包含注意力机制、前馈网络、残差连接和归一化操作。</li></ul></li><li><p><strong>输出生成</strong>：</p><ul><li>对最终输出进行归一化和线性变换，生成 logits。</li></ul></li></ol><hr><h3 id="总结：-5"><a href="#总结：-5" class="headerlink" title="总结："></a><strong>总结</strong>：</h3><p><code>class Transformer</code> 实现了完整的 Transformer 模型，包括嵌入、位置编码、多层 Transformer 块的处理以及最终的输出生成。</p><h2 id="示例解析"><a href="#示例解析" class="headerlink" title="示例解析"></a>示例解析</h2><h3 id="示例输入"><a href="#示例输入" class="headerlink" title="示例输入"></a><strong>示例输入</strong></h3><p>假设我们有以下输入：</p><ul><li><strong>输入 tokens</strong>: <code>[[1, 2, 3]]</code>，形状为 <code>(batch_size=1, seq_len=3)</code>。</li><li><strong>模型参数</strong>:<ul><li><code>dim=4</code>（模型维度）。</li><li><code>n_heads=2</code>（注意力头数）。</li><li><code>vocab_size=10</code>（词汇表大小）。</li><li><code>max_seq_len=8</code>（最大序列长度）。</li></ul></li></ul><hr><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a><strong>执行流程</strong></h3><h4 id="1-Token-Embedding"><a href="#1-Token-Embedding" class="headerlink" title="1. Token Embedding"></a>1. <strong>Token Embedding</strong></h4><ul><li><strong>输入</strong>: <code>tokens = [[1, 2, 3]]</code>，形状为 <code>(1, 3)</code>。</li><li><strong>操作</strong>: 将 token IDs 转换为嵌入向量。</li><li><strong>输出</strong>: 嵌入向量，形状为 <code>(1, 3, 4)</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设嵌入矩阵为：</span><br>embedding_matrix = [<br>    [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>],  <span class="hljs-comment"># token 1</span><br>    [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>],  <span class="hljs-comment"># token 2</span><br>    [<span class="hljs-number">0.9</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">1.2</span>],  <span class="hljs-comment"># token 3</span><br>]<br><span class="hljs-comment"># 输出：</span><br>h = [<br>    [[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>],  <span class="hljs-comment"># token 1</span><br>     [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>],  <span class="hljs-comment"># token 2</span><br>     [<span class="hljs-number">0.9</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">1.2</span>]]  <span class="hljs-comment"># token 3</span><br>]<br></code></pre></td></tr></table></figure><hr><h4 id="2-添加位置编码"><a href="#2-添加位置编码" class="headerlink" title="2. 添加位置编码"></a>2. <strong>添加位置编码</strong></h4><ul><li><strong>输入</strong>: 嵌入向量 <code>h</code>，形状为 <code>(1, 3, 4)</code>。</li><li><strong>操作</strong>: 使用 <code>freqs_cis</code> 添加旋转位置编码。</li><li><strong>输出</strong>: 添加位置编码后的向量，形状为 <code>(1, 3, 4)</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设 freqs_cis 为：</span><br>freqs_cis = [<br>    [<span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>],  <span class="hljs-comment"># 位置 1</span><br>    [<span class="hljs-number">1.1</span>, <span class="hljs-number">1.1</span>],  <span class="hljs-comment"># 位置 2</span><br>    [<span class="hljs-number">1.2</span>, <span class="hljs-number">1.2</span>],  <span class="hljs-comment"># 位置 3</span><br>]<br><span class="hljs-comment"># 输出：</span><br>h_with_pos = [<br>    [[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>],  <span class="hljs-comment"># token 1</span><br>     [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">1.1</span>],  <span class="hljs-comment"># token 2</span><br>     [<span class="hljs-number">0.9</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.2</span>, <span class="hljs-number">1.2</span>]]  <span class="hljs-comment"># token 3</span><br>]<br></code></pre></td></tr></table></figure><hr><h4 id="3-初始化-mask"><a href="#3-初始化-mask" class="headerlink" title="3. 初始化 mask"></a>3. <strong>初始化 mask</strong></h4><ul><li><strong>输入</strong>: 序列长度 <code>seq_len=3</code>。</li><li><strong>操作</strong>: 生成注意力掩码，防止模型看到未来的 token。</li><li><strong>输出</strong>: 注意力掩码，形状为 <code>(3, 3)</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输出：</span><br>mask = [<br>    [<span class="hljs-number">0</span>, -inf, -inf],  <span class="hljs-comment"># 位置 1</span><br>    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, -inf],     <span class="hljs-comment"># 位置 2</span><br>    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]         <span class="hljs-comment"># 位置 3</span><br>]<br></code></pre></td></tr></table></figure><hr><h4 id="4-进入-Transformer-层"><a href="#4-进入-Transformer-层" class="headerlink" title="4. 进入 Transformer 层"></a>4. <strong>进入 Transformer 层</strong></h4><ul><li><strong>输入</strong>: 添加位置编码后的向量 <code>h_with_pos</code>，形状为 <code>(1, 3, 4)</code>。</li><li><strong>操作</strong>: 通过多层 Transformer 块处理输入。</li></ul><hr><h4 id="5-Transformer-Block-1"><a href="#5-Transformer-Block-1" class="headerlink" title="5. Transformer Block 1"></a>5. <strong>Transformer Block 1</strong></h4><ul><li><strong>输入</strong>: <code>h_with_pos</code>，形状为 <code>(1, 3, 4)</code>。</li><li><strong>操作</strong>:<ol><li><strong>RMSNorm</strong>: 对输入进行归一化。</li><li><strong>Attention</strong>: 计算多头注意力机制。</li><li><strong>Add &amp; Norm</strong>: 残差连接和归一化。</li><li><strong>FeedForward</strong>: 计算前馈网络。</li><li><strong>Add &amp; Norm</strong>: 残差连接和归一化。</li></ol></li><li><strong>输出</strong>: Transformer 块的输出，形状为 <code>(1, 3, 4)</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设输出为：</span><br>h_block1 = [<br>    [[<span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">1.1</span>],  <span class="hljs-comment"># token 1</span><br>     [<span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">1.2</span>, <span class="hljs-number">1.2</span>],  <span class="hljs-comment"># token 2</span><br>     [<span class="hljs-number">1.0</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">1.3</span>, <span class="hljs-number">1.3</span>]]  <span class="hljs-comment"># token 3</span><br>]<br></code></pre></td></tr></table></figure><hr><h4 id="6-Transformer-Block-N"><a href="#6-Transformer-Block-N" class="headerlink" title="6. Transformer Block N"></a>6. <strong>Transformer Block N</strong></h4><ul><li><strong>输入</strong>: 上一层的输出，形状为 <code>(1, 3, 4)</code>。</li><li><strong>操作</strong>: 重复 Transformer 块的处理。</li><li><strong>输出</strong>: 最后一层 Transformer 块的输出，形状为 <code>(1, 3, 4)</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设输出为：</span><br>h_blockN = [<br>    [[<span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">1.2</span>, <span class="hljs-number">1.2</span>],  <span class="hljs-comment"># token 1</span><br>     [<span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">1.3</span>, <span class="hljs-number">1.3</span>],  <span class="hljs-comment"># token 2</span><br>     [<span class="hljs-number">1.1</span>, <span class="hljs-number">1.2</span>, <span class="hljs-number">1.4</span>, <span class="hljs-number">1.4</span>]]  <span class="hljs-comment"># token 3</span><br>]<br></code></pre></td></tr></table></figure><hr><h4 id="7-RMSNorm"><a href="#7-RMSNorm" class="headerlink" title="7. RMSNorm"></a>7. <strong>RMSNorm</strong></h4><ul><li><strong>输入</strong>: 最后一层 Transformer 块的输出，形状为 <code>(1, 3, 4)</code>。</li><li><strong>操作</strong>: 对输出进行归一化。</li><li><strong>输出</strong>: 归一化后的输出，形状为 <code>(1, 3, 4)</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设输出为：</span><br>h_norm = [<br>    [[<span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">1.3</span>, <span class="hljs-number">1.3</span>],  <span class="hljs-comment"># token 1</span><br>     [<span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">1.4</span>, <span class="hljs-number">1.4</span>],  <span class="hljs-comment"># token 2</span><br>     [<span class="hljs-number">1.2</span>, <span class="hljs-number">1.3</span>, <span class="hljs-number">1.5</span>, <span class="hljs-number">1.5</span>]]  <span class="hljs-comment"># token 3</span><br>]<br></code></pre></td></tr></table></figure><hr><h4 id="8-输出线性变换"><a href="#8-输出线性变换" class="headerlink" title="8. 输出线性变换"></a>8. <strong>输出线性变换</strong></h4><ul><li><strong>输入</strong>: 归一化后的输出，形状为 <code>(1, 3, 4)</code>。</li><li><strong>操作</strong>: 通过线性层将输出映射到词汇表空间。</li><li><strong>输出</strong>: logits，形状为 <code>(1, 3, vocab_size=10)</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设输出为：</span><br>logits = [<br>    [[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">1.0</span>],  <span class="hljs-comment"># token 1</span><br>     [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.1</span>],  <span class="hljs-comment"># token 2</span><br>     [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">1.2</span>]]  <span class="hljs-comment"># token 3</span><br>]<br></code></pre></td></tr></table></figure><hr><h3 id="最终输出"><a href="#最终输出" class="headerlink" title="最终输出"></a><strong>最终输出</strong></h3><ul><li><strong>输出</strong>: logits，形状为 <code>(1, 3, 10)</code>。</li><li><strong>解释</strong>: 每个 token 的输出是一个长度为 <code>vocab_size=10</code> 的向量，表示每个 token 的概率分布。</li></ul><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul><li><strong>输入</strong>: <code>tokens = [[1, 2, 3]]</code>，形状为 <code>(1, 3)</code>。</li><li><strong>输出</strong>: logits，形状为 <code>(1, 3, 10)</code>。</li><li><strong>中间步骤</strong>:<ol><li>Token Embedding：<code>(1, 3) -&gt; (1, 3, 4)</code>。</li><li>添加位置编码：<code>(1, 3, 4) -&gt; (1, 3, 4)</code>。</li><li>初始化 mask：<code>(3, 3)</code>。</li><li>多层 Transformer 块：<code>(1, 3, 4) -&gt; (1, 3, 4)</code>。</li><li>RMSNorm：<code>(1, 3, 4) -&gt; (1, 3, 4)</code>。</li><li>输出线性变换：<code>(1, 3, 4) -&gt; (1, 3, 10)</code>。</li></ol></li></ul><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>nlp</category>
      
      <category>llm</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>llm</tag>
      
      <tag>llama</tag>
      
      <tag>源码解析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>llama3源码解析-04：generation.py模块解析</title>
    <link href="/2024/12/30/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/nlp/llm/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9Allama3%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-04%EF%BC%9Ageneration.py%E6%A8%A1%E5%9D%97%E8%A7%A3%E6%9E%90/"/>
    <url>/2024/12/30/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/nlp/llm/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9Allama3%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-04%EF%BC%9Ageneration.py%E6%A8%A1%E5%9D%97%E8%A7%A3%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/Llama3_Repo.jpeg" alt="llama"></p><h2 id="整体"><a href="#整体" class="headerlink" title="整体"></a>整体</h2><p><code>generation.py</code> 模块是 Llama 3 模型的核心生成模块，负责实现文本生成功能，包括文本补全和对话生成。</p><hr><h3 id="1-核心功能"><a href="#1-核心功能" class="headerlink" title="1. 核心功能"></a><strong>1. 核心功能</strong></h3><ul><li><strong>文本生成</strong>：根据输入的提示（prompt）生成文本。</li><li><strong>文本补全</strong>：对给定的文本提示进行补全。</li><li><strong>对话生成</strong>：根据对话历史生成助理的回复。</li><li><strong>生成控制</strong>：支持通过温度（temperature）、top-p 采样（nucleus sampling）等参数控制生成过程。</li></ul><hr><h3 id="2-主要类和方法"><a href="#2-主要类和方法" class="headerlink" title="2. 主要类和方法"></a><strong>2. 主要类和方法</strong></h3><h4 id="class-Llama"><a href="#class-Llama" class="headerlink" title="class Llama"></a><strong><code>class Llama</code></strong></h4><ul><li><strong><code>build</code> 方法</strong>：初始化并加载 Llama 模型和分词器。</li><li><strong><code>generate</code> 方法</strong>：核心生成方法，根据输入的 tokenized prompts 生成文本。</li><li><strong><code>text_completion</code> 方法</strong>：对文本提示进行补全。</li><li><strong><code>chat_completion</code> 方法</strong>：根据对话历史生成助理的回复。</li></ul><h4 id="class-CompletionPrediction"><a href="#class-CompletionPrediction" class="headerlink" title="class CompletionPrediction"></a><strong><code>class CompletionPrediction</code></strong></h4><ul><li>用于表示文本补全的生成结果，包含生成的文本、token 列表和 logprobs（可选）。</li></ul><h4 id="class-ChatPrediction"><a href="#class-ChatPrediction" class="headerlink" title="class ChatPrediction"></a><strong><code>class ChatPrediction</code></strong></h4><ul><li>用于表示对话生成的生成结果，包含助理的回复、token 列表和 logprobs（可选）。</li></ul><hr><h3 id="3-核心方法详解"><a href="#3-核心方法详解" class="headerlink" title="3. 核心方法详解"></a><strong>3. 核心方法详解</strong></h3><h4 id="generate-方法"><a href="#generate-方法" class="headerlink" title="generate 方法"></a><strong><code>generate</code> 方法</strong></h4><ul><li><strong>功能</strong>：根据输入的 tokenized prompts 生成文本。</li><li><strong>参数</strong>：<ul><li><code>prompt_tokens</code>：tokenized prompts，形状为 <code>(batch_size, seq_len)</code>。</li><li><code>max_gen_len</code>：生成文本的最大长度。</li><li><code>temperature</code>：控制生成随机性的温度参数。</li><li><code>top_p</code>：top-p 采样的概率阈值。</li><li><code>logprobs</code>：是否计算 token 的 log 概率。</li><li><code>echo</code>：是否在生成结果中包含输入提示。</li></ul></li><li><strong>返回值</strong>：生成的 token 序列和 logprobs（可选）。</li></ul><h4 id="text-completion-方法"><a href="#text-completion-方法" class="headerlink" title="text_completion 方法"></a><strong><code>text_completion</code> 方法</strong></h4><ul><li><strong>功能</strong>：对文本提示进行补全。</li><li><strong>参数</strong>：<ul><li><code>prompts</code>：文本提示列表。</li><li><code>temperature</code>：控制生成随机性的温度参数。</li><li><code>top_p</code>：top-p 采样的概率阈值。</li><li><code>max_gen_len</code>：生成文本的最大长度。</li><li><code>logprobs</code>：是否计算 token 的 log 概率。</li><li><code>echo</code>：是否在生成结果中包含输入提示。</li></ul></li><li><strong>返回值</strong>：文本补全的生成结果列表。</li></ul><h4 id="chat-completion-方法"><a href="#chat-completion-方法" class="headerlink" title="chat_completion 方法"></a><strong><code>chat_completion</code> 方法</strong></h4><ul><li><strong>功能</strong>：根据对话历史生成助理的回复。</li><li><strong>参数</strong>：<ul><li><code>dialogs</code>：对话历史列表。</li><li><code>temperature</code>：控制生成随机性的温度参数。</li><li><code>top_p</code>：top-p 采样的概率阈值。</li><li><code>max_gen_len</code>：生成文本的最大长度。</li><li><code>logprobs</code>：是否计算 token 的 log 概率。</li></ul></li><li><strong>返回值</strong>：对话生成的生成结果列表。</li></ul><hr><h3 id="4-生成控制参数"><a href="#4-生成控制参数" class="headerlink" title="4. 生成控制参数"></a><strong>4. 生成控制参数</strong></h3><ul><li><strong>温度（temperature）</strong>：<ul><li>控制生成随机性的参数。</li><li>值越大，生成结果越随机；值越小，生成结果越确定。</li></ul></li><li><strong>top-p 采样（nucleus sampling）</strong>：<ul><li>从概率累积值超过 <code>top_p</code> 的最小 token 集合中采样。</li><li>用于控制生成结果的多样性和质量。</li></ul></li><li><strong>logprobs</strong>：<ul><li>是否计算生成 token 的 log 概率。</li><li>用于分析生成结果的置信度。</li></ul></li></ul><hr><h3 id="5-生成流程"><a href="#5-生成流程" class="headerlink" title="5. 生成流程"></a><strong>5. 生成流程</strong></h3><ol><li><strong>输入处理</strong>：<ul><li>将输入提示或对话历史转换为 tokenized prompts。</li></ul></li><li><strong>生成文本</strong>：<ul><li>使用 <code>generate</code> 方法生成 token 序列。</li><li>通过温度、top-p 采样等参数控制生成过程。</li></ul></li><li><strong>输出处理</strong>：<ul><li>将生成的 token 序列解码为文本。</li><li>返回生成结果，包含生成的文本、token 列表和 logprobs（可选）。</li></ul></li></ol><hr><h3 id="6-示例"><a href="#6-示例" class="headerlink" title="6. 示例"></a><strong>6. 示例</strong></h3><h4 id="文本补全"><a href="#文本补全" class="headerlink" title="文本补全"></a><strong>文本补全</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">prompts = [<span class="hljs-string">&quot;Once upon a time&quot;</span>]<br>results = llama.text_completion(prompts, max_gen_len=<span class="hljs-number">50</span>)<br><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:<br>    <span class="hljs-built_in">print</span>(result[<span class="hljs-string">&quot;generation&quot;</span>])<br></code></pre></td></tr></table></figure><h4 id="对话生成"><a href="#对话生成" class="headerlink" title="对话生成"></a><strong>对话生成</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">dialogs = [<br>    [&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;What is the capital of France?&quot;</span>&#125;]<br>]<br>results = llama.chat_completion(dialogs, max_gen_len=<span class="hljs-number">50</span>)<br><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:<br>    <span class="hljs-built_in">print</span>(result[<span class="hljs-string">&quot;generation&quot;</span>][<span class="hljs-string">&quot;content&quot;</span>])<br></code></pre></td></tr></table></figure><hr><h3 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a><strong>7. 总结</strong></h3><p><code>generation.py</code> 模块是 Llama 3 模型的核心生成模块，提供了文本补全和对话生成功能。通过温度、top-p 采样等参数，用户可以灵活控制生成过程。</p><h2 id="build-方法"><a href="#build-方法" class="headerlink" title="build 方法"></a><code>build</code> 方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@staticmethod</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build</span>(<span class="hljs-params"></span><br><span class="hljs-params">    ckpt_dir: <span class="hljs-built_in">str</span>,</span><br><span class="hljs-params">    tokenizer_path: <span class="hljs-built_in">str</span>,</span><br><span class="hljs-params">    max_seq_len: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    max_batch_size: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    model_parallel_size: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    seed: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-string">&quot;Llama&quot;</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    初始化并加载 Llama 模型。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        ckpt_dir (str): 包含模型检查点文件的目录路径。</span><br><span class="hljs-string">        tokenizer_path (str): 分词器文件的路径。</span><br><span class="hljs-string">        max_seq_len (int): 输入序列的最大长度。</span><br><span class="hljs-string">        max_batch_size (int): 推理时的最大批次大小。</span><br><span class="hljs-string">        model_parallel_size (Optional[int], optional): 模型并行的大小（GPU 数量）。如果未提供，则从环境变量中获取。默认为 None。</span><br><span class="hljs-string">        seed (int, optional): 随机种子，确保所有进程的随机性一致。默认为 1。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        Llama: 加载了模型和分词器的 Llama 实例。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    异常:</span><br><span class="hljs-string">        AssertionError: 如果检查点目录中没有检查点文件，或者模型并行大小与检查点文件数量不匹配。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    注意:</span><br><span class="hljs-string">        该方法会初始化分布式进程组，设置设备为 CUDA，并加载预训练模型和分词器。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 检查 max_seq_len 是否在有效范围内</span><br>    <span class="hljs-keyword">assert</span> <span class="hljs-number">1</span> &lt;= max_seq_len &lt;= <span class="hljs-number">8192</span>, <span class="hljs-string">f&quot;max_seq_len must be between 1 and 8192, got <span class="hljs-subst">&#123;max_seq_len&#125;</span>.&quot;</span><br>    <span class="hljs-comment"># 检查检查点目录是否存在</span><br>    <span class="hljs-keyword">assert</span> os.path.isdir(ckpt_dir), <span class="hljs-string">f&quot;Checkpoint directory &#x27;<span class="hljs-subst">&#123;ckpt_dir&#125;</span>&#x27; does not exist.&quot;</span><br>    <span class="hljs-comment"># 检查分词器文件是否存在</span><br>    <span class="hljs-keyword">assert</span> os.path.isfile(tokenizer_path), <span class="hljs-string">f&quot;Tokenizer file &#x27;<span class="hljs-subst">&#123;tokenizer_path&#125;</span>&#x27; does not exist.&quot;</span><br><br>    <span class="hljs-comment"># 如果分布式进程组未初始化，则初始化</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed.is_initialized():<br>        torch.distributed.init_process_group(<span class="hljs-string">&quot;nccl&quot;</span>)<br>    <span class="hljs-comment"># 如果模型并行未初始化，则初始化</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> model_parallel_is_initialized():<br>        <span class="hljs-comment"># 如果未提供 model_parallel_size，则从环境变量中获取</span><br>        <span class="hljs-keyword">if</span> model_parallel_size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            model_parallel_size = <span class="hljs-built_in">int</span>(os.environ.get(<span class="hljs-string">&quot;WORLD_SIZE&quot;</span>, <span class="hljs-number">1</span>))<br>        initialize_model_parallel(model_parallel_size)<br><br>    <span class="hljs-comment"># 获取当前进程的本地 rank</span><br>    local_rank = <span class="hljs-built_in">int</span>(os.environ.get(<span class="hljs-string">&quot;LOCAL_RANK&quot;</span>, <span class="hljs-number">0</span>))<br>    <span class="hljs-comment"># 设置当前进程使用的 GPU 设备</span><br>    torch.cuda.set_device(local_rank)<br><br>    <span class="hljs-comment"># 设置随机种子，确保所有进程的随机性一致</span><br>    torch.manual_seed(seed)<br><br>    <span class="hljs-comment"># 如果当前进程不是主进程（rank &gt; 0），则关闭标准输出</span><br>    <span class="hljs-keyword">if</span> local_rank &gt; <span class="hljs-number">0</span>:<br>        sys.stdout = <span class="hljs-built_in">open</span>(os.devnull, <span class="hljs-string">&quot;w&quot;</span>)<br><br>    <span class="hljs-comment"># 记录加载模型的开始时间</span><br>    start_time = time.time()<br>    <span class="hljs-comment"># 获取检查点目录中的所有 .pth 文件，并按文件名排序</span><br>    checkpoints = <span class="hljs-built_in">sorted</span>(Path(ckpt_dir).glob(<span class="hljs-string">&quot;*.pth&quot;</span>))<br>    <span class="hljs-comment"># 检查检查点目录中是否有检查点文件</span><br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(checkpoints) &gt; <span class="hljs-number">0</span>, <span class="hljs-string">f&quot;no checkpoint files found in <span class="hljs-subst">&#123;ckpt_dir&#125;</span>&quot;</span><br>    <span class="hljs-comment"># 检查模型并行大小是否与检查点文件数量匹配</span><br>    <span class="hljs-keyword">assert</span> model_parallel_size == <span class="hljs-built_in">len</span>(<br>        checkpoints<br>    ), <span class="hljs-string">f&quot;Loading a checkpoint for MP=<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(checkpoints)&#125;</span> but world size is <span class="hljs-subst">&#123;model_parallel_size&#125;</span>&quot;</span><br>    <span class="hljs-comment"># 获取当前进程对应的检查点文件</span><br>    ckpt_path = checkpoints[get_model_parallel_rank()]<br>    <span class="hljs-comment"># 加载检查点文件到 CPU</span><br>    checkpoint = torch.load(ckpt_path, map_location=<span class="hljs-string">&quot;cpu&quot;</span>)<br>    <span class="hljs-comment"># 加载模型参数文件</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(Path(ckpt_dir) / <span class="hljs-string">&quot;params.json&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        params = json.loads(f.read())<br><br>    <span class="hljs-comment"># 初始化模型参数</span><br>    model_args: ModelArgs = ModelArgs(<br>        max_seq_len=max_seq_len,<br>        max_batch_size=max_batch_size,<br>        **params,<br>    )<br>    <span class="hljs-comment"># 初始化分词器</span><br>    tokenizer = Tokenizer(model_path=tokenizer_path)<br>    <span class="hljs-comment"># 检查模型词汇表大小是否与分词器词汇表大小匹配</span><br>    <span class="hljs-keyword">assert</span> model_args.vocab_size == tokenizer.n_words<br>    <span class="hljs-comment"># 如果当前设备支持 bfloat16，则设置默认张量类型为 bfloat16，否则设置为 float16</span><br>    <span class="hljs-keyword">if</span> torch.cuda.is_bf16_supported():<br>        torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)<br>    <span class="hljs-keyword">else</span>:<br>        torch.set_default_tensor_type(torch.cuda.HalfTensor)<br>    <span class="hljs-comment"># 初始化 Transformer 模型</span><br>    model = Transformer(model_args)<br>    <span class="hljs-comment"># 加载模型权重</span><br>    model.load_state_dict(checkpoint, strict=<span class="hljs-literal">False</span>)<br>    <span class="hljs-comment"># 打印模型加载时间</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Loaded in <span class="hljs-subst">&#123;time.time() - start_time:<span class="hljs-number">.2</span>f&#125;</span> seconds&quot;</span>)<br><br>    <span class="hljs-comment"># 返回 Llama 实例，包含加载的模型和分词器</span><br>    <span class="hljs-keyword">return</span> Llama(model, tokenizer)<br></code></pre></td></tr></table></figure><hr><h3 id="详细解释"><a href="#详细解释" class="headerlink" title="详细解释"></a><strong>详细解释</strong></h3><h4 id="1-参数检查"><a href="#1-参数检查" class="headerlink" title="1. 参数检查"></a><strong>1. 参数检查</strong></h4><ul><li><strong><code>max_seq_len</code></strong>: 检查输入序列的最大长度是否在有效范围内（1 到 8192）。</li><li><strong><code>ckpt_dir</code></strong>: 检查检查点目录是否存在。</li><li><strong><code>tokenizer_path</code></strong>: 检查分词器文件是否存在。</li></ul><h4 id="2-分布式初始化"><a href="#2-分布式初始化" class="headerlink" title="2. 分布式初始化"></a><strong>2. 分布式初始化</strong></h4><ul><li>如果分布式进程组未初始化，则使用 <code>torch.distributed.init_process_group</code> 初始化。</li><li>如果模型并行未初始化，则根据 <code>model_parallel_size</code> 或环境变量初始化模型并行。</li></ul><h4 id="3-设备设置"><a href="#3-设备设置" class="headerlink" title="3. 设备设置"></a><strong>3. 设备设置</strong></h4><ul><li>获取当前进程的本地 rank，并设置使用的 GPU 设备。</li></ul><h4 id="4-随机种子"><a href="#4-随机种子" class="headerlink" title="4. 随机种子"></a><strong>4. 随机种子</strong></h4><ul><li>设置随机种子，确保所有进程的随机性一致。</li></ul><h4 id="5-检查点加载"><a href="#5-检查点加载" class="headerlink" title="5. 检查点加载"></a><strong>5. 检查点加载</strong></h4><ul><li>获取检查点目录中的所有 <code>.pth</code> 文件，并按文件名排序。</li><li>检查检查点文件数量和模型并行大小是否匹配。</li><li>加载当前进程对应的检查点文件到 CPU。</li></ul><h4 id="6-模型参数加载"><a href="#6-模型参数加载" class="headerlink" title="6. 模型参数加载"></a><strong>6. 模型参数加载</strong></h4><ul><li>从 <code>params.json</code> 文件中加载模型参数。</li><li>使用 <code>ModelArgs</code> 初始化模型参数。</li></ul><h4 id="7-分词器初始化"><a href="#7-分词器初始化" class="headerlink" title="7. 分词器初始化"></a><strong>7. 分词器初始化</strong></h4><ul><li>使用 <code>Tokenizer</code> 初始化分词器，并检查模型词汇表大小是否与分词器词汇表大小匹配。</li></ul><h4 id="8-张量类型设置"><a href="#8-张量类型设置" class="headerlink" title="8. 张量类型设置"></a><strong>8. 张量类型设置</strong></h4><ul><li>如果当前设备支持 <code>bfloat16</code>，则设置默认张量类型为 <code>bfloat16</code>，否则设置为 <code>float16</code>。</li></ul><h4 id="9-模型初始化"><a href="#9-模型初始化" class="headerlink" title="9. 模型初始化"></a><strong>9. 模型初始化</strong></h4><ul><li>使用 <code>Transformer</code> 初始化模型，并加载检查点中的权重。</li></ul><h4 id="10-返回-Llama-实例"><a href="#10-返回-Llama-实例" class="headerlink" title="10. 返回 Llama 实例"></a><strong>10. 返回 Llama 实例</strong></h4><ul><li>返回包含加载的模型和分词器的 <code>Llama</code> 实例。</li></ul><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p><code>build</code> 方法负责初始化并加载 Llama 模型，包括分布式设置、设备配置、检查点加载、模型参数初始化、分词器初始化等。它是 Llama 模型的核心初始化方法，确保模型能够正确加载并准备好进行推理或训练。</p><h2 id="generate-方法-1"><a href="#generate-方法-1" class="headerlink" title="generate 方法"></a><code>generate</code> 方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@torch.inference_mode()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    prompt_tokens: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]],</span><br><span class="hljs-params">    max_gen_len: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    temperature: <span class="hljs-built_in">float</span> = <span class="hljs-number">0.6</span>,</span><br><span class="hljs-params">    top_p: <span class="hljs-built_in">float</span> = <span class="hljs-number">0.9</span>,</span><br><span class="hljs-params">    logprobs: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    echo: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]], <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]]]]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据输入的 tokenized prompts 生成文本。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        prompt_tokens (List[List[int]]): tokenized prompts，每个 prompt 是一个整数列表。</span><br><span class="hljs-string">        max_gen_len (int): 生成文本的最大长度。</span><br><span class="hljs-string">        temperature (float, optional): 控制生成随机性的温度参数。默认为 0.6。</span><br><span class="hljs-string">        top_p (float, optional): top-p 采样的概率阈值。默认为 0.9。</span><br><span class="hljs-string">        logprobs (bool, optional): 是否计算 token 的 log 概率。默认为 False。</span><br><span class="hljs-string">        echo (bool, optional): 是否在生成结果中包含输入提示。默认为 False。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        Tuple[List[List[int]], Optional[List[List[float]]]]: 生成的 token 序列和 logprobs（可选）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    注意:</span><br><span class="hljs-string">        该方法使用 nucleus sampling（top-p 采样）生成文本，支持通过温度参数控制生成随机性。</span><br><span class="hljs-string">        如果 logprobs 为 True，则返回每个生成 token 的 log 概率。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 获取模型参数</span><br>    params = <span class="hljs-variable language_">self</span>.model.params<br>    <span class="hljs-comment"># 获取批次大小</span><br>    bsz = <span class="hljs-built_in">len</span>(prompt_tokens)<br>    <span class="hljs-comment"># 检查批次大小是否超过模型的最大批次大小</span><br>    <span class="hljs-keyword">assert</span> bsz &lt;= params.max_batch_size, (bsz, params.max_batch_size)<br><br>    <span class="hljs-comment"># 计算输入 prompts 的最小和最大长度</span><br>    min_prompt_len = <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(t) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> prompt_tokens)<br>    max_prompt_len = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">len</span>(t) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> prompt_tokens)<br>    <span class="hljs-comment"># 检查输入 prompts 的最大长度是否超过模型的最大序列长度</span><br>    <span class="hljs-keyword">assert</span> max_prompt_len &lt;= params.max_seq_len<br>    <span class="hljs-comment"># 计算总长度（输入 prompts 长度 + 生成文本长度）</span><br>    total_len = <span class="hljs-built_in">min</span>(params.max_seq_len, max_gen_len + max_prompt_len)<br><br>    <span class="hljs-comment"># 获取填充 token 的 ID</span><br>    pad_id = <span class="hljs-variable language_">self</span>.tokenizer.pad_id<br>    <span class="hljs-comment"># 初始化 tokens 张量，用 pad_id 填充</span><br>    tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=<span class="hljs-string">&quot;cuda&quot;</span>)<br>    <span class="hljs-comment"># 将输入 prompts 填充到 tokens 张量中</span><br>    <span class="hljs-keyword">for</span> k, t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(prompt_tokens):<br>        tokens[k, : <span class="hljs-built_in">len</span>(t)] = torch.tensor(t, dtype=torch.long, device=<span class="hljs-string">&quot;cuda&quot;</span>)<br>    <span class="hljs-comment"># 如果 logprobs 为 True，则初始化 token_logprobs 张量</span><br>    <span class="hljs-keyword">if</span> logprobs:<br>        token_logprobs = torch.zeros_like(tokens, dtype=torch.<span class="hljs-built_in">float</span>)<br><br>    <span class="hljs-comment"># 初始化当前位置和 EOS（结束符）标记</span><br>    prev_pos = <span class="hljs-number">0</span><br>    eos_reached = torch.tensor([<span class="hljs-literal">False</span>] * bsz, device=<span class="hljs-string">&quot;cuda&quot;</span>)<br>    <span class="hljs-comment"># 创建输入文本的掩码（非填充部分为 True）</span><br>    input_text_mask = tokens != pad_id<br><br>    <span class="hljs-comment"># 如果输入 prompts 的长度等于总长度，则直接计算 logits</span><br>    <span class="hljs-keyword">if</span> min_prompt_len == total_len:<br>        logits = <span class="hljs-variable language_">self</span>.model.forward(tokens, prev_pos)<br>        <span class="hljs-comment"># 计算 token 的 log 概率</span><br>        token_logprobs = -F.cross_entropy(<br>            <span class="hljs-built_in">input</span>=logits.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            target=tokens,<br>            reduction=<span class="hljs-string">&quot;none&quot;</span>,<br>            ignore_index=pad_id,<br>        )<br><br>    <span class="hljs-comment"># 获取停止 token 的 ID</span><br>    stop_tokens = torch.tensor(<span class="hljs-built_in">list</span>(<span class="hljs-variable language_">self</span>.tokenizer.stop_tokens))<br><br>    <span class="hljs-comment"># 逐 token 生成文本</span><br>    <span class="hljs-keyword">for</span> cur_pos <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(min_prompt_len, total_len):<br>        <span class="hljs-comment"># 计算当前 token 的 logits</span><br>        logits = <span class="hljs-variable language_">self</span>.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)<br>        <span class="hljs-comment"># 如果 temperature &gt; 0，则使用温度参数和 top-p 采样生成下一个 token</span><br>        <span class="hljs-keyword">if</span> temperature &gt; <span class="hljs-number">0</span>:<br>            probs = torch.softmax(logits[:, -<span class="hljs-number">1</span>] / temperature, dim=-<span class="hljs-number">1</span>)<br>            next_token = sample_top_p(probs, top_p)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 如果 temperature = 0，则选择概率最大的 token</span><br>            next_token = torch.argmax(logits[:, -<span class="hljs-number">1</span>], dim=-<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 将下一个 token 填充到 tokens 张量中</span><br>        next_token = next_token.reshape(-<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 如果当前位置在输入 prompts 范围内，则保留输入 token</span><br>        next_token = torch.where(<br>            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token<br>        )<br>        tokens[:, cur_pos] = next_token<br>        <span class="hljs-comment"># 如果 logprobs 为 True，则计算 token 的 log 概率</span><br>        <span class="hljs-keyword">if</span> logprobs:<br>            token_logprobs[:, prev_pos + <span class="hljs-number">1</span> : cur_pos + <span class="hljs-number">1</span>] = -F.cross_entropy(<br>                <span class="hljs-built_in">input</span>=logits.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>                target=tokens[:, prev_pos + <span class="hljs-number">1</span> : cur_pos + <span class="hljs-number">1</span>],<br>                reduction=<span class="hljs-string">&quot;none&quot;</span>,<br>                ignore_index=pad_id,<br>            )<br>        <span class="hljs-comment"># 检查是否生成停止 token</span><br>        eos_reached |= (~input_text_mask[:, cur_pos]) &amp; (<br>            torch.isin(next_token, stop_tokens)<br>        )<br>        <span class="hljs-comment"># 更新当前位置</span><br>        prev_pos = cur_pos<br>        <span class="hljs-comment"># 如果所有批次都生成停止 token，则提前结束</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">all</span>(eos_reached):<br>            <span class="hljs-keyword">break</span><br><br>    <span class="hljs-comment"># 如果 logprobs 为 True，则将 token_logprobs 转换为列表</span><br>    <span class="hljs-keyword">if</span> logprobs:<br>        token_logprobs = token_logprobs.tolist()<br>    <span class="hljs-comment"># 初始化输出 token 序列和 logprobs</span><br>    out_tokens, out_logprobs = [], []<br>    <span class="hljs-comment"># 处理每个批次的生成结果</span><br>    <span class="hljs-keyword">for</span> i, toks <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tokens.tolist()):<br>        <span class="hljs-comment"># 如果 echo 为 False，则从生成部分开始截取</span><br>        start = <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> echo <span class="hljs-keyword">else</span> <span class="hljs-built_in">len</span>(prompt_tokens[i])<br>        toks = toks[start : <span class="hljs-built_in">len</span>(prompt_tokens[i]) + max_gen_len]<br>        probs = <span class="hljs-literal">None</span><br>        <span class="hljs-comment"># 如果 logprobs 为 True，则截取对应的 logprobs</span><br>        <span class="hljs-keyword">if</span> logprobs:<br>            probs = token_logprobs[i][start : <span class="hljs-built_in">len</span>(prompt_tokens[i]) + max_gen_len]<br>        <span class="hljs-comment"># 如果生成结果中包含停止 token，则截取到停止 token 之前</span><br>        <span class="hljs-keyword">for</span> stop_token <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.tokenizer.stop_tokens:<br>            <span class="hljs-keyword">try</span>:<br>                eos_idx = toks.index(stop_token)<br>                toks = toks[:eos_idx]<br>                probs = probs[:eos_idx] <span class="hljs-keyword">if</span> logprobs <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">except</span> ValueError:<br>                <span class="hljs-keyword">pass</span><br>        <span class="hljs-comment"># 将结果添加到输出列表中</span><br>        out_tokens.append(toks)<br>        out_logprobs.append(probs)<br>    <span class="hljs-comment"># 返回生成的 token 序列和 logprobs（可选）</span><br>    <span class="hljs-keyword">return</span> (out_tokens, out_logprobs <span class="hljs-keyword">if</span> logprobs <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><hr><h3 id="详细解释-1"><a href="#详细解释-1" class="headerlink" title="详细解释"></a><strong>详细解释</strong></h3><h4 id="1-输入处理"><a href="#1-输入处理" class="headerlink" title="1. 输入处理"></a><strong>1. 输入处理</strong></h4><ul><li><strong><code>prompt_tokens</code></strong>: 输入的 tokenized prompts，每个 prompt 是一个整数列表。</li><li><strong><code>max_gen_len</code></strong>: 生成文本的最大长度。</li><li><strong><code>temperature</code></strong>: 控制生成随机性的温度参数。</li><li><strong><code>top_p</code></strong>: top-p 采样的概率阈值。</li><li><strong><code>logprobs</code></strong>: 是否计算 token 的 log 概率。</li><li><strong><code>echo</code></strong>: 是否在生成结果中包含输入提示。</li></ul><h4 id="2-初始化"><a href="#2-初始化" class="headerlink" title="2. 初始化"></a><strong>2. 初始化</strong></h4><ul><li><strong><code>params</code></strong>: 获取模型参数。</li><li><strong><code>bsz</code></strong>: 获取批次大小。</li><li><strong><code>min_prompt_len</code> 和 <code>max_prompt_len</code></strong>: 计算输入 prompts 的最小和最大长度。</li><li><strong><code>total_len</code></strong>: 计算总长度（输入 prompts 长度 + 生成文本长度）。</li><li><strong><code>tokens</code></strong>: 初始化 tokens 张量，用 pad_id 填充，并将输入 prompts 填充到 tokens 张量中。</li></ul><h4 id="3-生成文本"><a href="#3-生成文本" class="headerlink" title="3. 生成文本"></a><strong>3. 生成文本</strong></h4><ul><li><strong>逐 token 生成</strong>:<ul><li>使用 <code>model.forward</code> 计算当前 token 的 logits。</li><li>如果 <code>temperature &gt; 0</code>，则使用温度参数和 top-p 采样生成下一个 token。</li><li>如果 <code>temperature = 0</code>，则选择概率最大的 token。</li><li>将下一个 token 填充到 tokens 张量中。</li><li>如果 <code>logprobs</code> 为 True，则计算 token 的 log 概率。</li><li>检查是否生成停止 token，如果所有批次都生成停止 token，则提前结束。</li></ul></li></ul><h4 id="4-输出处理"><a href="#4-输出处理" class="headerlink" title="4. 输出处理"></a><strong>4. 输出处理</strong></h4><ul><li><strong><code>out_tokens</code> 和 <code>out_logprobs</code></strong>: 处理每个批次的生成结果。<ul><li>如果 <code>echo</code> 为 False，则从生成部分开始截取。</li><li>如果生成结果中包含停止 token，则截取到停止 token 之前。</li></ul></li><li><strong>返回结果</strong>: 返回生成的 token 序列和 logprobs（可选）。</li></ul><hr><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h3><p><code>generate</code> 方法是 Llama 3 模型的核心生成方法，负责根据输入的 tokenized prompts 生成文本。通过温度、top-p 采样等参数，用户可以灵活控制生成过程。该方法支持计算 token 的 log 概率，并可以处理停止 token 和输入提示。</p><h2 id="generate-详细流程图"><a href="#generate-详细流程图" class="headerlink" title="generate 详细流程图"></a><code>generate</code> 详细流程图</h2><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241230115215231.png" alt="generate流程图"></p><hr><h3 id="详细步骤说明"><a href="#详细步骤说明" class="headerlink" title="详细步骤说明"></a><strong>详细步骤说明</strong></h3><h4 id="1-输入处理-1"><a href="#1-输入处理-1" class="headerlink" title="1. 输入处理"></a><strong>1. 输入处理</strong></h4><ul><li><strong>输入</strong>: <code>prompt_tokens</code>，tokenized prompts，每个 prompt 是一个整数列表。</li></ul><h4 id="2-初始化-1"><a href="#2-初始化-1" class="headerlink" title="2. 初始化"></a><strong>2. 初始化</strong></h4><ul><li><strong>初始化 tokens 张量</strong>: 用 pad_id 填充，并将输入 prompts 填充到 tokens 张量中。</li><li><strong>计算 min_prompt_len 和 max_prompt_len</strong>: 输入 prompts 的最小和最大长度。</li><li><strong>检查 max_prompt_len 是否超过 max_seq_len</strong>: 确保输入 prompts 的长度不超过模型的最大序列长度。</li><li><strong>计算 total_len</strong>: 输入 prompts 长度 + 生成文本长度。</li><li><strong>初始化 token_logprobs</strong>: 如果 <code>logprobs=True</code>，则初始化 token_logprobs 张量。</li><li><strong>初始化 prev_pos 和 eos_reached</strong>: 当前位置和 EOS（结束符）标记。</li><li><strong>创建 input_text_mask</strong>: 输入文本的掩码（非填充部分为 True）。</li></ul><h4 id="3-生成文本-1"><a href="#3-生成文本-1" class="headerlink" title="3. 生成文本"></a><strong>3. 生成文本</strong></h4><ul><li><strong>检查 min_prompt_len 是否等于 total_len</strong>:<ul><li>如果相等，则直接计算 logits 和 token_logprobs。</li><li>否则，逐 token 生成文本。</li></ul></li><li><strong>逐 token 生成文本</strong>:<ul><li><strong>计算当前 token 的 logits</strong>。</li><li><strong>检查 temperature 是否大于 0</strong>:<ul><li>如果大于 0，则使用温度参数和 top-p 采样生成下一个 token。</li><li>否则，选择概率最大的 token。</li></ul></li><li><strong>填充下一个 token 到 tokens 张量</strong>。</li><li><strong>检查 logprobs 是否为 True</strong>:<ul><li>如果为 True，则计算 token 的 log 概率。</li></ul></li><li><strong>检查是否生成停止 token</strong>。</li><li><strong>更新 prev_pos</strong>。</li><li><strong>检查所有批次是否都生成停止 token</strong>:<ul><li>如果是，则提前结束。</li><li>否则，继续生成下一个 token。</li></ul></li></ul></li></ul><h4 id="4-输出处理-1"><a href="#4-输出处理-1" class="headerlink" title="4. 输出处理"></a><strong>4. 输出处理</strong></h4><ul><li><strong>处理每个批次的生成结果</strong>:<ul><li><strong>检查 echo 是否为 False</strong>:<ul><li>如果为 False，则从生成部分开始截取。</li><li>否则，保留输入提示。</li></ul></li><li><strong>截取到停止 token 之前</strong>。</li><li><strong>将结果添加到输出列表中</strong>。</li></ul></li><li><strong>返回生成的 token 序列和 logprobs（可选）</strong>。</li></ul><hr><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a><strong>总结</strong></h3><p><code>generate</code> 方法的流程图清晰地展示了从输入到输出的完整生成过程，包括初始化、逐 token 生成和输出处理。通过该流程图，可以更好地理解 Llama 3 模型的文本生成机制。</p><h2 id="text-completion-和-chat-completion"><a href="#text-completion-和-chat-completion" class="headerlink" title="text_completion 和 chat_completion"></a><code>text_completion</code> 和 <code>chat_completion</code></h2><hr><h3 id="1-text-completion-方法"><a href="#1-text-completion-方法" class="headerlink" title="1. text_completion 方法"></a><strong>1. <code>text_completion</code> 方法</strong></h3><h4 id="功能"><a href="#功能" class="headerlink" title="功能"></a><strong>功能</strong></h4><p>对给定的文本提示进行补全，生成后续文本。</p><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a><strong>参数</strong></h4><ul><li><strong><code>prompts</code></strong>: 文本提示列表，每个提示是一个字符串。</li><li><strong><code>temperature</code></strong>: 控制生成随机性的温度参数。值越大，生成结果越随机；值越小，生成结果越确定。</li><li><strong><code>top_p</code></strong>: top-p 采样的概率阈值。用于控制生成结果的多样性和质量。</li><li><strong><code>max_gen_len</code></strong>: 生成文本的最大长度。如果未提供，则使用模型的最大序列长度减 1。</li><li><strong><code>logprobs</code></strong>: 是否计算 token 的 log 概率。默认为 False。</li><li><strong><code>echo</code></strong>: 是否在生成结果中包含输入提示。默认为 False。</li></ul><h4 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a><strong>返回值</strong></h4><ul><li><strong><code>List[CompletionPrediction]</code></strong>: 文本补全的生成结果列表，每个结果包含生成的文本、token 列表和 logprobs（可选）。</li></ul><h4 id="代码逻辑"><a href="#代码逻辑" class="headerlink" title="代码逻辑"></a><strong>代码逻辑</strong></h4><ol><li><strong>检查 <code>max_gen_len</code></strong>:<ul><li>如果未提供 <code>max_gen_len</code>，则使用模型的最大序列长度减 1。</li></ul></li><li><strong>编码输入提示</strong>:<ul><li>使用分词器将输入提示编码为 tokenized prompts。</li></ul></li><li><strong>调用 <code>generate</code> 方法</strong>:<ul><li>使用 <code>generate</code> 方法生成 token 序列。</li></ul></li><li><strong>解码生成结果</strong>:<ul><li>将生成的 token 序列解码为文本。</li></ul></li><li><strong>返回生成结果</strong>:<ul><li>如果 <code>logprobs</code> 为 True，则返回生成的文本、token 列表和 logprobs。</li><li>否则，仅返回生成的文本。</li></ul></li></ol><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a><strong>示例</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">prompts = [<span class="hljs-string">&quot;Once upon a time&quot;</span>]<br>results = llama.text_completion(prompts, max_gen_len=<span class="hljs-number">50</span>)<br><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:<br>    <span class="hljs-built_in">print</span>(result[<span class="hljs-string">&quot;generation&quot;</span>])<br></code></pre></td></tr></table></figure><hr><h3 id="2-chat-completion-方法"><a href="#2-chat-completion-方法" class="headerlink" title="2. chat_completion 方法"></a><strong>2. <code>chat_completion</code> 方法</strong></h3><h4 id="功能-1"><a href="#功能-1" class="headerlink" title="功能"></a><strong>功能</strong></h4><p>根据对话历史生成助理的回复。</p><h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a><strong>参数</strong></h4><ul><li><strong><code>dialogs</code></strong>: 对话历史列表，每个对话是一个消息列表，每个消息包含角色（<code>role</code>）和内容（<code>content</code>）。</li><li><strong><code>temperature</code></strong>: 控制生成随机性的温度参数。值越大，生成结果越随机；值越小，生成结果越确定。</li><li><strong><code>top_p</code></strong>: top-p 采样的概率阈值。用于控制生成结果的多样性和质量。</li><li><strong><code>max_gen_len</code></strong>: 生成文本的最大长度。如果未提供，则使用模型的最大序列长度减 1。</li><li><strong><code>logprobs</code></strong>: 是否计算 token 的 log 概率。默认为 False。</li></ul><h4 id="返回值-1"><a href="#返回值-1" class="headerlink" title="返回值"></a><strong>返回值</strong></h4><ul><li><strong><code>List[ChatPrediction]</code></strong>: 对话生成的生成结果列表，每个结果包含助理的回复、token 列表和 logprobs（可选）。</li></ul><h4 id="代码逻辑-1"><a href="#代码逻辑-1" class="headerlink" title="代码逻辑"></a><strong>代码逻辑</strong></h4><ol><li><strong>检查 <code>max_gen_len</code></strong>:<ul><li>如果未提供 <code>max_gen_len</code>，则使用模型的最大序列长度减 1。</li></ul></li><li><strong>编码对话历史</strong>:<ul><li>使用 <code>ChatFormat</code> 将对话历史编码为 tokenized prompts。</li></ul></li><li><strong>调用 <code>generate</code> 方法</strong>:<ul><li>使用 <code>generate</code> 方法生成 token 序列。</li></ul></li><li><strong>解码生成结果</strong>:<ul><li>将生成的 token 序列解码为文本。</li></ul></li><li><strong>返回生成结果</strong>:<ul><li>如果 <code>logprobs</code> 为 True，则返回助理的回复、token 列表和 logprobs。</li><li>否则，仅返回助理的回复。</li></ul></li></ol><h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a><strong>示例</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">dialogs = [<br>    [&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;What is the capital of France?&quot;</span>&#125;]<br>]<br>results = llama.chat_completion(dialogs, max_gen_len=<span class="hljs-number">50</span>)<br><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:<br>    <span class="hljs-built_in">print</span>(result[<span class="hljs-string">&quot;generation&quot;</span>][<span class="hljs-string">&quot;content&quot;</span>])<br></code></pre></td></tr></table></figure><hr><h3 id="3-主要区别"><a href="#3-主要区别" class="headerlink" title="3. 主要区别"></a><strong>3. 主要区别</strong></h3><table><thead><tr><th>特性</th><th><code>text_completion</code></th><th><code>chat_completion</code></th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>文本提示列表</td><td>对话历史列表</td></tr><tr><td><strong>输出</strong></td><td>文本补全结果</td><td>助理的回复</td></tr><tr><td><strong>编码方式</strong></td><td>直接使用分词器编码</td><td>使用 <code>ChatFormat</code> 编码对话历史</td></tr><tr><td><strong>适用场景</strong></td><td>单轮文本补全</td><td>多轮对话生成</td></tr><tr><td><strong>返回格式</strong></td><td><code>CompletionPrediction</code></td><td><code>ChatPrediction</code></td></tr></tbody></table><hr><h3 id="4-生成控制参数-1"><a href="#4-生成控制参数-1" class="headerlink" title="4. 生成控制参数"></a><strong>4. 生成控制参数</strong></h3><ul><li><strong>温度（temperature）</strong>:<ul><li>控制生成随机性的参数。</li><li>值越大，生成结果越随机；值越小，生成结果越确定。</li></ul></li><li><strong>top-p 采样（nucleus sampling）</strong>:<ul><li>从概率累积值超过 <code>top_p</code> 的最小 token 集合中采样。</li><li>用于控制生成结果的多样性和质量。</li></ul></li><li><strong>logprobs</strong>:<ul><li>是否计算生成 token 的 log 概率。</li><li>用于分析生成结果的置信度。</li></ul></li></ul><hr><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a><strong>5. 总结</strong></h3><ul><li><strong><code>text_completion</code></strong>: 用于文本补全，适用于单轮文本生成任务。</li><li><strong><code>chat_completion</code></strong>: 用于对话生成，适用于多轮对话任务。</li><li>两者都通过 <code>generate</code> 方法实现核心生成逻辑，并支持温度、top-p 采样等参数控制生成过程。</li></ul><h2 id="sample-top-p"><a href="#sample-top-p" class="headerlink" title="sample_top_p"></a>sample_top_p</h2><p>该函数实现了 <strong>top-p 采样（nucleus sampling）</strong>，用于从概率分布中选择 token。</p><hr><h3 id="代码注释"><a href="#代码注释" class="headerlink" title="代码注释"></a><strong>代码注释</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sample_top_p</span>(<span class="hljs-params">probs, p</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    对概率分布进行 top-p 采样（nucleus sampling）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        probs (torch.Tensor): 概率分布张量，形状为 (batch_size, vocab_size)。</span><br><span class="hljs-string">        p (float): top-p 采样的概率阈值，取值范围为 (0, 1]。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        torch.Tensor: 采样得到的 token 索引，形状为 (batch_size, 1)。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    注意:</span><br><span class="hljs-string">        top-p 采样从概率累积值超过 p 的最小 token 集合中采样，用于控制生成结果的多样性和质量。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 对概率分布进行降序排序，并获取排序后的概率和索引</span><br>    probs_sort, probs_idx = torch.sort(probs, dim=-<span class="hljs-number">1</span>, descending=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment"># 计算概率分布的累积和</span><br>    probs_sum = torch.cumsum(probs_sort, dim=-<span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># 创建掩码，标记概率累积值超过 p 的 token</span><br>    mask = probs_sum - probs_sort &gt; p<br>    <span class="hljs-comment"># 将掩码对应的概率置为 0</span><br>    probs_sort[mask] = <span class="hljs-number">0.0</span><br>    <span class="hljs-comment"># 对剩余的概率进行归一化</span><br>    probs_sort.div_(probs_sort.<span class="hljs-built_in">sum</span>(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>))<br>    <span class="hljs-comment"># 从归一化后的概率分布中进行多项式采样</span><br>    next_token = torch.multinomial(probs_sort, num_samples=<span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># 根据采样结果获取原始 token 索引</span><br>    next_token = torch.gather(probs_idx, -<span class="hljs-number">1</span>, next_token)<br>    <span class="hljs-keyword">return</span> next_token<br></code></pre></td></tr></table></figure><hr><h3 id="详细解释-2"><a href="#详细解释-2" class="headerlink" title="详细解释"></a><strong>详细解释</strong></h3><h4 id="1-输入参数"><a href="#1-输入参数" class="headerlink" title="1. 输入参数"></a><strong>1. 输入参数</strong></h4><ul><li><strong><code>probs</code></strong>: 概率分布张量，形状为 <code>(batch_size, vocab_size)</code>，表示每个 token 的概率。</li><li><strong><code>p</code></strong>: top-p 采样的概率阈值，取值范围为 <code>(0, 1]</code>。例如，<code>p=0.9</code> 表示从概率累积值超过 0.9 的最小 token 集合中采样。</li></ul><h4 id="2-概率排序"><a href="#2-概率排序" class="headerlink" title="2. 概率排序"></a><strong>2. 概率排序</strong></h4><ul><li><strong><code>torch.sort</code></strong>: 对概率分布进行降序排序，返回排序后的概率 <code>probs_sort</code> 和对应的索引 <code>probs_idx</code>。</li></ul><h4 id="3-计算累积和"><a href="#3-计算累积和" class="headerlink" title="3. 计算累积和"></a><strong>3. 计算累积和</strong></h4><ul><li><strong><code>torch.cumsum</code></strong>: 计算排序后概率的累积和 <code>probs_sum</code>。</li></ul><h4 id="4-创建掩码"><a href="#4-创建掩码" class="headerlink" title="4. 创建掩码"></a><strong>4. 创建掩码</strong></h4><ul><li><strong><code>mask</code></strong>: 标记概率累积值超过 <code>p</code> 的 token。例如，如果 <code>p=0.9</code>，则掩码标记概率累积值超过 0.9 的 token。</li></ul><h4 id="5-概率置零"><a href="#5-概率置零" class="headerlink" title="5. 概率置零"></a><strong>5. 概率置零</strong></h4><ul><li><strong><code>probs_sort[mask] = 0.0</code></strong>: 将掩码对应的概率置为 0，排除概率累积值超过 <code>p</code> 的 token。</li></ul><h4 id="6-归一化"><a href="#6-归一化" class="headerlink" title="6. 归一化"></a><strong>6. 归一化</strong></h4><ul><li><strong><code>probs_sort.div_</code></strong>: 对剩余的概率进行归一化，使其总和为 1。</li></ul><h4 id="7-多项式采样"><a href="#7-多项式采样" class="headerlink" title="7. 多项式采样"></a><strong>7. 多项式采样</strong></h4><ul><li><strong><code>torch.multinomial</code></strong>: 从归一化后的概率分布中进行多项式采样，返回采样得到的 token 索引。</li></ul><h4 id="8-获取原始索引"><a href="#8-获取原始索引" class="headerlink" title="8. 获取原始索引"></a><strong>8. 获取原始索引</strong></h4><ul><li><strong><code>torch.gather</code></strong>: 根据采样结果获取原始 token 索引。</li></ul><h4 id="9-返回结果"><a href="#9-返回结果" class="headerlink" title="9. 返回结果"></a><strong>9. 返回结果</strong></h4><ul><li><strong><code>next_token</code></strong>: 采样得到的 token 索引，形状为 <code>(batch_size, 1)</code>。</li></ul><hr><h3 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a><strong>示例</strong></h3><p>假设有以下概率分布和参数：</p><ul><li><strong><code>probs</code></strong>: <code>[[0.1, 0.4, 0.2, 0.3]]</code>，形状为 <code>(1, 4)</code>。</li><li><strong><code>p</code></strong>: <code>0.9</code>。</li></ul><h4 id="执行步骤"><a href="#执行步骤" class="headerlink" title="执行步骤"></a><strong>执行步骤</strong></h4><ol><li><strong>排序</strong>:<ul><li><code>probs_sort</code>: <code>[[0.4, 0.3, 0.2, 0.1]]</code>。</li><li><code>probs_idx</code>: <code>[[1, 3, 2, 0]]</code>。</li></ul></li><li><strong>计算累积和</strong>:<ul><li><code>probs_sum</code>: <code>[[0.4, 0.7, 0.9, 1.0]]</code>。</li></ul></li><li><strong>创建掩码</strong>:<ul><li><code>mask</code>: <code>[[False, False, True, True]]</code>。</li></ul></li><li><strong>概率置零</strong>:<ul><li><code>probs_sort</code>: <code>[[0.4, 0.3, 0.0, 0.0]]</code>。</li></ul></li><li><strong>归一化</strong>:<ul><li><code>probs_sort</code>: <code>[[0.57, 0.43, 0.0, 0.0]]</code>。</li></ul></li><li><strong>多项式采样</strong>:<ul><li><code>next_token</code>: <code>[[1]]</code>（假设采样结果为 1）。</li></ul></li><li><strong>获取原始索引</strong>:<ul><li><code>next_token</code>: <code>[[3]]</code>。</li></ul></li></ol><h4 id="返回结果"><a href="#返回结果" class="headerlink" title="返回结果"></a><strong>返回结果</strong></h4><ul><li><code>next_token</code>: <code>[[3]]</code>。</li></ul><hr><h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a><strong>总结</strong></h3><p><code>sample_top_p</code> 函数实现了 top-p 采样（nucleus sampling），用于从概率分布中选择 token。通过控制概率阈值 <code>p</code>，可以灵活调整生成结果的多样性和质量。该函数是 Llama 3 模型生成过程的核心组件之一。</p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>nlp</category>
      
      <category>llm</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>llm</tag>
      
      <tag>llama</tag>
      
      <tag>源码解析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>llama3源码解析-01：整体代码结构及模块功能</title>
    <link href="/2024/12/26/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/nlp/llm/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9Allama3%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-01%EF%BC%9A%E6%95%B4%E4%BD%93%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84%E5%8F%8A%E6%A8%A1%E5%9D%97%E5%8A%9F%E8%83%BD/"/>
    <url>/2024/12/26/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/nlp/llm/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9Allama3%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-01%EF%BC%9A%E6%95%B4%E4%BD%93%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84%E5%8F%8A%E6%A8%A1%E5%9D%97%E5%8A%9F%E8%83%BD/</url>
    
    <content type="html"><![CDATA[<hr><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/Llama3_Repo.jpeg" alt="img"></p><p>Llama3 是一个基于 Transformer 架构的大规模语言模型，主要用于文本生成和对话任务。以下是 Llama3 项目的整体架构和各个模块的功能总结：</p><h2 id="1-项目架构"><a href="#1-项目架构" class="headerlink" title="1. 项目架构"></a>1. <strong>项目架构</strong></h2><p>Llama3 项目主要由三个核心文件组成：</p><ul><li><strong>generation.py</strong>: 负责模型的加载、文本生成和对话生成的核心逻辑。</li><li><strong>model.py</strong>: 定义了 Transformer 模型的结构，包括注意力机制、前馈网络、层归一化等组件。</li><li><strong>tokenizer.py</strong>: 负责文本的编码和解码，使用 Tiktoken 作为分词器。</li></ul><h2 id="2-模块功能"><a href="#2-模块功能" class="headerlink" title="2. 模块功能"></a>2. <strong>模块功能</strong></h2><h3 id="generation-py"><a href="#generation-py" class="headerlink" title="generation.py"></a><strong>generation.py</strong></h3><ul><li><strong>Llama 类</strong>: 这是 Llama3 的核心类，负责模型的加载、文本生成和对话生成。<ul><li><strong>build 方法</strong>: 初始化模型并加载预训练权重。它负责初始化分布式进程组、设置设备、加载模型和分词器。</li><li><strong>generate 方法</strong>: 根据输入的 token 序列生成文本。支持温度调节、top-p 采样、logprobs 计算等功能。</li><li><strong>text_completion 方法</strong>: 对输入的文本提示进行补全，生成文本补全结果。</li><li><strong>chat_completion 方法</strong>: 对输入的对话进行补全，生成助手的回复。</li><li><strong>sample_top_p 函数</strong>: 实现 top-p（nucleus）采样，用于控制生成文本的多样性。</li></ul></li></ul><h3 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a><strong>model.py</strong></h3><ul><li><strong>ModelArgs 类</strong>: 定义了模型的基本参数，如维度、层数、头数、词汇表大小等。</li><li><strong>RMSNorm 类</strong>: 实现了 RMS（Root Mean Square）归一化，用于替代传统的 LayerNorm。</li><li><strong>precompute_freqs_cis 函数</strong>: 预计算旋转位置编码的频率。</li><li><strong>reshape_for_broadcast 函数</strong>: 调整频率张量的形状以进行广播。</li><li><strong>apply_rotary_emb 函数</strong>: 应用旋转位置编码到查询和键张量上。</li><li><strong>repeat_kv 函数</strong>: 重复键和值张量以匹配头的数量。</li><li><strong>Attention 类</strong>: 实现了多头注意力机制，支持键值缓存。</li><li><strong>FeedForward 类</strong>: 实现了前馈网络，使用 SwiGLU 激活函数。</li><li><strong>TransformerBlock 类</strong>: 组合了注意力机制和前馈网络，构成了 Transformer 的一个基本块。</li><li><strong>Transformer 类</strong>: 整个 Transformer 模型，包含多个 TransformerBlock 层，负责前向传播。</li></ul><h3 id="tokenizer-py"><a href="#tokenizer-py" class="headerlink" title="tokenizer.py"></a><strong>tokenizer.py</strong></h3><ul><li><strong>Tokenizer 类</strong>: 负责文本的编码和解码，使用 Tiktoken 作为分词器。<ul><li><strong>encode 方法</strong>: 将字符串编码为 token ID 序列，支持添加 BOS 和 EOS token。</li><li><strong>decode 方法</strong>: 将 token ID 序列解码为字符串。</li><li><strong>_split_whitespaces_or_nonwhitespaces 方法</strong>: 将字符串分割为不超过最大长度的子串。</li></ul></li><li><strong>ChatFormat 类</strong>: 负责对话格式的编码。<ul><li><strong>encode_header 方法</strong>: 编码消息头（如角色信息）。</li><li><strong>encode_message 方法</strong>: 编码整个消息，包括消息头和内容。</li><li><strong>encode_dialog_prompt 方法</strong>: 编码整个对话，生成模型输入的 token 序列。</li></ul></li></ul><h2 id="3-核心功能和关键技术"><a href="#3-核心功能和关键技术" class="headerlink" title="3. 核心功能和关键技术"></a>3. <strong>核心功能和关键技术</strong></h2><h3 id="3-1-核心功能"><a href="#3-1-核心功能" class="headerlink" title="3.1 核心功能"></a>3.1 核心功能</h3><ul><li><strong>模型加载与初始化</strong>: 通过 <code>Llama.build</code> 方法加载预训练模型和分词器，初始化分布式训练环境。</li><li><strong>文本生成</strong>: 通过 <code>Llama.generate</code> 方法实现文本生成，支持温度调节、top-p 采样、logprobs 计算等功能。</li><li><strong>对话生成</strong>: 通过 <code>Llama.chat_completion</code> 方法生成对话回复，支持多轮对话。</li><li><strong>分词与编码</strong>: 通过 <code>Tokenizer</code> 类实现文本的编码和解码，支持特殊 token 的处理。</li><li><strong>Transformer 模型</strong>: 通过 <code>Transformer</code> 类实现 Transformer 模型的前向传播，支持键值缓存和旋转位置编码。</li></ul><h3 id="3-2-关键技术"><a href="#3-2-关键技术" class="headerlink" title="3.2 关键技术"></a>3.2 <strong>关键技术</strong></h3><ul><li><strong>旋转位置编码</strong>: 使用旋转位置编码（Rotary Position Embedding）来增强模型对位置信息的感知。</li><li><strong>RMSNorm</strong>: 使用 RMSNorm 替代传统的 LayerNorm，减少计算量。</li><li><strong>SwiGLU 激活函数</strong>: 在前馈网络中使用 SwiGLU 激活函数，增强模型的表达能力。</li><li><strong>Top-p 采样</strong>: 使用 top-p 采样来控制生成文本的多样性，避免生成过于随机的文本。</li></ul><h2 id="4-Llama3-模型中数据从输入到输出的整体过程"><a href="#4-Llama3-模型中数据从输入到输出的整体过程" class="headerlink" title="4. Llama3 模型中数据从输入到输出的整体过程"></a>4. <strong>Llama3 模型中数据从输入到输出的整体过程</strong></h2><p>Llama3 模型的数据处理流程从输入文本到生成输出文本，涉及多个步骤和模块的协同工作。以下是数据从输入到输出的整体过程：</p><h3 id="4-1-输入文本的编码"><a href="#4-1-输入文本的编码" class="headerlink" title="4.1 输入文本的编码"></a><strong>4.1 输入文本的编码</strong></h3><ol><li><strong>文本输入</strong>: 用户提供文本输入，可以是单句文本（用于文本补全）或多轮对话（用于对话生成）。</li><li><strong>分词与编码</strong>:<ul><li><strong>Tokenizer 类</strong>: 使用 <code>Tokenizer.encode</code> 方法将输入文本转换为 token ID 序列。该方法会根据配置决定是否添加 BOS（Begin of Sequence）和 EOS（End of Sequence）token。</li><li><strong>特殊 token 处理</strong>: 如果输入文本中包含特殊 token（如对话中的角色标记），Tokenizer 会将其编码为对应的 token ID。</li><li><strong>对话格式编码</strong>: 对于对话任务，<code>ChatFormat</code> 类会将对话中的每条消息编码为 token 序列，并添加适当的特殊 token（如 <code>&lt;|start_header_id|&gt;</code> 和 <code>&lt;|end_header_id|&gt;</code>）。</li></ul></li></ol><h3 id="4-2-模型的前向传播"><a href="#4-2-模型的前向传播" class="headerlink" title="4.2 模型的前向传播"></a><strong>4.2 模型的前向传播</strong></h3><ol><li><strong>输入 token 序列</strong>:<ul><li>编码后的 token 序列被转换为 PyTorch 张量，并输入到 Transformer 模型中。</li><li>如果输入序列长度小于模型的最大序列长度，会使用 pad token 进行填充。</li></ul></li><li><strong>嵌入层</strong>:<ul><li><strong>tok_embeddings</strong>: 输入 token 序列通过嵌入层（<code>VocabParallelEmbedding</code>）转换为词向量表示。</li><li>每个 token 被映射为一个高维向量（维度由 <code>ModelArgs.dim</code> 决定）。</li></ul></li><li><strong>旋转位置编码</strong>:<ul><li><strong>precompute_freqs_cis</strong>: 预计算旋转位置编码的频率。</li><li><strong>apply_rotary_emb</strong>: 将旋转位置编码应用到查询（query）和键（key）张量上，增强模型对位置信息的感知。</li></ul></li><li><strong>Transformer 层</strong>:<ul><li><strong>TransformerBlock</strong>: 模型由多个 TransformerBlock 组成，每个块包含一个多头注意力机制和一个前馈网络。<ul><li><strong>Attention</strong>: 多头注意力机制计算查询、键和值之间的注意力分数，生成上下文感知的表示。</li><li><strong>FeedForward</strong>: 前馈网络使用 SwiGLU 激活函数进一步增强表示。</li></ul></li><li><strong>RMSNorm</strong>: 在每个 TransformerBlock 中，使用 RMSNorm 对输入进行归一化。</li><li><strong>残差连接</strong>: 在每个 TransformerBlock 中，输入和输出通过残差连接进行叠加。</li></ul></li><li><strong>输出层</strong>:<ul><li><strong>norm</strong>: 最后一层 TransformerBlock 的输出通过 RMSNorm 进行归一化。</li><li><strong>output</strong>: 归一化后的输出通过线性层（<code>ColumnParallelLinear</code>）映射回词汇表空间，生成每个 token 的 logits（未归一化的概率分布）。</li></ul></li></ol><h3 id="4-3-文本生成"><a href="#4-3-文本生成" class="headerlink" title="4.3 文本生成"></a><strong>4.3 文本生成</strong></h3><ol><li><strong>logits 处理</strong>:<ul><li>模型输出的 logits 表示每个 token 的未归一化概率分布。</li><li>根据生成任务的需求，可以选择不同的采样策略（如贪婪采样、top-p 采样等）。</li></ul></li><li><strong>采样</strong>:<ul><li><strong>sample_top_p</strong>: 使用 top-p 采样从 logits 中选择下一个 token。该方法会从累积概率超过阈值 p 的最小 token 集合中进行采样，确保生成的文本既多样又合理。</li><li><strong>温度调节</strong>: 通过调节温度参数（temperature），控制生成文本的随机性。温度越高，生成的文本越随机；温度越低，生成的文本越确定。</li></ul></li><li><strong>生成 token 序列</strong>:<ul><li>根据采样结果，逐步生成 token 序列，直到达到最大生成长度或遇到停止 token（如 EOS token）。</li><li>在生成过程中，模型会不断更新键值缓存（key-value cache），以加速后续 token 的生成。</li></ul></li></ol><h3 id="4-4-输出文本的解码"><a href="#4-4-输出文本的解码" class="headerlink" title="4.4 输出文本的解码"></a><strong>4.4 输出文本的解码</strong></h3><ol><li><strong>token 序列解码</strong>:<ul><li>生成的 token 序列通过 <code>Tokenizer.decode</code> 方法解码为文本。</li><li>解码过程中会去除特殊 token（如 BOS 和 EOS token），并将 token ID 映射回对应的字符或子词。</li></ul></li><li><strong>输出文本</strong>:<ul><li>解码后的文本作为模型的最终输出，返回给用户。</li><li>对于对话任务，输出文本通常是助手的回复；对于文本补全任务，输出文本是输入提示的补全结果。</li></ul></li></ol><h3 id="4-5-整体流程总结"><a href="#4-5-整体流程总结" class="headerlink" title="4.5 整体流程总结"></a><strong>4.5 整体流程总结</strong></h3><ol><li><strong>输入文本</strong> → <strong>Tokenizer 编码</strong> → <strong>token 序列</strong></li><li><strong>token 序列</strong> → <strong>嵌入层</strong> → <strong>词向量表示</strong></li><li><strong>词向量表示</strong> → <strong>旋转位置编码</strong> → <strong>上下文感知表示</strong></li><li><strong>上下文感知表示</strong> → <strong>Transformer 层</strong> → <strong>logits</strong></li><li><strong>logits</strong> → <strong>采样策略</strong> → <strong>生成 token 序列</strong></li><li><strong>生成 token 序列</strong> → <strong>Tokenizer 解码</strong> → <strong>输出文本</strong></li></ol><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. <strong>总结</strong></h2><p>Llama3 模型的处理流程从输入文本到输出文本，涉及分词、编码、嵌入、Transformer 层的前向传播、采样和解码等多个步骤。每个步骤都通过精心设计的模块（如 Tokenizer、TransformerBlock、Attention 等）实现，确保模型能够高效地生成高质量的文本。通过调节温度、top-p 采样等参数，用户可以控制生成文本的多样性和质量，满足不同任务的需求。</p><p>Llama3 是一个功能强大的语言模型，支持文本生成和对话生成任务。其核心架构基于 Transformer，使用了旋转位置编码、RMSNorm、SwiGLU 激活函数等先进技术。通过 <code>generation.py</code> 中的 <code>Llama</code> 类，用户可以方便地加载模型、生成文本和对话。<code>model.py</code> 定义了 Transformer 模型的结构，<code>tokenizer.py</code> 负责文本的编码和解码。整体项目架构清晰，模块功能明确，适合大规模语言模型的训练和推理任务。</p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>nlp</category>
      
      <category>llm</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>llm</tag>
      
      <tag>llama</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>llama3源码解析-02：tokenizer模块解析</title>
    <link href="/2024/12/26/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/nlp/llm/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9Allama3%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-02%EF%BC%9Atokenizer%E6%A8%A1%E5%9D%97%E8%A7%A3%E6%9E%90/"/>
    <url>/2024/12/26/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/nlp/llm/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%9Allama3%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-02%EF%BC%9Atokenizer%E6%A8%A1%E5%9D%97%E8%A7%A3%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/Llama3_Repo.jpeg" alt="img"></p><h2 id="Tokenizer类"><a href="#Tokenizer类" class="headerlink" title="Tokenizer类"></a>Tokenizer类</h2><p>以下是 <code>tokenizer.py</code> 中 <code>Tokenizer</code> 类的逐行详细解释：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Tokenizer</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 特殊 token 的字典，用于存储特殊 token 及其对应的 token ID</span><br>    special_tokens: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">int</span>]<br><br>    <span class="hljs-comment"># 保留的特殊 token 数量，默认为 256</span><br>    num_reserved_special_tokens = <span class="hljs-number">256</span><br><br>    <span class="hljs-comment"># 正则表达式模式，用于匹配文本中的子词和特殊字符</span><br>    pat_str = <span class="hljs-string">r&quot;(?i:&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d)|[^\r\n\p&#123;L&#125;\p&#123;N&#125;]?\p&#123;L&#125;+|\p&#123;N&#125;&#123;1,3&#125;| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&quot;</span>  <span class="hljs-comment"># noqa: E501</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_path: <span class="hljs-built_in">str</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Initializes the Tokenizer with a Tiktoken model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            model_path (str): The path to the Tiktoken model file.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 检查模型文件是否存在</span><br>        <span class="hljs-keyword">assert</span> os.path.isfile(model_path), model_path<br><br>        <span class="hljs-comment"># 加载 Tiktoken 模型的 BPE（Byte Pair Encoding）合并表</span><br>        mergeable_ranks = load_tiktoken_bpe(model_path)<br>        <span class="hljs-comment"># 获取基础 token 的数量</span><br>        num_base_tokens = <span class="hljs-built_in">len</span>(mergeable_ranks)<br>        <span class="hljs-comment"># 定义特殊 token 列表</span><br>        special_tokens = [<br>            <span class="hljs-string">&quot;&lt;|begin_of_text|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|end_of_text|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|reserved_special_token_0|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|reserved_special_token_1|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|reserved_special_token_2|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|reserved_special_token_3|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|start_header_id|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|end_header_id|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|reserved_special_token_4|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|eot_id|&gt;&quot;</span>,  <span class="hljs-comment"># end of turn</span><br>        ] + [<br>            <span class="hljs-comment"># 生成剩余的保留特殊 token</span><br>            <span class="hljs-string">f&quot;&lt;|reserved_special_token_<span class="hljs-subst">&#123;i&#125;</span>|&gt;&quot;</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>, <span class="hljs-variable language_">self</span>.num_reserved_special_tokens - <span class="hljs-number">5</span>)<br>        ]<br>        <span class="hljs-comment"># 将特殊 token 映射到 token ID，ID 从基础 token 数量开始递增</span><br>        <span class="hljs-variable language_">self</span>.special_tokens = &#123;<br>            token: num_base_tokens + i <span class="hljs-keyword">for</span> i, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(special_tokens)<br>        &#125;<br>        <span class="hljs-comment"># 初始化 Tiktoken 的 Encoding 对象</span><br>        <span class="hljs-variable language_">self</span>.model = tiktoken.Encoding(<br>            name=Path(model_path).name,<br>            pat_str=<span class="hljs-variable language_">self</span>.pat_str,<br>            mergeable_ranks=mergeable_ranks,<br>            special_tokens=<span class="hljs-variable language_">self</span>.special_tokens,<br>        )<br>        <span class="hljs-comment"># 记录日志，表示模型已加载</span><br>        logger.info(<span class="hljs-string">f&quot;Reloaded tiktoken model from <span class="hljs-subst">&#123;model_path&#125;</span>&quot;</span>)<br><br>        <span class="hljs-comment"># 获取词汇表大小</span><br>        <span class="hljs-variable language_">self</span>.n_words: <span class="hljs-built_in">int</span> = <span class="hljs-variable language_">self</span>.model.n_vocab<br>        <span class="hljs-comment"># 获取 BOS（Begin of Sequence）token 的 ID</span><br>        <span class="hljs-variable language_">self</span>.bos_id: <span class="hljs-built_in">int</span> = <span class="hljs-variable language_">self</span>.special_tokens[<span class="hljs-string">&quot;&lt;|begin_of_text|&gt;&quot;</span>]<br>        <span class="hljs-comment"># 获取 EOS（End of Sequence）token 的 ID</span><br>        <span class="hljs-variable language_">self</span>.eos_id: <span class="hljs-built_in">int</span> = <span class="hljs-variable language_">self</span>.special_tokens[<span class="hljs-string">&quot;&lt;|end_of_text|&gt;&quot;</span>]<br>        <span class="hljs-comment"># 设置 pad token 的 ID，默认为 -1</span><br>        <span class="hljs-variable language_">self</span>.pad_id: <span class="hljs-built_in">int</span> = -<span class="hljs-number">1</span><br>        <span class="hljs-comment"># 定义停止 token 集合，包含 EOS 和 EOT（End of Turn）token</span><br>        <span class="hljs-variable language_">self</span>.stop_tokens = &#123;<br>            <span class="hljs-variable language_">self</span>.special_tokens[<span class="hljs-string">&quot;&lt;|end_of_text|&gt;&quot;</span>],<br>            <span class="hljs-variable language_">self</span>.special_tokens[<span class="hljs-string">&quot;&lt;|eot_id|&gt;&quot;</span>],<br>        &#125;<br>        <span class="hljs-comment"># 记录词汇表大小、BOS ID 和 EOS ID</span><br>        logger.info(<br>            <span class="hljs-string">f&quot;#words: <span class="hljs-subst">&#123;self.n_words&#125;</span> - BOS ID: <span class="hljs-subst">&#123;self.bos_id&#125;</span> - EOS ID: <span class="hljs-subst">&#123;self.eos_id&#125;</span>&quot;</span><br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        s: <span class="hljs-built_in">str</span>,</span><br><span class="hljs-params">        *,</span><br><span class="hljs-params">        bos: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        eos: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        allowed_special: <span class="hljs-type">Union</span>[<span class="hljs-type">Literal</span>[<span class="hljs-string">&quot;all&quot;</span>], AbstractSet[<span class="hljs-built_in">str</span>]] = <span class="hljs-built_in">set</span>(<span class="hljs-params"></span>),</span><br><span class="hljs-params">        disallowed_special: <span class="hljs-type">Union</span>[<span class="hljs-type">Literal</span>[<span class="hljs-string">&quot;all&quot;</span>], Collection[<span class="hljs-built_in">str</span>]] = (<span class="hljs-params"></span>),</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Encodes a string into a list of token IDs.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            s (str): The input string to be encoded.</span><br><span class="hljs-string">            bos (bool): Whether to prepend the beginning-of-sequence token.</span><br><span class="hljs-string">            eos (bool): Whether to append the end-of-sequence token.</span><br><span class="hljs-string">            allowed_tokens (&quot;all&quot;|set[str]): allowed special tokens in string</span><br><span class="hljs-string">            disallowed_tokens (&quot;all&quot;|set[str]): special tokens that raise an error when in string</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            list[int]: A list of token IDs.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 确保输入是字符串</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">type</span>(s) <span class="hljs-keyword">is</span> <span class="hljs-built_in">str</span><br><br>        <span class="hljs-comment"># Tiktoken 分词器单次编码的最大字符数</span><br>        TIKTOKEN_MAX_ENCODE_CHARS = <span class="hljs-number">400_000</span><br><br>        <span class="hljs-comment"># 最大连续非空格或空格字符数</span><br>        MAX_NO_WHITESPACES_CHARS = <span class="hljs-number">25_000</span><br><br>        <span class="hljs-comment"># 将输入字符串分割为子串，确保每个子串不超过最大字符数</span><br>        substrs = (<br>            substr<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(s), TIKTOKEN_MAX_ENCODE_CHARS)<br>            <span class="hljs-keyword">for</span> substr <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>._split_whitespaces_or_nonwhitespaces(<br>                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS<br>            )<br>        )<br>        <span class="hljs-comment"># 初始化 token ID 列表</span><br>        t: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>] = []<br>        <span class="hljs-comment"># 对每个子串进行编码</span><br>        <span class="hljs-keyword">for</span> substr <span class="hljs-keyword">in</span> substrs:<br>            t.extend(<br>                <span class="hljs-variable language_">self</span>.model.encode(<br>                    substr,<br>                    allowed_special=allowed_special,<br>                    disallowed_special=disallowed_special,<br>                )<br>            )<br>        <span class="hljs-comment"># 如果需要，在开头添加 BOS token</span><br>        <span class="hljs-keyword">if</span> bos:<br>            t.insert(<span class="hljs-number">0</span>, <span class="hljs-variable language_">self</span>.bos_id)<br>        <span class="hljs-comment"># 如果需要，在结尾添加 EOS token</span><br>        <span class="hljs-keyword">if</span> eos:<br>            t.append(<span class="hljs-variable language_">self</span>.eos_id)<br>        <span class="hljs-comment"># 返回编码后的 token ID 列表</span><br>        <span class="hljs-keyword">return</span> t<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, t: <span class="hljs-type">Sequence</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Decodes a list of token IDs into a string.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            t (List[int]): The list of token IDs to be decoded.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            str: The decoded string.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 将 token ID 序列解码为字符串</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.model.decode(cast(<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], t))<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_split_whitespaces_or_nonwhitespaces</span>(<span class="hljs-params"></span><br><span class="hljs-params">        s: <span class="hljs-built_in">str</span>, max_consecutive_slice_len: <span class="hljs-built_in">int</span></span><br><span class="hljs-params">    </span>) -&gt; Iterator[<span class="hljs-built_in">str</span>]:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`</span><br><span class="hljs-string">        consecutive whitespaces or consecutive non-whitespaces.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 初始化当前子串的长度和类型</span><br>        current_slice_len = <span class="hljs-number">0</span><br>        current_slice_is_space = s[<span class="hljs-number">0</span>].isspace() <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(s) &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span><br>        slice_start = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># 遍历字符串，按空格或非空格进行分割</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(s)):<br>            is_now_space = s[i].isspace()<br><br>            <span class="hljs-keyword">if</span> current_slice_is_space ^ is_now_space:<br>                current_slice_len = <span class="hljs-number">1</span><br>                current_slice_is_space = is_now_space<br>            <span class="hljs-keyword">else</span>:<br>                current_slice_len += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> current_slice_len &gt; max_consecutive_slice_len:<br>                    <span class="hljs-keyword">yield</span> s[slice_start:i]<br>                    slice_start = i<br>                    current_slice_len = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">yield</span> s[slice_start:]<br></code></pre></td></tr></table></figure><h3 id="逐行解释总结"><a href="#逐行解释总结" class="headerlink" title="逐行解释总结"></a>逐行解释总结</h3><ol><li><p><strong>特殊 token 处理</strong>:</p><ul><li><code>special_tokens</code> 字典存储特殊 token 及其对应的 token ID。</li><li><code>num_reserved_special_tokens</code> 定义了保留的特殊 token 数量。</li><li><code>pat_str</code> 是正则表达式模式，用于匹配文本中的子词和特殊字符。</li></ul></li><li><p><strong>初始化方法 (<code>__init__</code>)</strong>:</p><ul><li>加载 Tiktoken 模型的 BPE 合并表，并初始化特殊 token。</li><li>设置 BOS、EOS 和 pad token 的 ID，并定义停止 token 集合。</li><li>初始化 Tiktoken 的 <code>Encoding</code> 对象，用于实际的编码和解码操作。</li></ul></li><li><p><strong>编码方法 (<code>encode</code>)</strong>:</p><ul><li>将输入字符串分割为子串，确保每个子串不超过最大字符数。</li><li>对每个子串进行编码，并根据参数决定是否添加 BOS 和 EOS token。</li><li>返回编码后的 token ID 列表。</li></ul></li><li><p><strong>解码方法 (<code>decode</code>)</strong>:</p><ul><li>将 token ID 序列解码为字符串。</li></ul></li><li><p><strong>子串分割方法 (<code>_split_whitespaces_or_nonwhitespaces</code>)</strong>:</p><ul><li><p>将字符串按空格或非空格进行分割，确保每个子串不超过最大连续字符数。</p></li><li><h3 id="什么保留空格？"><a href="#什么保留空格？" class="headerlink" title="什么保留空格？"></a><strong>什么保留空格？</strong></h3><ol><li><strong>语义完整性</strong>:<ul><li>空格在文本中用于分隔单词、标点符号等，是文本结构的重要组成部分。</li><li>如果丢弃空格，分词结果可能会将多个单词合并，导致语义错误。例如：<ul><li>输入: <code>&quot;Hello, how are you?&quot;</code></li><li>丢弃空格: <code>&quot;Hello,howareyou?&quot;</code>（分词结果错误）</li><li>保留空格: <code>&quot;Hello, how are you?&quot;</code>（分词结果正确）</li></ul></li></ul></li><li><strong>特殊 token 处理</strong>:<ul><li>某些特殊 token（如 <code>&lt;|end_of_text|&gt;</code>）可能被空格包围，保留空格可以确保这些特殊 token 被正确识别和处理。</li></ul></li><li><strong>模型输入格式</strong>:<ul><li>许多语言模型（如 GPT）的输入需要保留空格，以确保生成的文本格式正确。</li></ul></li></ol></li></ul></li></ol><h3 id="对输入字符串进行tokenize的详细流程"><a href="#对输入字符串进行tokenize的详细流程" class="headerlink" title="对输入字符串进行tokenize的详细流程"></a>对输入字符串进行tokenize的详细流程</h3><p>结合一个具体的字符串示例，详细说明 <code>Tokenizer</code> 如何处理输入字符串。假设我们已经有一个 Tiktoken 模型文件，并且输入字符串为：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">&quot;Hello, how are you? &lt;|end_of_text|&gt;&quot;</span><br></code></pre></td></tr></table></figure><p>我们将逐步分析 <code>Tokenizer</code> 如何处理这个字符串。</p><hr><h4 id="1-输入字符串"><a href="#1-输入字符串" class="headerlink" title="1. 输入字符串"></a><strong>1. 输入字符串</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">s = <span class="hljs-string">&quot;Hello, how are you? &lt;|end_of_text|&gt;&quot;</span><br></code></pre></td></tr></table></figure><hr><h4 id="2-参数设置"><a href="#2-参数设置" class="headerlink" title="2. 参数设置"></a><strong>2. 参数设置</strong></h4><p>假设调用 <code>encode</code> 方法时，参数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">bos = <span class="hljs-literal">True</span>  <span class="hljs-comment"># 添加 BOS token</span><br>eos = <span class="hljs-literal">False</span>  <span class="hljs-comment"># 不添加 EOS token</span><br>allowed_special = &#123;<span class="hljs-string">&quot;&lt;|end_of_text|&gt;&quot;</span>&#125;  <span class="hljs-comment"># 允许的特殊 token</span><br>disallowed_special = ()  <span class="hljs-comment"># 不允许的特殊 token（为空）</span><br></code></pre></td></tr></table></figure><hr><h4 id="3-输入验证"><a href="#3-输入验证" class="headerlink" title="3. 输入验证"></a><strong>3. 输入验证</strong></h4><ul><li><strong>类型检查</strong>:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">assert</span> <span class="hljs-built_in">type</span>(s) <span class="hljs-keyword">is</span> <span class="hljs-built_in">str</span><br></code></pre></td></tr></table></figure>输入是一个字符串，检查通过。</li></ul><hr><h4 id="4-字符串分割"><a href="#4-字符串分割" class="headerlink" title="4. 字符串分割"></a><strong>4. 字符串分割</strong></h4><p>由于输入字符串较短（远小于 <code>TIKTOKEN_MAX_ENCODE_CHARS = 400_000</code>），不需要按字符数分割。但为了演示，我们假设字符串较长，需要按空格和非空格进行分割。</p><h5 id="分割逻辑"><a href="#分割逻辑" class="headerlink" title="分割逻辑"></a><strong>分割逻辑</strong></h5><ul><li>输入字符串: <code>&quot;Hello, how are you? &lt;|end_of_text|&gt;&quot;</code></li><li>按空格和非空格分割：<ul><li><code>&quot;Hello,&quot;</code>（非空格）</li><li><code>&quot; how &quot;</code>（空格）</li><li><code>&quot;are&quot;</code>（非空格）</li><li><code>&quot; you? &quot;</code>（空格）</li><li><code>&quot;&lt;|end_of_text|&gt;&quot;</code>（非空格）</li></ul></li></ul><h5 id="分割结果"><a href="#分割结果" class="headerlink" title="分割结果"></a><strong>分割结果</strong></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">substrs = [<span class="hljs-string">&quot;Hello,&quot;</span>, <span class="hljs-string">&quot; how &quot;</span>, <span class="hljs-string">&quot;are&quot;</span>, <span class="hljs-string">&quot; you? &quot;</span>, <span class="hljs-string">&quot;&lt;|end_of_text|&gt;&quot;</span>]<br></code></pre></td></tr></table></figure><hr><h4 id="5-子串编码"><a href="#5-子串编码" class="headerlink" title="5. 子串编码"></a><strong>5. 子串编码</strong></h4><p>对每个子串进行编码，生成 token ID 序列。</p><h5 id="假设的-Tiktoken-模型"><a href="#假设的-Tiktoken-模型" class="headerlink" title="假设的 Tiktoken 模型"></a><strong>假设的 Tiktoken 模型</strong></h5><p>假设 Tiktoken 模型的词汇表和编码规则如下：</p><ul><li><code>&quot;Hello&quot;</code> → <code>[15496]</code></li><li><code>&quot;,&quot;</code> → <code>[11]</code></li><li><code>&quot; how&quot;</code> → <code>[703]</code></li><li><code>&quot;are&quot;</code> → <code>[527]</code></li><li><code>&quot; you&quot;</code> → <code>[366]</code></li><li><code>&quot;?&quot;</code> → <code>[30]</code></li><li><code>&quot;&lt;|end_of_text|&gt;&quot;</code> → <code>[50256]</code>（特殊 token）</li></ul><h5 id="编码过程"><a href="#编码过程" class="headerlink" title="编码过程"></a><strong>编码过程</strong></h5><ol><li><strong>子串 <code>&quot;Hello,&quot;</code></strong>:<ul><li>编码为 <code>[15496, 11]</code>。</li></ul></li><li><strong>子串 <code>&quot; how &quot;</code></strong>:<ul><li>编码为 <code>[703]</code>。</li></ul></li><li><strong>子串 <code>&quot;are&quot;</code></strong>:<ul><li>编码为 <code>[527]</code>。</li></ul></li><li><strong>子串 <code>&quot; you? &quot;</code></strong>:<ul><li>编码为 <code>[366, 30]</code>。</li></ul></li><li><strong>子串 <code>&quot;&lt;|end_of_text|&gt;&quot;</code></strong>:<ul><li>这是一个特殊 token，编码为 <code>[50256]</code>。</li></ul></li></ol><h5 id="编码结果"><a href="#编码结果" class="headerlink" title="编码结果"></a><strong>编码结果</strong></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">t = [<span class="hljs-number">15496</span>, <span class="hljs-number">11</span>, <span class="hljs-number">703</span>, <span class="hljs-number">527</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50256</span>]<br></code></pre></td></tr></table></figure><hr><h4 id="6-添加特殊-token"><a href="#6-添加特殊-token" class="headerlink" title="6. 添加特殊 token"></a><strong>6. 添加特殊 token</strong></h4><p>根据参数设置，需要在开头添加 BOS token，不添加 EOS token。</p><h5 id="BOS-token"><a href="#BOS-token" class="headerlink" title="BOS token"></a><strong>BOS token</strong></h5><p>假设 BOS token 的 ID 为 <code>50257</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> bos:<br>    t.insert(<span class="hljs-number">0</span>, <span class="hljs-number">50257</span>)<br></code></pre></td></tr></table></figure><h5 id="更新后的-token-序列"><a href="#更新后的-token-序列" class="headerlink" title="更新后的 token 序列"></a><strong>更新后的 token 序列</strong></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">t = [<span class="hljs-number">50257</span>, <span class="hljs-number">15496</span>, <span class="hljs-number">11</span>, <span class="hljs-number">703</span>, <span class="hljs-number">527</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50256</span>]<br></code></pre></td></tr></table></figure><hr><h4 id="7-返回编码结果"><a href="#7-返回编码结果" class="headerlink" title="7. 返回编码结果"></a><strong>7. 返回编码结果</strong></h4><p>最终的 token ID 序列为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">50257</span>, <span class="hljs-number">15496</span>, <span class="hljs-number">11</span>, <span class="hljs-number">703</span>, <span class="hljs-number">527</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50256</span>]<br></code></pre></td></tr></table></figure><hr><h4 id="8-解码过程"><a href="#8-解码过程" class="headerlink" title="8. 解码过程"></a><strong>8. 解码过程</strong></h4><p>如果需要将 token ID 序列解码回字符串，可以使用 <code>decode</code> 方法。</p><h5 id="解码逻辑"><a href="#解码逻辑" class="headerlink" title="解码逻辑"></a><strong>解码逻辑</strong></h5><ul><li>将每个 token ID 映射回对应的子词或字符。</li><li>特殊 token（如 <code>&lt;|end_of_text|&gt;</code>）会被解码为原始字符串。</li></ul><h5 id="解码结果"><a href="#解码结果" class="headerlink" title="解码结果"></a><strong>解码结果</strong></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">decoded_str = <span class="hljs-string">&quot;&lt;|begin_of_text|&gt;Hello, how are you? &lt;|end_of_text|&gt;&quot;</span><br></code></pre></td></tr></table></figure><hr><h4 id="9-总结"><a href="#9-总结" class="headerlink" title="9. 总结"></a><strong>9. 总结</strong></h4><p>结合具体示例，<code>Tokenizer</code> 处理输入字符串的详细流程如下：</p><ol><li><strong>输入字符串</strong>: <code>&quot;Hello, how are you? &lt;|end_of_text|&gt;&quot;</code></li><li><strong>分割字符串</strong>: 按空格和非空格分割为 <code>[&quot;Hello,&quot;, &quot; how &quot;, &quot;are&quot;, &quot; you? &quot;, &quot;&lt;|end_of_text|&gt;&quot;]</code>。</li><li><strong>编码子串</strong>: 将每个子串编码为 token ID 序列 <code>[15496, 11, 703, 527, 366, 30, 50256]</code>。</li><li><strong>添加 BOS token</strong>: 在开头插入 BOS token，得到 <code>[50257, 15496, 11, 703, 527, 366, 30, 50256]</code>。</li><li><strong>返回结果</strong>: 返回最终的 token ID 序列。</li><li><strong>解码</strong>: 将 token ID 序列解码回原始字符串 <code>&quot;&lt;|begin_of_text|&gt;Hello, how are you? &lt;|end_of_text|&gt;&quot;</code>。</li></ol><p>通过这种分步骤的处理方式，<code>Tokenizer</code> 能够高效地将输入字符串转换为模型可处理的 token ID 序列，并支持特殊 token 的灵活处理。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><code>Tokenizer</code> 类负责文本的编码和解码，使用 Tiktoken 作为底层分词器。它支持特殊 token 的处理，并提供了灵活的编码和解码接口。通过 <code>encode</code> 方法，可以将文本转换为 token ID 序列；通过 <code>decode</code> 方法，可以将 token ID 序列转换回文本。此外，<code>Tokenizer</code> 还提供了对长文本的分割功能，确保编码过程不会因输入过长而失败。</p><h2 id="ChatFormat类"><a href="#ChatFormat类" class="headerlink" title="ChatFormat类"></a>ChatFormat类</h2><p><code>ChatFormat</code> 类的主要功能是将对话消息（<code>Message</code>）编码为模型可以理解的 token 序列，特别适用于对话生成任务。</p><hr><h3 id="代码注释"><a href="#代码注释" class="headerlink" title="代码注释"></a><strong>代码注释</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ChatFormat</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, tokenizer: Tokenizer</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        初始化 ChatFormat 类。</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            tokenizer (Tokenizer): 用于编码和解码的 Tokenizer 实例。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-variable language_">self</span>.tokenizer = tokenizer<br></code></pre></td></tr></table></figure><ul><li><strong>功能</strong>: 初始化 <code>ChatFormat</code> 类，绑定一个 <code>Tokenizer</code> 实例，用于后续的编码操作。</li><li><strong>参数</strong>:<ul><li><code>tokenizer</code>: 一个 <code>Tokenizer</code> 对象，用于将文本转换为 token ID 序列。</li></ul></li></ul><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode_header</span>(<span class="hljs-params">self, message: Message</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    编码消息头，包括角色信息和分隔符。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        message (Message): 包含角色和内容的消息字典。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        List[int]: 编码后的 token ID 序列。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    tokens = []<br>    <span class="hljs-comment"># 添加消息头开始标记</span><br>    tokens.append(<span class="hljs-variable language_">self</span>.tokenizer.special_tokens[<span class="hljs-string">&quot;&lt;|start_header_id|&gt;&quot;</span>])<br>    <span class="hljs-comment"># 编码角色信息（如 &quot;user&quot; 或 &quot;assistant&quot;）</span><br>    tokens.extend(<span class="hljs-variable language_">self</span>.tokenizer.encode(message[<span class="hljs-string">&quot;role&quot;</span>], bos=<span class="hljs-literal">False</span>, eos=<span class="hljs-literal">False</span>))<br>    <span class="hljs-comment"># 添加消息头结束标记</span><br>    tokens.append(<span class="hljs-variable language_">self</span>.tokenizer.special_tokens[<span class="hljs-string">&quot;&lt;|end_header_id|&gt;&quot;</span>])<br>    <span class="hljs-comment"># 添加换行符（两个换行符，用于分隔消息头和内容）</span><br>    tokens.extend(<span class="hljs-variable language_">self</span>.tokenizer.encode(<span class="hljs-string">&quot;\n\n&quot;</span>, bos=<span class="hljs-literal">False</span>, eos=<span class="hljs-literal">False</span>))<br>    <span class="hljs-keyword">return</span> tokens<br></code></pre></td></tr></table></figure><ul><li><strong>功能</strong>: 编码消息头，包括角色信息和分隔符。</li><li><strong>参数</strong>:<ul><li><code>message</code>: 一个 <code>Message</code> 字典，包含 <code>role</code>（角色）和 <code>content</code>（内容）。</li></ul></li><li><strong>返回值</strong>: 编码后的 token ID 序列。</li><li><strong>详细步骤</strong>:<ol><li>添加消息头开始标记 <code>&lt;|start_header_id|&gt;</code>。</li><li>编码角色信息（如 <code>&quot;user&quot;</code> 或 <code>&quot;assistant&quot;</code>）。</li><li>添加消息头结束标记 <code>&lt;|end_header_id|&gt;</code>。</li><li>添加两个换行符 <code>\n\n</code>，用于分隔消息头和内容。</li></ol></li></ul><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode_message</span>(<span class="hljs-params">self, message: Message</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    编码整个消息，包括消息头和内容。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        message (Message): 包含角色和内容的消息字典。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        List[int]: 编码后的 token ID 序列。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 编码消息头</span><br>    tokens = <span class="hljs-variable language_">self</span>.encode_header(message)<br>    <span class="hljs-comment"># 编码消息内容，并去除首尾空白字符</span><br>    tokens.extend(<br>        <span class="hljs-variable language_">self</span>.tokenizer.encode(message[<span class="hljs-string">&quot;content&quot;</span>].strip(), bos=<span class="hljs-literal">False</span>, eos=<span class="hljs-literal">False</span>)<br>    )<br>    <span class="hljs-comment"># 添加消息结束标记 &lt;|eot_id|&gt;</span><br>    tokens.append(<span class="hljs-variable language_">self</span>.tokenizer.special_tokens[<span class="hljs-string">&quot;&lt;|eot_id|&gt;&quot;</span>])<br>    <span class="hljs-keyword">return</span> tokens<br></code></pre></td></tr></table></figure><ul><li><strong>功能</strong>: 编码整个消息，包括消息头和内容。</li><li><strong>参数</strong>:<ul><li><code>message</code>: 一个 <code>Message</code> 字典，包含 <code>role</code>（角色）和 <code>content</code>（内容）。</li></ul></li><li><strong>返回值</strong>: 编码后的 token ID 序列。</li><li><strong>详细步骤</strong>:<ol><li>调用 <code>encode_header</code> 方法，编码消息头。</li><li>编码消息内容，并去除首尾空白字符。</li><li>添加消息结束标记 <code>&lt;|eot_id|&gt;</code>，表示当前消息的结束。</li></ol></li></ul><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode_dialog_prompt</span>(<span class="hljs-params">self, dialog: Dialog</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    编码整个对话，生成模型输入的 token 序列。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        dialog (Dialog): 对话列表，包含多条消息。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        List[int]: 编码后的 token ID 序列。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    tokens = []<br>    <span class="hljs-comment"># 添加对话开始标记 &lt;|begin_of_text|&gt;</span><br>    tokens.append(<span class="hljs-variable language_">self</span>.tokenizer.special_tokens[<span class="hljs-string">&quot;&lt;|begin_of_text|&gt;&quot;</span>])<br>    <span class="hljs-comment"># 编码每条消息</span><br>    <span class="hljs-keyword">for</span> message <span class="hljs-keyword">in</span> dialog:<br>        tokens.extend(<span class="hljs-variable language_">self</span>.encode_message(message))<br>    <span class="hljs-comment"># 添加助手消息的开始标记，供模型生成回复</span><br>    tokens.extend(<span class="hljs-variable language_">self</span>.encode_header(&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;&quot;</span>&#125;))<br>    <span class="hljs-keyword">return</span> tokens<br></code></pre></td></tr></table></figure><ul><li><strong>功能</strong>: 编码整个对话，生成模型输入的 token 序列。</li><li><strong>参数</strong>:<ul><li><code>dialog</code>: 一个 <code>Dialog</code> 列表，包含多条消息。</li></ul></li><li><strong>返回值</strong>: 编码后的 token ID 序列。</li><li><strong>详细步骤</strong>:<ol><li>添加对话开始标记 <code>&lt;|begin_of_text|&gt;</code>。</li><li>遍历对话中的每条消息，调用 <code>encode_message</code> 方法进行编码。</li><li>添加助手消息的开始标记（包括角色信息和分隔符），供模型生成回复。</li></ol></li></ul><hr><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h3><h4 id="功能"><a href="#功能" class="headerlink" title="功能"></a><strong>功能</strong></h4><p><code>ChatFormat</code> 类的主要功能是将对话消息（<code>Message</code>）编码为模型可以理解的 token 序列。它特别适用于对话生成任务，能够处理多轮对话，并生成符合模型输入格式的 token 序列。</p><h4 id="核心方法"><a href="#核心方法" class="headerlink" title="核心方法"></a><strong>核心方法</strong></h4><ol><li><p><strong><code>encode_header</code></strong>:</p><ul><li>编码消息头，包括角色信息和分隔符。</li><li>用于标识每条消息的角色（如 <code>&quot;user&quot;</code> 或 <code>&quot;assistant&quot;</code>）。</li></ul></li><li><p><strong><code>encode_message</code></strong>:</p><ul><li>编码整个消息，包括消息头和内容。</li><li>在消息末尾添加结束标记 <code>&lt;|eot_id|&gt;</code>，表示当前消息的结束。</li></ul></li><li><p><strong><code>encode_dialog_prompt</code></strong>:</p><ul><li>编码整个对话，生成模型输入的 token 序列。</li><li>在对话末尾添加助手消息的开始标记，供模型生成回复。</li></ul></li></ol><h4 id="特殊-token"><a href="#特殊-token" class="headerlink" title="特殊 token"></a><strong>特殊 token</strong></h4><ul><li><code>&lt;|begin_of_text|&gt;</code>: 对话开始标记。</li><li><code>&lt;|start_header_id|&gt;</code>: 消息头开始标记。</li><li><code>&lt;|end_header_id|&gt;</code>: 消息头结束标记。</li><li><code>&lt;|eot_id|&gt;</code>: 消息结束标记。</li></ul><h4 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a><strong>适用场景</strong></h4><ul><li><strong>对话生成</strong>: 将多轮对话编码为模型输入，生成助手的回复。</li><li><strong>消息格式化</strong>: 确保每条消息的格式符合模型的要求，包括角色信息和内容分隔符。</li></ul><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a><strong>示例</strong></h4><p>假设有以下对话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dialog = [<br>    &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Hello, how are you?&quot;</span>&#125;,<br>    &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>&#125;,<br>]<br></code></pre></td></tr></table></figure><p>调用 <code>encode_dialog_prompt(dialog)</code> 后，生成的 token 序列可能如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">[<br>    &lt;|begin_of_text|&gt;,<br>    &lt;|start_header_id|&gt;, <span class="hljs-string">&quot;user&quot;</span>, &lt;|end_header_id|&gt;, <span class="hljs-string">&quot;\n\n&quot;</span>, <span class="hljs-string">&quot;Hello, how are you?&quot;</span>, &lt;|eot_id|&gt;,<br>    &lt;|start_header_id|&gt;, <span class="hljs-string">&quot;assistant&quot;</span>, &lt;|end_header_id|&gt;, <span class="hljs-string">&quot;\n\n&quot;</span>, <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>, &lt;|eot_id|&gt;,<br>    &lt;|start_header_id|&gt;, <span class="hljs-string">&quot;assistant&quot;</span>, &lt;|end_header_id|&gt;, <span class="hljs-string">&quot;\n\n&quot;</span><br>]<br></code></pre></td></tr></table></figure><h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a><strong>总结</strong></h4><p><code>ChatFormat</code> 类通过定义清晰的对话格式和特殊 token，确保对话消息能够被模型正确理解和处理。它是对话生成任务中不可或缺的一部分，能够有效提升模型生成回复的准确性和连贯性。</p><h2 id="Message-实例解析"><a href="#Message-实例解析" class="headerlink" title="Message 实例解析"></a><strong>Message</strong> 实例解析</h2><p>结合实际的 <code>Message</code> 输入，详细说明 <code>Tokenizer</code> 和 <code>ChatFormat</code> 两个类如何协同工作，从 <code>Message</code> 输入到输出的完整流程。我们将重点关注 <code>encode_message</code>、<code>decode_message</code> 和 <code>encode_dialog_prompt</code> 方法，并明确区分 <code>encode_message</code> 和 <code>encode_dialog_prompt</code> 的输入和输出。</p><hr><h3 id="1-输入数据"><a href="#1-输入数据" class="headerlink" title="1. 输入数据"></a><strong>1. 输入数据</strong></h3><p>假设我们有以下 <code>Message</code> 和 <code>Dialog</code> 输入：</p><h4 id="单个消息（Message）"><a href="#单个消息（Message）" class="headerlink" title="单个消息（Message）"></a><strong>单个消息（Message）</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">message = &#123;<br>    <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,<br>    <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Hello, how are you?&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure><h4 id="多轮对话（Dialog）"><a href="#多轮对话（Dialog）" class="headerlink" title="多轮对话（Dialog）"></a><strong>多轮对话（Dialog）</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dialog = [<br>    &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Hello, how are you?&quot;</span>&#125;,<br>    &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>&#125;,<br>]<br></code></pre></td></tr></table></figure><hr><h3 id="2-Tokenizer-和-ChatFormat-初始化"><a href="#2-Tokenizer-和-ChatFormat-初始化" class="headerlink" title="2. Tokenizer 和 ChatFormat 初始化"></a><strong>2. Tokenizer 和 ChatFormat 初始化</strong></h3><p>首先，我们需要初始化 <code>Tokenizer</code> 和 <code>ChatFormat</code> 类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 假设模型文件路径为 &quot;tiktoken_model.model&quot;</span><br>tokenizer = Tokenizer(model_path=<span class="hljs-string">&quot;tiktoken_model.model&quot;</span>)<br>chat_format = ChatFormat(tokenizer)<br></code></pre></td></tr></table></figure><hr><h3 id="3-encode-message-的流程"><a href="#3-encode-message-的流程" class="headerlink" title="3. encode_message 的流程"></a><strong>3. encode_message 的流程</strong></h3><p><code>encode_message</code> 方法用于编码单个消息，包括消息头和内容。</p><h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a><strong>输入</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">message = &#123;<br>    <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,<br>    <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Hello, how are you?&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a><strong>步骤</strong></h4><ol><li><p><strong>编码消息头</strong>:</p><ul><li>调用 <code>encode_header</code> 方法，生成消息头的 token 序列。</li><li>假设 <code>encode_header</code> 返回的 token 序列为：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">50258</span>, <span class="hljs-number">366</span>, <span class="hljs-number">50259</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>]  <span class="hljs-comment"># &lt;|start_header_id|&gt;, &quot;user&quot;, &lt;|end_header_id|&gt;, &quot;\n\n&quot;</span><br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>编码消息内容</strong>:</p><ul><li>调用 <code>Tokenizer.encode</code> 方法，编码消息内容 <code>&quot;Hello, how are you?&quot;</code>。</li><li>假设返回的 token 序列为：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">15496</span>, <span class="hljs-number">11</span>, <span class="hljs-number">703</span>, <span class="hljs-number">527</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>]  <span class="hljs-comment"># &quot;Hello, how are you?&quot;</span><br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>添加消息结束标记</strong>:</p><ul><li>添加 <code>&lt;|eot_id|&gt;</code> 标记，假设其 ID 为 <code>50256</code>。</li><li>最终的 token 序列为：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">50258</span>, <span class="hljs-number">366</span>, <span class="hljs-number">50259</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">15496</span>, <span class="hljs-number">11</span>, <span class="hljs-number">703</span>, <span class="hljs-number">527</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50256</span>]<br></code></pre></td></tr></table></figure></li></ul></li></ol><h4 id="输出"><a href="#输出" class="headerlink" title="输出"></a><strong>输出</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">encoded_message = chat_format.encode_message(message)<br><span class="hljs-comment"># encoded_message = [50258, 366, 50259, 11, 11, 15496, 11, 703, 527, 366, 30, 50256]</span><br></code></pre></td></tr></table></figure><hr><h3 id="4-decode-message-的流程"><a href="#4-decode-message-的流程" class="headerlink" title="4. decode_message 的流程"></a><strong>4. decode_message 的流程</strong></h3><p><code>decode_message</code> 方法用于将 token 序列解码回原始消息。</p><h4 id="输入-1"><a href="#输入-1" class="headerlink" title="输入"></a><strong>输入</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">encoded_message = [<span class="hljs-number">50258</span>, <span class="hljs-number">366</span>, <span class="hljs-number">50259</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">15496</span>, <span class="hljs-number">11</span>, <span class="hljs-number">703</span>, <span class="hljs-number">527</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50256</span>]<br></code></pre></td></tr></table></figure><h4 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a><strong>步骤</strong></h4><ol><li><p><strong>解码 token 序列</strong>:</p><ul><li>调用 <code>Tokenizer.decode</code> 方法，将 token 序列解码为字符串。</li><li>假设解码结果为：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\nHello, how are you?&lt;|eot_id|&gt;&quot;</span><br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>提取消息内容</strong>:</p><ul><li>从解码结果中提取消息内容 <code>&quot;Hello, how are you?&quot;</code>。</li></ul></li></ol><h4 id="输出-1"><a href="#输出-1" class="headerlink" title="输出"></a><strong>输出</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">decoded_message = tokenizer.decode(encoded_message)<br><span class="hljs-comment"># decoded_message = &quot;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\nHello, how are you?&lt;|eot_id|&gt;&quot;</span><br></code></pre></td></tr></table></figure><hr><h3 id="5-encode-dialog-prompt-的流程"><a href="#5-encode-dialog-prompt-的流程" class="headerlink" title="5. encode_dialog_prompt 的流程"></a><strong>5. encode_dialog_prompt 的流程</strong></h3><p><code>encode_dialog_prompt</code> 方法用于编码整个对话，生成模型输入的 token 序列。</p><h4 id="输入-2"><a href="#输入-2" class="headerlink" title="输入"></a><strong>输入</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dialog = [<br>    &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Hello, how are you?&quot;</span>&#125;,<br>    &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>&#125;,<br>]<br></code></pre></td></tr></table></figure><h4 id="步骤-2"><a href="#步骤-2" class="headerlink" title="步骤"></a><strong>步骤</strong></h4><ol><li><p><strong>添加对话开始标记</strong>:</p><ul><li>添加 <code>&lt;|begin_of_text|&gt;</code> 标记，假设其 ID 为 <code>50257</code>。</li><li>当前的 token 序列为：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">50257</span>]<br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>编码每条消息</strong>:</p><ul><li>对每条消息调用 <code>encode_message</code> 方法，生成 token 序列。</li><li>假设第一条消息的 token 序列为：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">50258</span>, <span class="hljs-number">366</span>, <span class="hljs-number">50259</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">15496</span>, <span class="hljs-number">11</span>, <span class="hljs-number">703</span>, <span class="hljs-number">527</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50256</span>]<br></code></pre></td></tr></table></figure></li><li>假设第二条消息的 token 序列为：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">50258</span>, <span class="hljs-number">527</span>, <span class="hljs-number">50259</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">703</span>, <span class="hljs-number">527</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50256</span>]<br></code></pre></td></tr></table></figure></li><li>将两条消息的 token 序列合并：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">50257</span>, <span class="hljs-number">50258</span>, <span class="hljs-number">366</span>, <span class="hljs-number">50259</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">15496</span>, <span class="hljs-number">11</span>, <span class="hljs-number">703</span>, <span class="hljs-number">527</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50256</span>, <span class="hljs-number">50258</span>, <span class="hljs-number">527</span>, <span class="hljs-number">50259</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">703</span>, <span class="hljs-number">527</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50256</span>]<br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>添加助手消息的开始标记</strong>:</p><ul><li>添加助手消息的开始标记（包括角色信息和分隔符），供模型生成回复。</li><li>假设生成的 token 序列为：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">50258</span>, <span class="hljs-number">527</span>, <span class="hljs-number">50259</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>]<br></code></pre></td></tr></table></figure></li><li>最终的 token 序列为：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">50257</span>, <span class="hljs-number">50258</span>, <span class="hljs-number">366</span>, <span class="hljs-number">50259</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">15496</span>, <span class="hljs-number">11</span>, <span class="hljs-number">703</span>, <span class="hljs-number">527</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50256</span>, <span class="hljs-number">50258</span>, <span class="hljs-number">527</span>, <span class="hljs-number">50259</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">703</span>, <span class="hljs-number">527</span>, <span class="hljs-number">366</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50256</span>, <span class="hljs-number">50258</span>, <span class="hljs-number">527</span>, <span class="hljs-number">50259</span>, <span class="hljs-number">11</span>, <span class="hljs-number">11</span>]<br></code></pre></td></tr></table></figure></li></ul></li></ol><h4 id="输出-2"><a href="#输出-2" class="headerlink" title="输出"></a><strong>输出</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">encoded_dialog = chat_format.encode_dialog_prompt(dialog)<br><span class="hljs-comment"># encoded_dialog = [50257, 50258, 366, 50259, 11, 11, 15496, 11, 703, 527, 366, 30, 50256, 50258, 527, 50259, 11, 11, 366, 30, 703, 527, 366, 30, 50256, 50258, 527, 50259, 11, 11]</span><br></code></pre></td></tr></table></figure><hr><h3 id="6-区分-encode-message-和-encode-dialog-prompt"><a href="#6-区分-encode-message-和-encode-dialog-prompt" class="headerlink" title="6. 区分 encode_message 和 encode_dialog_prompt"></a><strong>6. 区分 encode_message 和 encode_dialog_prompt</strong></h3><h4 id="encode-message"><a href="#encode-message" class="headerlink" title="encode_message"></a><strong>encode_message</strong></h4><ul><li><strong>输入</strong>: 单个 <code>Message</code> 字典。</li><li><strong>输出</strong>: 编码后的 token 序列，包含消息头、内容和结束标记。</li><li><strong>用途</strong>: 用于编码单条消息。</li></ul><h4 id="encode-dialog-prompt"><a href="#encode-dialog-prompt" class="headerlink" title="encode_dialog_prompt"></a><strong>encode_dialog_prompt</strong></h4><ul><li><strong>输入</strong>: 一个 <code>Dialog</code> 列表，包含多条消息。</li><li><strong>输出</strong>: 编码后的 token 序列，包含对话开始标记、所有消息的编码以及助手消息的开始标记。</li><li><strong>用途</strong>: 用于编码整个对话，生成模型输入的 token 序列。</li></ul><hr><h3 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a><strong>7. 总结</strong></h3><p>通过 <code>Tokenizer</code> 和 <code>ChatFormat</code> 两个类的协同工作，我们可以将 <code>Message</code> 和 <code>Dialog</code> 编码为模型可以理解的 token 序列，并能够将 token 序列解码回原始文本。以下是完整的流程总结：</p><ol><li><p><strong>单个消息编码</strong>:</p><ul><li>使用 <code>encode_message</code> 方法，将 <code>Message</code> 编码为 token 序列。</li><li>输出包含消息头、内容和结束标记。</li></ul></li><li><p><strong>对话编码</strong>:</p><ul><li>使用 <code>encode_dialog_prompt</code> 方法，将 <code>Dialog</code> 编码为 token 序列。</li><li>输出包含对话开始标记、所有消息的编码以及助手消息的开始标记。</li></ul></li><li><p><strong>解码</strong>:</p><ul><li>使用 <code>Tokenizer.decode</code> 方法，将 token 序列解码回原始文本。</li></ul></li></ol><p>通过这种方式，<code>Tokenizer</code> 和 <code>ChatFormat</code> 能够高效地处理对话数据，为语言模型提供格式化的输入。</p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>nlp</category>
      
      <category>llm</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>llm</tag>
      
      <tag>llama</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 日志处理最佳实践：使用loguru模块构建高效日志系统</title>
    <link href="/2024/12/21/%E5%BC%80%E5%8F%91/Python/Python-004%EF%BC%9APython%20%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%9A%E4%BD%BF%E7%94%A8loguru%E6%A8%A1%E5%9D%97%E6%9E%84%E5%BB%BA%E9%AB%98%E6%95%88%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"/>
    <url>/2024/12/21/%E5%BC%80%E5%8F%91/Python/Python-004%EF%BC%9APython%20%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%9A%E4%BD%BF%E7%94%A8loguru%E6%A8%A1%E5%9D%97%E6%9E%84%E5%BB%BA%E9%AB%98%E6%95%88%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="Python-日志处理最佳实践：使用-loguru-构建高效日志系统"><a href="#Python-日志处理最佳实践：使用-loguru-构建高效日志系统" class="headerlink" title="Python 日志处理最佳实践：使用 loguru 构建高效日志系统"></a>Python 日志处理最佳实践：使用 <code>loguru</code> 构建高效日志系统</h1><p>在现代软件开发中，日志记录是不可或缺的一部分。Python 的标准库 <code>logging</code> 是一个强大且灵活的日志记录工具，但在实际项目中，配置和管理日志可能会变得复杂。为了简化日志记录的过程，<code>loguru</code> 库应运而生。<code>loguru</code> 是一个功能强大且易于使用的日志库，它提供了更简洁的 API 和更丰富的功能。本文将结合实际项目经验，总结使用 <code>loguru</code> 模块的最佳实践，并提供一个完整的代码示例。</p><hr><h2 id="1-为什么选择-loguru？"><a href="#1-为什么选择-loguru？" class="headerlink" title="1. 为什么选择 loguru？"></a>1. 为什么选择 <code>loguru</code>？</h2><p>相比于 <code>logging</code> 模块，<code>loguru</code> 提供了以下优势：</p><ul><li><strong>简洁的 API</strong>：无需复杂的配置，只需几行代码即可完成日志记录。</li><li><strong>自动日志轮转</strong>：支持日志文件的自动轮转，避免日志文件过大。</li><li><strong>丰富的日志格式</strong>：支持多种日志格式，包括颜色高亮、时间戳等。</li><li><strong>异常捕获</strong>：自动捕获未处理的异常，并记录完整的堆栈信息。</li><li><strong>日志过滤</strong>：支持基于日志级别和模块的过滤。</li><li><strong>高性能</strong>：内部优化了性能，适合高并发场景。</li></ul><hr><h2 id="2-最佳实践概述"><a href="#2-最佳实践概述" class="headerlink" title="2. 最佳实践概述"></a>2. 最佳实践概述</h2><p>在实际项目中，使用 <code>loguru</code> 的最佳实践包括以下几点：</p><ol><li><strong>统一的日志配置</strong>：通过一个统一的日志配置函数，避免在每个模块中重复编写日志配置代码。</li><li><strong>根据模块名称自动命名日志文件</strong>：每个模块的日志存储在独立的文件中，便于管理和分析。</li><li><strong>日志轮转</strong>：使用 <code>loguru</code> 的日志轮转功能，自动管理日志文件的大小和数量。</li><li><strong>日志级别控制</strong>：在开发环境中使用 <code>DEBUG</code> 级别，在生产环境中使用 <code>INFO</code> 或 <code>WARNING</code> 级别。</li><li><strong>日志格式化</strong>：使用统一的日志格式，便于阅读和分析。</li><li><strong>日志安全</strong>：避免记录敏感信息，如密码、密钥等。</li></ol><hr><h2 id="3-示例项目结构"><a href="#3-示例项目结构" class="headerlink" title="3. 示例项目结构"></a>3. 示例项目结构</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus">my_project/<br>├── <span class="hljs-selector-tag">main</span><span class="hljs-selector-class">.py</span><br>├── module_a<span class="hljs-selector-class">.py</span><br>├── module_b<span class="hljs-selector-class">.py</span><br>├── logger_config<span class="hljs-selector-class">.py</span><br>└── logs/<br>    ├── module_a<span class="hljs-selector-class">.log</span><br>    ├── module_b<span class="hljs-selector-class">.log</span><br>    └── app.log<br></code></pre></td></tr></table></figure><hr><h2 id="4-安装-loguru"><a href="#4-安装-loguru" class="headerlink" title="4. 安装 loguru"></a>4. 安装 <code>loguru</code></h2><p>首先，确保你已经安装了 <code>loguru</code> 库。如果没有安装，可以使用以下命令进行安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install loguru<br></code></pre></td></tr></table></figure><hr><h2 id="5-统一的日志配置函数"><a href="#5-统一的日志配置函数" class="headerlink" title="5. 统一的日志配置函数"></a>5. 统一的日志配置函数</h2><p>为了减少重复代码，我们可以在一个单独的模块中定义一个统一的日志配置函数 <code>setup_logger</code>，并在每个模块中调用它。</p><h3 id="logger-config-py"><a href="#logger-config-py" class="headerlink" title="logger_config.py"></a><code>logger_config.py</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> loguru <span class="hljs-keyword">import</span> logger<br><span class="hljs-keyword">import</span> sys<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_logger</span>(<span class="hljs-params">module_name, log_dir=<span class="hljs-string">&quot;logs&quot;</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据模块名称配置日志记录器，并自动生成日志文件名。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    :param module_name: 模块名称，用于生成日志文件名</span><br><span class="hljs-string">    :param log_dir: 日志文件存储目录，默认为 &quot;logs&quot;</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 配置日志格式</span><br>    log_format = (<br>        <span class="hljs-string">&quot;&lt;green&gt;&#123;time:YYYY-MM-DD HH:mm:ss&#125;&lt;/green&gt; | &quot;</span><br>        <span class="hljs-string">&quot;&lt;level&gt;&#123;level: &lt;8&#125;&lt;/level&gt; | &quot;</span><br>        <span class="hljs-string">&quot;&lt;cyan&gt;&#123;name&#125;&lt;/cyan&gt;:&lt;cyan&gt;&#123;function&#125;&lt;/cyan&gt;:&lt;cyan&gt;&#123;line&#125;&lt;/cyan&gt; - &lt;level&gt;&#123;message&#125;&lt;/level&gt;&quot;</span><br>    )<br><br>    <span class="hljs-comment"># 配置控制台日志</span><br>    logger.remove()  <span class="hljs-comment"># 移除默认的日志处理器</span><br>    logger.add(sys.stdout, <span class="hljs-built_in">format</span>=log_format, level=<span class="hljs-string">&quot;INFO&quot;</span>)<br><br>    <span class="hljs-comment"># 配置文件日志（根据模块名称生成日志文件名）</span><br>    log_file = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;log_dir&#125;</span>/<span class="hljs-subst">&#123;module_name&#125;</span>.log&quot;</span><br>    logger.add(<br>        log_file,  <span class="hljs-comment"># 日志文件路径</span><br>        <span class="hljs-built_in">format</span>=log_format,<br>        level=<span class="hljs-string">&quot;DEBUG&quot;</span>,<br>        rotation=<span class="hljs-string">&quot;10 MB&quot;</span>,  <span class="hljs-comment"># 日志文件轮转大小</span><br>        retention=<span class="hljs-string">&quot;7 days&quot;</span>,  <span class="hljs-comment"># 保留日志文件的时间</span><br>        compression=<span class="hljs-string">&quot;zip&quot;</span>,  <span class="hljs-comment"># 日志文件压缩格式</span><br>    )<br><br>    <span class="hljs-keyword">return</span> logger<br></code></pre></td></tr></table></figure><hr><h2 id="6-主模块：main-py"><a href="#6-主模块：main-py" class="headerlink" title="6. 主模块：main.py"></a>6. 主模块：<code>main.py</code></h2><p>在主模块中，我们调用 <code>setup_logger</code> 函数来配置日志记录器，并记录日志。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> logger_config <span class="hljs-keyword">import</span> setup_logger<br><span class="hljs-keyword">import</span> module_a<br><span class="hljs-keyword">import</span> module_b<br><br><span class="hljs-comment"># 配置主模块的日志记录器</span><br>logger = setup_logger(<span class="hljs-string">&quot;main&quot;</span>)<br><br><span class="hljs-comment"># 记录日志</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    logger.info(<span class="hljs-string">&quot;主模块启动&quot;</span>)<br>    module_a.run()<br>    module_b.run()<br>    logger.info(<span class="hljs-string">&quot;主模块结束&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure><hr><h2 id="7-模块-A：module-a-py"><a href="#7-模块-A：module-a-py" class="headerlink" title="7. 模块 A：module_a.py"></a>7. 模块 A：<code>module_a.py</code></h2><p>在模块 A 中，我们同样调用 <code>setup_logger</code> 函数来配置日志记录器，并记录日志。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> logger_config <span class="hljs-keyword">import</span> setup_logger<br><br><span class="hljs-comment"># 配置模块 A 的日志记录器</span><br>logger = setup_logger(<span class="hljs-string">&quot;module_a&quot;</span>)<br><br><span class="hljs-comment"># 记录日志</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>():<br>    logger.debug(<span class="hljs-string">&quot;模块 A 的调试信息&quot;</span>)<br>    logger.info(<span class="hljs-string">&quot;模块 A 的普通信息&quot;</span>)<br>    logger.warning(<span class="hljs-string">&quot;模块 A 的警告信息&quot;</span>)<br>    logger.error(<span class="hljs-string">&quot;模块 A 的错误信息&quot;</span>)<br>    logger.critical(<span class="hljs-string">&quot;模块 A 的严重错误信息&quot;</span>)<br></code></pre></td></tr></table></figure><hr><h2 id="8-模块-B：module-b-py"><a href="#8-模块-B：module-b-py" class="headerlink" title="8. 模块 B：module_b.py"></a>8. 模块 B：<code>module_b.py</code></h2><p>在模块 B 中，我们同样调用 <code>setup_logger</code> 函数来配置日志记录器，并记录日志。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> logger_config <span class="hljs-keyword">import</span> setup_logger<br><br><span class="hljs-comment"># 配置模块 B 的日志记录器</span><br>logger = setup_logger(<span class="hljs-string">&quot;module_b&quot;</span>)<br><br><span class="hljs-comment"># 记录日志</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>():<br>    logger.debug(<span class="hljs-string">&quot;模块 B 的调试信息&quot;</span>)<br>    logger.info(<span class="hljs-string">&quot;模块 B 的普通信息&quot;</span>)<br>    logger.warning(<span class="hljs-string">&quot;模块 B 的警告信息&quot;</span>)<br>    logger.error(<span class="hljs-string">&quot;模块 B 的错误信息&quot;</span>)<br>    logger.critical(<span class="hljs-string">&quot;模块 B 的严重错误信息&quot;</span>)<br></code></pre></td></tr></table></figure><hr><h2 id="9-运行结果"><a href="#9-运行结果" class="headerlink" title="9. 运行结果"></a>9. 运行结果</h2><h3 id="控制台输出"><a href="#控制台输出" class="headerlink" title="控制台输出"></a>控制台输出</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | INFO     | main:main:<span class="hljs-number">10</span> - 主模块启动<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | INFO     | module_a:run:<span class="hljs-number">6</span> - 模块 A 的普通信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | WARNING  | module_a:run:<span class="hljs-number">8</span> - 模块 A 的警告信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | ERROR    | module_a:run:<span class="hljs-number">10</span> - 模块 A 的错误信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | CRITICAL | module_a:run:<span class="hljs-number">12</span> - 模块 A 的严重错误信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | INFO     | module_b:run:<span class="hljs-number">6</span> - 模块 B 的普通信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | WARNING  | module_b:run:<span class="hljs-number">8</span> - 模块 B 的警告信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | ERROR    | module_b:run:<span class="hljs-number">10</span> - 模块 B 的错误信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | CRITICAL | module_b:run:<span class="hljs-number">12</span> - 模块 B 的严重错误信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | INFO     | main:main:<span class="hljs-number">14</span> - 主模块结束<br></code></pre></td></tr></table></figure><h3 id="日志文件内容"><a href="#日志文件内容" class="headerlink" title="日志文件内容"></a>日志文件内容</h3><h4 id="logs-app-log（主模块日志）："><a href="#logs-app-log（主模块日志）：" class="headerlink" title="logs/app.log（主模块日志）："></a><code>logs/app.log</code>（主模块日志）：</h4><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | INFO     | main:main:<span class="hljs-number">10</span> - 主模块启动<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | INFO     | main:main:<span class="hljs-number">14</span> - 主模块结束<br></code></pre></td></tr></table></figure><h4 id="logs-module-a-log（模块-A-日志）："><a href="#logs-module-a-log（模块-A-日志）：" class="headerlink" title="logs/module_a.log（模块 A 日志）："></a><code>logs/module_a.log</code>（模块 A 日志）：</h4><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | DEBUG    | module_a:run:<span class="hljs-number">4</span> - 模块 A 的调试信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | INFO     | module_a:run:<span class="hljs-number">6</span> - 模块 A 的普通信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | WARNING  | module_a:run:<span class="hljs-number">8</span> - 模块 A 的警告信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | ERROR    | module_a:run:<span class="hljs-number">10</span> - 模块 A 的错误信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | CRITICAL | module_a:run:<span class="hljs-number">12</span> - 模块 A 的严重错误信息<br></code></pre></td></tr></table></figure><h4 id="logs-module-b-log（模块-B-日志）："><a href="#logs-module-b-log（模块-B-日志）：" class="headerlink" title="logs/module_b.log（模块 B 日志）："></a><code>logs/module_b.log</code>（模块 B 日志）：</h4><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | DEBUG    | module_b:run:<span class="hljs-number">4</span> - 模块 B 的调试信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | INFO     | module_b:run:<span class="hljs-number">6</span> - 模块 B 的普通信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | WARNING  | module_b:run:<span class="hljs-number">8</span> - 模块 B 的警告信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | ERROR    | module_b:run:<span class="hljs-number">10</span> - 模块 B 的错误信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> | CRITICAL | module_b:run:<span class="hljs-number">12</span> - 模块 B 的严重错误信息<br></code></pre></td></tr></table></figure><hr><h2 id="10-总结"><a href="#10-总结" class="headerlink" title="10. 总结"></a>10. 总结</h2><p>通过统一的日志配置函数 <code>setup_logger</code>，我们实现了以下目标：</p><ol><li><strong>自动生成日志文件名</strong>：根据模块名称动态生成日志文件名，避免手动指定文件名。</li><li><strong>减少重复代码</strong>：在每个模块中只需调用 <code>setup_logger</code> 函数，无需重复编写日志配置代码。</li><li><strong>灵活的日志管理</strong>：每个模块的日志存储在独立的文件中，便于管理和分析。</li><li><strong>日志轮转</strong>：通过 <code>loguru</code> 的日志轮转功能，自动管理日志文件的大小和数量。</li><li><strong>日志级别控制</strong>：在开发环境中使用 <code>DEBUG</code> 级别，在生产环境中使用 <code>INFO</code> 或 <code>WARNING</code> 级别。</li><li><strong>日志格式化</strong>：使用统一的日志格式，便于阅读和分析。</li><li><strong>日志安全</strong>：避免记录敏感信息，如密码、密钥等。</li></ol><p><code>loguru</code> 的简洁性和强大功能使其成为 Python 日志记录的首选工具。希望本文对你在实际项目中使用 <code>loguru</code> 有所帮助！如果你有任何问题或需要进一步的帮助，请随时留言。</p>]]></content>
    
    
    <categories>
      
      <category>开发</category>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MiniCPM详解</title>
    <link href="/2024/12/20/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB14%EF%BC%9AMiniCPM%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90/"/>
    <url>/2024/12/20/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB14%EF%BC%9AMiniCPM%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>“MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies” 由清华大学和 Modelbest Inc. 的众多研究人员共同撰写，介绍了 MiniCPM 系列小型语言模型，包括模型架构、训练方法、实验结果等，展现了其在模型和数据维度的可扩展性，以及在小型语言模型中的优势。</p><h2 id="1-研究背景"><a href="#1-研究背景" class="headerlink" title="1. 研究背景"></a>1. 研究背景</h2><h3 id="1-1-之前研究存在的问题"><a href="#1-1-之前研究存在的问题" class="headerlink" title="1.1 之前研究存在的问题"></a>1.1 之前研究存在的问题</h3><p>近年来，大型语言模型（LLMs）如GPT-3、PaLM等在自然语言处理领域取得了显著的进展。这些模型通常拥有数十亿甚至上万亿的参数，能够处理复杂的任务并展现出强大的泛化能力。然而，训练和部署这些大型模型面临着巨大的资源消耗和成本问题。例如，训练一个万亿参数的模型需要大量的计算资源和能源，且在实际应用中，这些模型的部署也面临着效率和可行性的挑战。</p><h3 id="1-2-研究难点"><a href="#1-2-研究难点" class="headerlink" title="1.2 研究难点"></a>1.2 研究难点</h3><p>尽管大型模型的性能令人印象深刻，但其高昂的训练成本和资源消耗使得许多研究者和企业难以负担。此外，这些模型在实际应用中的部署也面临着诸多限制，尤其是在资源受限的设备上，如个人电脑或智能手机。因此，如何在不牺牲性能的前提下，开发出资源效率更高的小型语言模型（SLMs）成为了当前研究的一个重要方向。</p><h3 id="1-3-相关工作总结"><a href="#1-3-相关工作总结" class="headerlink" title="1.3 相关工作总结"></a>1.3 相关工作总结</h3><p>近年来，小型语言模型（SLMs）的研究逐渐受到关注。一些研究者提出了通过数据优化、模型剪枝和架构重构等方法来提升SLMs的性能。例如，Phi系列、TinyLlama、MobileLLM等模型通过高质量数据和结构优化，展示了SLMs在特定任务上的潜力。然而，这些模型在综合能力上仍然难以与大型模型相媲美，且缺乏透明和可扩展的训练方法。</p><h2 id="2-研究方法"><a href="#2-研究方法" class="headerlink" title="2. 研究方法"></a>2. 研究方法</h2><h3 id="2-1-模型风洞实验"><a href="#2-1-模型风洞实验" class="headerlink" title="2.1 模型风洞实验"></a>2.1 模型风洞实验</h3><h4 id="Scaling-Hyper-parameters-Invariant-LM"><a href="#Scaling-Hyper-parameters-Invariant-LM" class="headerlink" title="Scaling Hyper-parameters Invariant LM"></a>Scaling Hyper-parameters Invariant LM</h4><p>Scaling Hyper-parameters Invariant LM（缩放超参数不变的语言模型）是一种旨在通过调整模型的超参数来实现不同规模模型的性能稳定的方法。这种方法的核心思想是利用Tensor Program框架中的宽度缩放和深度缩放技术，以预测大型语言模型（LLMs）的损失，并确保在不同规模的模型上获得最佳的学习率。</p><p>在MiniCPM中，作者采用了Tensor Program的这两种缩放技术：</p><ol><li><p><strong>宽度缩放（Width Scaling）</strong>：这种技术通过调整模型的隐藏层维度来改变模型的宽度。在MiniCPM中，宽度缩放被应用于所有模型，包括MiniCPM-1.2B和MiniCPM-2.4B。</p></li><li><p><strong>深度缩放（Depth Scaling）</strong>：这种技术通过增加模型的层数来改变模型的深度。尽管Yang等人在2023年的研究中观察到，当网络块深度大于2时，深度缩放的效果不理想，但作者在实践中发现，对于MiniCPM模型，深度缩放仍然能够带来稳定的学习率。</p></li></ol><p>此外，作者没有采用注意力softmax缩放技术，因为他们在实践中发现，即使不使用这种技术，模型的学习率仍然是稳定的。</p><p>通过这些缩放技术，MiniCPM能够在不同规模的模型上实现超参数的稳定性和一致性，从而提高模型的训练效率和性能。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223141035777.png" alt="image-20241223141035777"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223141106997.png" alt="image-20241223141106997"></p><h4 id="最优批量大小"><a href="#最优批量大小" class="headerlink" title="最优批量大小"></a>最优批量大小</h4><p>在”3.2 Optimal Batch Size”部分，论文探讨了批量大小（batch size）对模型训练的影响以及如何确定最佳的批量大小。批量大小决定了模型在每次迭代中处理的数据量，它直接影响模型的收敛速度和计算资源的消耗。以下是该部分的几个主要观点：</p><ol><li><p><strong>批量大小与收敛速度和资源消耗的平衡</strong>：</p><ul><li>如果批量大小过大，会导致大量的数据和计算成本。</li><li>如果批量大小过小，则需要更多的训练步骤，可能导致损失函数下降有限。</li></ul></li><li><p><strong>实验设置</strong>：</p><ul><li>论文在三个不同大小的模型上进行了实验：0.009B、0.03B 和 0.17B。</li><li>每个模型在六个不同的批量大小上进行训练，全局学习率为0.01，使用余弦学习率调度器。</li></ul></li><li><p><strong>观察到的趋势</strong>：</p><ul><li>批量大小与损失之间的关系显示，随着损失的减少，最佳批量大小会增大。</li><li>通过拟合等损失点，发现批量大小与损失之间存在线性关系。</li></ul></li><li><p><strong>公式推导</strong>：</p><ul><li>论文提出了一个公式来描述批量大小（bs）与C4数据集上的损失（L）之间的关系：<br>$$<br>bs&#x3D;\frac{1.21\times10^{9}}{L^{6.24}}<br>$$</li></ul></li></ol><p>总的来说，论文通过实验和分析，展示了如何根据预期的损失来确定最佳批量大小，并提出了一个基于损失预测的批量大小估计方法。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223141919897.png" alt="image-20241223141919897"></p><h4 id="最优学习率"><a href="#最优学习率" class="headerlink" title="最优学习率"></a>最优学习率</h4><p>由于我们使用了张量程序（Yang等人，2022年；2023年），我们预计学习率在模型扩展期间不会发生显著变化。为了验证这一点，我们在0.04B、0.1B、0.3B和0.5B的六个学习率实验中进行测试。在图3中，我们发现尽管模型大小增加了十倍，最优基础学习率2并没有明显变化，保持在大约0.01左右。我们进一步在2.1B的规模上进行简单验证，确认0.01的学习率确实实现了最低损失。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/ffd302f2750f29d6e75e666fa43c51a3-image.png" alt="图3：损失与学习率的关系。在应用张量程序后，学习率的变动变得非常小。"></p><h3 id="2-2-WSD-学习率调度器（LRS）"><a href="#2-2-WSD-学习率调度器（LRS）" class="headerlink" title="2.2 WSD 学习率调度器（LRS）"></a>2.2 WSD 学习率调度器（LRS）</h3><h4 id="分析-Cosine-LRS"><a href="#分析-Cosine-LRS" class="headerlink" title="分析 Cosine LRS"></a>分析 Cosine LRS</h4><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/96d5f382f6d650b0f98f618576276f19-image.png" alt="图4：具有不同周期的余弦学习率调度器。Y轴是C4语料库上的损失。"></p><p>在”4.1 Analysing Cosine LRS”小节中，论文分析了余弦学习率调度器（Cosine LRS）的关键特性和其对模型性能的影响。以下是该小节的主要内容：</p><ol><li><p><strong>余弦学习率调度器的基本原理</strong>：</p><ul><li>余弦学习率调度器（Cosine LRS）是一种常用的学习率调整策略，它在训练过程中逐渐降低学习率，遵循余弦曲线的变化。具体来说，学习率在预热阶段达到最大值后，按照余弦函数的形状逐渐下降。</li></ul></li><li><p><strong>关键超参数</strong>：</p><ul><li>余弦学习率调度器的一个关键超参数是步长 $T$，即余弦函数首次降到最小值的时间点。通常，$T$ 被设置为总训练步数 $S$。</li></ul></li><li><p><strong>实验验证</strong>：</p><ul><li>论文通过在0.036B模型上进行实验，验证了不同 $T$ 值对学习率调度器性能的影响。实验结果表明，当 $T&#x3D;S$ 时，模型的损失最低。这表明在整个训练过程中保持较高的学习率有助于模型找到更好的全局最优解。</li><li>当 $T&lt;S$ 或 $T&gt;S$ 时，模型的性能不如 $T&#x3D;S$ 时的表现。特别是，$T&gt;S$ 会导致性能下降。</li></ul></li><li><p><strong>假设分析</strong>：</p><ul><li>论文提出了两个假设来解释为什么 $T&#x3D;S$ 时余弦学习率调度器表现优异：<ol><li>当 $T&#x3D;S$ 时，高学习率训练的持续时间更长，这有助于模型找到更好的全局最优解。</li><li>当 $T&#x3D;S$ 时，学习率衰减阶段更为彻底，这可能涉及独特的训练动态，使模型能够找到更好的局部最优解。</li></ol></li></ul></li><li><p><strong>实验结果</strong>：</p><ul><li>实验结果显示，在 $T&#x3D;S$ 时，余弦学习率调度器的性能最佳。具体来说，当训练步数为 $S&#x3D;20N, 40N, 60N, 80N$ 时，损失总是由 $\operatorname{Cosine}(T)$ 达到最低，而不是 $\operatorname{Cosine}(T)$ 或 $\operatorname{CosineLoop}(T)$。</li></ul></li></ol><p>通过这些分析，论文强调了余弦学习率调度器在特定条件下（即 $T&#x3D;S$）的优越性，并为后续提出的WSD（Warmup-Stable-Decay）学习率调度器奠定了基础。</p><h4 id="提出-WSD-LRS"><a href="#提出-WSD-LRS" class="headerlink" title="提出 WSD LRS"></a>提出 WSD LRS</h4><p>在”4.2 WSD LRS”小节中，论文提出了一种新的学习率调度器，称为Warmup-Stable-Decay (WSD) 学习率调度器。以下是该小节的主要内容：</p><h5 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h5><ul><li><strong>背景</strong>：现有的学习率调度器（如Cosine LRS）在训练过程中逐渐降低学习率，但在某些情况下，可能需要更灵活的调度策略来优化模型性能。</li><li><strong>目标</strong>：提出一种新的学习率调度器，能够在训练的不同阶段有效地调整学习率，以提高模型的训练效率和性能。</li></ul><h5 id="WSD-LRS的定义"><a href="#WSD-LRS的定义" class="headerlink" title="WSD LRS的定义"></a>WSD LRS的定义</h5><ul><li><strong>定义</strong>：WSD LRS将训练过程分为三个阶段：预热阶段（Warmup）、稳定阶段（Stable）和衰减阶段（Decay）。</li><li><strong>公式</strong>：<br>$$<br>WSD(T; s)&#x3D;\left{\begin{array}{l}<br>\frac{s}{W}\eta,\quad s&lt;W\<br>\eta,\quad W&lt;s&lt;T\<br>f(s-T)\eta,\quad T&lt;s&lt;S<br>\end{array}\right.<br>$$<br>其中，$0&lt;f(s-T)\leq 1$ 是一个关于 $s$ 的递减函数，$\eta$ 是最大学习率。</li></ul><h5 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h5><ul><li><strong>损失下降</strong>：在0.036B模型上使用WSD LRS进行实验，结果显示在衰减阶段，随着学习率的降低，损失显著下降，并且很快达到或低于Cosine LRS在步长 $T&#x3D;S$ 时的表现。</li></ul><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223143810882.png" alt="image-20241223143810882"></p><ul><li><strong>快速测试</strong>：缩短衰减阶段可以加快不同模型检查点的快速测试。实验表明，使用10%的总标记数进行衰减足以达到最佳效果。</li></ul><h5 id="数据扩展的有效性"><a href="#数据扩展的有效性" class="headerlink" title="数据扩展的有效性"></a>数据扩展的有效性</h5><ul><li><strong>连续训练</strong>：使用WSD LRS可以持续训练固定大小的模型到极端收敛。实验比较了连续训练0.036B模型和使用40N数据的0.17B模型的性能，结果显示0.036B模型在增加约4倍的训练计算量的情况下，可以达到与0.17B模型相当的性能，同时节省了约5倍的推理计算量。</li></ul><h4 id="衰减阶段的分析"><a href="#衰减阶段的分析" class="headerlink" title="衰减阶段的分析"></a>衰减阶段的分析</h4><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223144222463.png" alt="image-20241223144222463"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223144246606.png" alt="image-20241223144246606"></p><p>在”4.4 Analysis of the Decay Stage”部分，论文对MiniCPM-2.4B模型在衰减阶段的训练过程进行了详细分析。以下是该部分的几个主要分析点：</p><ol><li><p><strong>权重更新与损失的关系</strong>：</p><ul><li>研究者计算了MiniCPM-2.4B模型中所有权重矩阵的最大权重元素更新。结果显示，权重更新与学习率的幅度之间存在强相关性。具体来说，在学习率衰减之前，模型检查点经历了显著的权重更新，但损失几乎没有减少。相反，在衰减阶段，尽管权重更新的幅度较小，损失却迅速下降。</li></ul></li><li><p><strong>梯度信息的分析</strong>：</p><ul><li>研究者记录了0.2B模型的每一步梯度信息，并评估了连续步骤之间的差异，以近似二阶梯度信息。结果显示，梯度范数在衰减阶段减小，梯度之间的角度余弦值主要为正值，表明在衰减阶段，模型参数在每一步都是一致变化的。</li><li>一阶梯度在每一步显著减小，与学习率的变化密切相关。二阶梯度的幅度略有增加，表明损失函数的曲率增大，接近局部最优解。</li></ul></li><li><p><strong>优化过程的几何解释</strong>：</p><ul><li>研究者将优化过程视为在高维流形上的轨迹，并计算了一阶和二阶梯向导数。结果显示，一阶梯向导数与学习率成指数衰减，而二阶梯向导数的幅度略有增加。这表明在衰减阶段，模型参数的变化更加一致，损失函数的曲率增大，接近局部最优解。</li></ul></li><li><p><strong>实验结果的可视化</strong>：</p><ul><li>图7展示了权重更新的最大差异，图8展示了梯度统计信息。这些图表直观地展示了在衰减阶段，尽管权重更新的幅度较小，但损失迅速下降的现象。</li></ul></li></ol><p>通过这些分析，研究者揭示了在衰减阶段，MiniCPM-2.4B模型的训练动态具有独特的特征，这些特征有助于模型更快地收敛到局部最优解。这些发现为进一步优化模型训练过程提供了重要的见解。</p><h4 id="缩放定律的测量"><a href="#缩放定律的测量" class="headerlink" title="缩放定律的测量"></a>缩放定律的测量</h4><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223144614364.png" alt="image-20241223144614364"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223144647906.png" alt="image-20241223144647906"></p><p>在”4.5 Measuring the Scaling Law with WSD LRS”部分，论文介绍了如何利用WSD（Warmup-Stable-Decay）学习率调度器来高效地研究大规模语言模型（LLMs）的规模定律。以下是该部分的详细介绍和分析：</p><h5 id="1-规模定律的重要性"><a href="#1-规模定律的重要性" class="headerlink" title="1. 规模定律的重要性"></a>1. 规模定律的重要性</h5><p>规模定律是指导LLMs发展的重要原则，它描述了模型规模和数据规模之间的关系。Kaplan等人（2020）和Hoffmann等人（2022）分别提出了不同的规模定律模型，前者认为模型规模增加十倍应对应数据规模增加一倍，而后者则认为两者应同比例增加。</p><h5 id="2-传统规模实验的挑战"><a href="#2-传统规模实验的挑战" class="headerlink" title="2. 传统规模实验的挑战"></a>2. 传统规模实验的挑战</h5><p>传统的规模实验需要在不同模型规模和数据规模上进行多次训练，计算成本高昂，时间复杂度为$O(m^2 C)$，其中$m$是模型数量，$C$是每次训练的成本。</p><h5 id="3-WSD调度器的优势"><a href="#3-WSD调度器的优势" class="headerlink" title="3. WSD调度器的优势"></a>3. WSD调度器的优势</h5><p>WSD调度器允许在稳定阶段的任意检查点开始衰减，从而在不需要从头开始训练的情况下，精确测量规模定律。这使得规模定律的测量在数据轴上具有线性成本$O(m C)$，大大提高了效率。</p><h5 id="4-实验设计"><a href="#4-实验设计" class="headerlink" title="4. 实验设计"></a>4. 实验设计</h5><ul><li><strong>模型和数据规模</strong>：研究者在6个不同规模的模型（从0.04B到2B）上进行了实验，每个模型在稳定阶段的6个不同检查点（从10N到60N数据）开始衰减。</li><li><strong>训练和评估</strong>：每个模型在五个独立的评估数据集上进行最终损失评估。为了公平比较，损失按字节数而非标记数平均。</li></ul><h5 id="5-规模定律的拟合"><a href="#5-规模定律的拟合" class="headerlink" title="5. 规模定律的拟合"></a>5. 规模定律的拟合</h5><ul><li><strong>拟合公式</strong>：研究者使用Hoffmann等人（2022）提出的公式来拟合损失与模型规模和数据规模的关系：<br>$$<br>L(N, D) &#x3D; C_N N^{-\alpha} + C_D D^{-\beta} + L_0<br>$$<br>其中，$N$和$D$分别是模型规模和数据规模，$C_N$和$C_D$是常数，$\alpha$和$\beta$是指数，$L_0$是常数项。</li><li><strong>最优模型和数据规模</strong>：通过拟合，研究者得到了每个数据集和检查点的最优模型规模$N_{\text{opt}}$和数据规模$D_{\text{opt}}$，并计算了数据-模型比例$\frac{D_{\text{opt}}}{N_{\text{opt}}}$。</li></ul><h5 id="6-结果分析"><a href="#6-结果分析" class="headerlink" title="6. 结果分析"></a>6. 结果分析</h5><ul><li><strong>数据-模型比例</strong>：研究结果表明，数据-模型比例比Hoffmann等人（2022）的结果高得多，平均约为192倍，而不是20倍。这表明较小的模型可以吸收更多的数据，从而提高推理和部署的效率。</li><li><strong>与Chinchilla Optimal的比较</strong>：尽管与Chinchilla Optimal的结果存在较大差异，但通过与Llama2的比较，研究者认为WSD调度器在更现代的配置下可能具有更高的数据-模型比例。</li></ul><h5 id="7-未来方向"><a href="#7-未来方向" class="headerlink" title="7. 未来方向"></a>7. 未来方向</h5><ul><li><strong>进一步研究</strong>：研究者计划深入分析衰减阶段的损失下降原因，并通过扩大模型和数据规模来增强MiniCPM的能力。</li></ul><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>通过引入WSD调度器，论文提出了一种高效的方法来研究LLMs的规模定律，显著降低了计算成本。研究结果表明，较小的模型可以吸收更多的数据，从而提高推理和部署的效率。未来的研究将进一步探索WSD调度器在其他模型上的应用及其对规模定律的影响。</p><h2 id="3-两阶段预训练策略"><a href="#3-两阶段预训练策略" class="headerlink" title="3. 两阶段预训练策略"></a>3. 两阶段预训练策略</h2><p>论文提出了一种两阶段的预训练策略，旨在提高小型语言模型（SLMs）的性能。以下是对该章节的详细总结和分析：</p><h3 id="3-1-背景与动机"><a href="#3-1-背景与动机" class="headerlink" title="3.1 背景与动机"></a>3.1 背景与动机</h3><ul><li><strong>背景</strong>：大型语言模型（LLMs）的训练成本高昂，且在个人电脑或智能手机等终端设备上部署效率低下。</li><li><strong>动机</strong>：探索小型语言模型（SLMs）作为资源高效的替代方案，并通过可扩展的训练策略，使其具备与大型模型相似的能力。</li></ul><h3 id="3-2-两阶段预训练策略"><a href="#3-2-两阶段预训练策略" class="headerlink" title="3.2 两阶段预训练策略"></a>3.2 两阶段预训练策略</h3><ul><li><p><strong>阶段划分</strong>：</p><ul><li><strong>第一阶段</strong>：仅使用大规模、低质量的预训练数据进行训练。</li><li><strong>第二阶段</strong>：在衰减阶段引入高质量、知识导向的监督微调（SFT）数据，混合到预训练数据中。</li></ul></li><li><p><strong>优势</strong>：</p><ul><li><strong>全面学习</strong>：在衰减阶段引入高质量数据，促进模型在更接近实际用户场景的数据分布上进行更显著的损失减少。</li><li><strong>持续训练</strong>：避免在整个预训练过程中均匀分布高质量数据，集中资源和持续预训练。</li></ul></li></ul><h3 id="3-3-实验验证"><a href="#3-3-实验验证" class="headerlink" title="3.3 实验验证"></a>3.3 实验验证</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223145718563.png" alt="image-20241223145718563"></p><ul><li><p><strong>实验设置</strong>：</p><ul><li><strong>模型选择</strong>：使用MiniCPM-2.4B和MiniCPM-1.2B模型。</li><li><strong>对比实验</strong>：<ul><li>A-1：2.4B模型，仅使用预训练数据进行衰减，随后进行4B标记的SFT。</li><li>A-2：2.4B模型，在衰减阶段混合高质量数据和SFT数据，随后进行4B标记的SFT。</li><li>B-1：1.2B模型，仅使用预训练数据进行衰减，随后进行6B标记的SFT。</li><li>B-2：1.2B模型，仅使用预训练数据进行衰减，随后进行12B标记的SFT。</li><li>B-3：1.2B模型，在衰减阶段混合高质量数据和SFT数据，随后进行6B标记的SFT。</li></ul></li></ul></li><li><p><strong>结果分析</strong>：</p><ul><li><strong>A-2 vs A-1</strong>：尽管A-2和A-1在SFT阶段使用相同的数据分布，但A-2在衰减阶段引入高质量数据，显著提升了模型性能。</li><li><strong>B-3 vs B-2</strong>：B-3在衰减阶段引入高质量数据，表现优于仅在SFT阶段引入高质量数据的B-2。</li></ul></li></ul><h3 id="3-4-结论"><a href="#3-4-结论" class="headerlink" title="3.4 结论"></a>3.4 结论</h3><ul><li><strong>策略有效性</strong>：引入高质量数据到衰减阶段比仅在SFT阶段引入数据更能提升模型性能。</li><li><strong>推荐</strong>：建议从衰减阶段开始，专门化和增强模型能力。</li></ul><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>论文提出的两阶段预训练策略通过在不同阶段引入不同类型的数据，显著提升了小型语言模型的性能。该策略不仅提高了模型的学习效率，还为未来的大型语言模型开发提供了有价值的参考。</p><h2 id="4-MiniCPM-模型"><a href="#4-MiniCPM-模型" class="headerlink" title="4. MiniCPM 模型"></a>4. MiniCPM 模型</h2><p>第6章节“6 Model”详细介绍了MiniCPM模型的各个方面，包括模型细节、训练阶段、训练数据分布、训练损失和模型评估。以下是各部分的详细介绍：</p><h3 id="4-1-Model-Details"><a href="#4-1-Model-Details" class="headerlink" title="4.1 Model Details"></a>4.1 Model Details</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223153902417.png" alt="image-20241223153902417"></p><h4 id="词汇"><a href="#词汇" class="headerlink" title="词汇"></a>词汇</h4><ul><li><strong>MiniCPM-2.4B</strong>：使用122,753词汇大小的tokenizer。</li><li><strong>MiniCPM-1.2B</strong>：使用73,440词汇大小的tokenizer，较小的词汇表有利于效率而不显著影响性能。</li></ul><h4 id="共享输入输出层"><a href="#共享输入输出层" class="headerlink" title="共享输入输出层"></a>共享输入输出层</h4><ul><li>为了减少参数空间，MiniCPM-2.4B和MiniCPM-1.2B都使用了嵌入共享技术。</li></ul><h4 id="深而薄的网络"><a href="#深而薄的网络" class="headerlink" title="深而薄的网络"></a>深而薄的网络</h4><ul><li><strong>MiniCPM-2.4B</strong>：在训练MiniCPM-1.2B之前，采用了更深更薄的网络架构。</li><li><strong>MiniCPM-1.2B</strong>：进一步加深和变薄，以适应长上下文任务。</li></ul><h4 id="组查询注意力"><a href="#组查询注意力" class="headerlink" title="组查询注意力"></a>组查询注意力</h4><ul><li><strong>MiniCPM-2.4B</strong>：未修改注意力层。</li><li><strong>MiniCPM-1.2B</strong>：应用了组查询注意力（Group Query Attention）以减少参数数量。</li></ul><h3 id="4-2-Training-Stages"><a href="#4-2-Training-Stages" class="headerlink" title="4.2 Training Stages"></a>4.2 Training Stages</h3><h4 id="稳定训练阶段"><a href="#稳定训练阶段" class="headerlink" title="稳定训练阶段"></a>稳定训练阶段</h4><ul><li>使用约1T的数据，主要来自开放数据集。</li><li>使用WSD学习率调度器，批量大小为3.93百万，最大学习率为0.01。</li></ul><h4 id="衰减阶段"><a href="#衰减阶段" class="headerlink" title="衰减阶段"></a>衰减阶段</h4><ul><li>使用预训练数据和高质量的SFT数据混合。</li><li>采用指数衰减形式，T设为5000步（20B标记）。</li></ul><h4 id="SFT阶段"><a href="#SFT阶段" class="headerlink" title="SFT阶段"></a>SFT阶段</h4><ul><li>使用类似衰减阶段的数据，但不包括预训练数据，训练约60亿标记。</li><li>学习率与衰减阶段末尾相同，使用WSD调度器。</li></ul><h3 id="4-3-Training-Data-Distribution"><a href="#4-3-Training-Data-Distribution" class="headerlink" title="4.3 Training Data Distribution"></a>4.3 Training Data Distribution</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223153949999.png" alt="image-20241223153949999"></p><ul><li><strong>稳定阶段</strong>：主要使用开放数据集，如CommonCrawl、Dolma、C4、Pile、Code Pre-train等。</li><li><strong>衰减阶段</strong>：数据混合包含更多样化和专有的数据，如UltraChat、SlimOrca、OssInstruct、EvolInstruct等。</li></ul><h3 id="4-4-Training-Loss"><a href="#4-4-Training-Loss" class="headerlink" title="4.4 Training Loss"></a>4.4 Training Loss</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223154011909.png" alt="image-20241223154011909"></p><ul><li>在C4数据集上的整体训练损失显示，衰减阶段的损失显著下降。</li><li>由于使用指数衰减，学习率降至最大值的10%以下后，损失仍继续下降，但最终检查点未用于微调。</li></ul><h3 id="4-5-Evaluation"><a href="#4-5-Evaluation" class="headerlink" title="4.5 Evaluation"></a>4.5 Evaluation</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223154036251.png" alt="image-20241223154036251"></p><ul><li>使用开源工具UltraEval进行评估，支持多种主流大型模型的性能评估。</li><li>评估数据集包括MMLU、CMMLU、C-Eval、MBPP、GSM8K、MATH、HumanEval、BBH等。</li><li>评估方法包括标准化输入提示和调整输入输出模板，确保公平比较。</li></ul><h4 id="评估结果"><a href="#评估结果" class="headerlink" title="评估结果"></a>评估结果</h4><ul><li><strong>MiniCPM-2.4B</strong>：在多个基准测试中表现优异，特别是在中文任务中表现优于Mistral-7B。</li><li><strong>MiniCPM-1.2B</strong>：在多个基准测试中也表现出色，特别是在直接生成任务中表现优于PPL测试。</li></ul><p>通过这些详细的介绍，可以看出MiniCPM模型在设计、训练和评估方面都经过了精心考虑和优化，取得了显著的性能提升。</p><h2 id="5-MiniCPM-家族模型"><a href="#5-MiniCPM-家族模型" class="headerlink" title="5. MiniCPM 家族模型"></a>5. MiniCPM 家族模型</h2><p>论文介绍了基于MiniCPM基础模型的几种扩展模型，包括MiniCPM-DPO、MiniCPM-128K和MiniCPM-MoE。这些模型在各自的应用领域中展示了卓越的性能。以下是该章节的详细介绍：</p><h3 id="5-1-MiniCPM-DPO"><a href="#5-1-MiniCPM-DPO" class="headerlink" title="5.1 MiniCPM-DPO"></a>5.1 MiniCPM-DPO</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223154302859.png" alt="image-20241223154302859"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223154345626.png" alt="image-20241223154345626"></p><ul><li><strong>背景</strong>：在微调阶段之后，使用DPO（Direct Preference Optimization）进行人类偏好对齐。</li><li><strong>训练</strong>：使用UltraFeedback作为主要对齐数据集，并构建了一个专有的偏好数据集以增强模型的代码和数学能力。</li><li><strong>结果</strong>：在MTBench上的得分从SFT后的6.89提高到7.25，超过了像Llama2-70B-Chat这样的大型模型。然而，基准测试结果略有下降，这是对齐税（alignment tax）的表现。</li></ul><h3 id="5-2-MiniCPM-128K"><a href="#5-2-MiniCPM-128K" class="headerlink" title="5.2 MiniCPM-128K"></a>5.2 MiniCPM-128K</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223154425567.png" alt="image-20241223154425567"></p><ul><li><strong>背景</strong>：处理长上下文任务需要模型能够隐含地理解长文本中的信息。</li><li><strong>初始化</strong>：禁用输入和输出之间的嵌入共享，以适应长上下文训练所需的词汇并行性。</li><li><strong>训练</strong>：使用WSD学习率调度器，训练数据分为“短数据”和“长数据”，长数据占44%，短数据占56%。使用Adjusted Base Frequency (ABF)和NTK-Aware RoPE Scaling进行扩展。</li><li><strong>评估</strong>：在∞Bench基准测试中表现优异，特别是在长上下文推理任务中，超越了ChatGLM3-6B-128K。</li></ul><h3 id="5-3-MiniCPM-MoE"><a href="#5-3-MiniCPM-MoE" class="headerlink" title="5.3 MiniCPM-MoE"></a>5.3 MiniCPM-MoE</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241223154445784.png" alt="image-20241223154445784"></p><ul><li><strong>背景</strong>：通过混合专家（MoE）技术扩展MiniCPM的能力。</li><li><strong>初始化</strong>：使用Sparse Upcycling进行初始化，将密集模型的每个MLP层替换为MoE层，路由参数随机初始化。</li><li><strong>训练</strong>：使用WSD学习率调度器，训练批量大小在稳定和衰减阶段为4M，在SFT阶段减少到2M。</li><li><strong>结果</strong>：在多个基准测试中表现优异，特别是在C-Eval、CMMLU、MMLU、HumanEval、MBPP、GSM8K和MATH等任务上。</li></ul><p>MiniCPM家族的扩展模型在各自的领域中展示了卓越的性能，证明了MiniCPM在多样化的SLM应用中的潜力。未来的研究方向包括深入分析衰减阶段的损失下降，并通过扩展模型规模和数据规模来增强MiniCPM的能力。</p><h2 id="6-研究结论"><a href="#6-研究结论" class="headerlink" title="6. 研究结论"></a>6. 研究结论</h2><ul><li>MiniCPM 系列模型展示了小型语言模型在资源效率和性能上的巨大潜力。通过创新的训练方法和架构设计，MiniCPM 不仅在小型模型中表现出色，还能够与大型模型相媲美。WSD 学习率调度器的引入为模型的持续训练和数据-模型缩放规律的研究提供了新的思路。此外，MiniCPM 家族的多样化模型进一步巩固了其在不同应用场景中的基础地位。</li></ul><h2 id="7-论文研究的不足"><a href="#7-论文研究的不足" class="headerlink" title="7. 论文研究的不足"></a>7. 论文研究的不足</h2><p>尽管 MiniCPM 在小型语言模型的研究中取得了显著进展，但仍存在一些不足之处：</p><ol><li><strong>未验证在大型模型上的应用</strong>：论文主要集中在小型模型的研究上，尚未验证 WSD 调度器在大型模型上的应用效果。</li><li><strong>缺乏对损失下降机制的深入分析</strong>：尽管 WSD 调度器在 Decay 阶段表现出色，但其背后的机制尚未得到深入分析，未来需要进一步研究。</li><li><strong>未考虑实际部署中的优化</strong>：虽然 MiniCPM 在资源效率上表现出色，但在实际部署中，仍需进一步优化以适应不同的硬件环境。</li></ol><h2 id="8-论文评价"><a href="#8-论文评价" class="headerlink" title="8.论文评价"></a>8.论文评价</h2><h3 id="优点与创新"><a href="#优点与创新" class="headerlink" title="优点与创新"></a>优点与创新</h3><ol><li><strong>MiniCPM模型系列</strong>：论文介绍了MiniCPM系列小型语言模型，包括1.2B和2.4B非嵌入参数变体，这些模型在各自的小规模类别中表现卓越，并且与7B-13B大型语言模型的能力相当。</li><li><strong>可扩展性</strong>：研究展示了在模型和数据维度上的可扩展性，为未来的大型语言模型（LLM）研究提供了潜力。</li><li><strong>模型风洞实验</strong>：通过广泛的模型风洞实验，确保了稳定和最优的模型扩展。</li><li><strong>温暖的稳定衰减（WSD）学习率调度器（LRS）</strong>：引入了WSD LRS，有利于连续训练和领域适应，并能够高效地研究数据-模型扩展规律。</li><li><strong>训练动态分析</strong>：对WSD LRS的训练动态进行了深入分析，揭示了模型预训练的有趣损失景观。</li><li><strong>更高的计算最优数据-模型比率</strong>：通过WSD LRS，能够在模型轴上线性努力，在数据轴上忽略不计的努力，从而得出比Chinchilla Optimal更高的计算最优数据-模型比率。</li><li><strong>MiniCPM家族</strong>：介绍了MiniCPM家族，包括MiniCPM-DPO、MiniCPM-MoE和MiniCPM-128K，进一步巩固了MiniCPM在多样化小型语言模型应用中的基础。</li><li><strong>公开可用</strong>：MiniCPM模型系列公开发布，便于其他研究人员使用和改进。</li></ol><h3 id="不足与反思"><a href="#不足与反思" class="headerlink" title="不足与反思"></a>不足与反思</h3><ol><li><strong>未扩展到大型语言模型</strong>：尽管研究了小型语言模型的扩展规律，但论文并未扩展到训练大型语言模型以验证扩展规律。</li><li><strong>WSD LRS在大型语言模型上的应用未完全探索</strong>：尽管对WSD LRS在大型语言模型上的潜在优势保持乐观，但目前尚未完全探索其在大型语言模型上的应用。</li></ol><h2 id="9-关键问题及回答"><a href="#9-关键问题及回答" class="headerlink" title="9. 关键问题及回答"></a>9. 关键问题及回答</h2><h3 id="问题1：MiniCPM模型在模型风洞实验中是如何进行超参数优化的？"><a href="#问题1：MiniCPM模型在模型风洞实验中是如何进行超参数优化的？" class="headerlink" title="问题1：MiniCPM模型在模型风洞实验中是如何进行超参数优化的？"></a><strong>问题1：MiniCPM模型在模型风洞实验中是如何进行超参数优化的？</strong></h3><p>MiniCPM模型在模型风洞实验中进行了广泛的超参数优化，主要包括以下几个方面：</p><ol><li><strong>宽度缩放</strong>：使用Tensor Program技术进行宽度缩放，支持CerebrasGPT等模型在不同模型规模下的超参数一致性。</li><li><strong>深度缩放</strong>：同样使用Tensor Program技术进行深度缩放，尽管对于深度大于2的网络，深度缩放的效果不如预期，但实验结果显示最优学习率是稳定的。</li><li><strong>学习率调度器</strong>：引入了Warmup-Stable-Decay（WSD）学习率调度器，适用于连续训练和领域适应。WSD调度器将训练阶段明确分为高学习率阶段和衰减阶段。</li></ol><p>通过这些超参数优化技术，MiniCPM模型能够在不同模型规模下实现稳定的训练和优化。</p><h3 id="问题2：Warmup-Stable-Decay（WSD）学习率调度器在MiniCPM模型中的具体实现和效果如何？"><a href="#问题2：Warmup-Stable-Decay（WSD）学习率调度器在MiniCPM模型中的具体实现和效果如何？" class="headerlink" title="问题2：Warmup-Stable-Decay（WSD）学习率调度器在MiniCPM模型中的具体实现和效果如何？"></a><strong>问题2：Warmup-Stable-Decay（WSD）学习率调度器在MiniCPM模型中的具体实现和效果如何？</strong></h3><p>Warmup-Stable-Decay（WSD）学习率调度器在MiniCPM模型中的具体实现如下：</p><ol><li><strong>高学习率阶段</strong>：模型在初始阶段采用较高的学习率进行训练，以便快速找到全局最优解。</li><li><strong>衰减阶段</strong>：当学习率达到一定值后，学习率逐渐减小，直至达到最小值。这一阶段的学习率衰减是线性的，并且在衰减结束后保持恒定。</li><li><strong>连续训练</strong>：WSD调度器允许模型在衰减阶段之后继续进行训练，从而实现连续训练。</li></ol><p>效果方面，WSD调度器在MiniCPM模型中表现出显著的优势：</p><ol><li><strong>损失降低</strong>：在衰减阶段，学习率的突然减小导致损失显著下降，并且损失迅速降至与Cosine LRS相当的水平。</li><li><strong>连续训练</strong>：通过WSD调度器，模型可以在衰减阶段之后继续训练，达到与较大模型相似的性能，同时节省了大量的计算资源。</li><li><strong>数据-模型缩放定律</strong>：WSD调度器使得研究数据-模型缩放定律变得更加高效，能够在模型轴和数据轴上以线性努力进行研究。</li></ol><h3 id="问题3：MiniCPM家族中的其他成员（如MiniCPM-DPO、MiniCPM-128K和MiniCPM-MoE）有哪些具体功能和表现？"><a href="#问题3：MiniCPM家族中的其他成员（如MiniCPM-DPO、MiniCPM-128K和MiniCPM-MoE）有哪些具体功能和表现？" class="headerlink" title="问题3：MiniCPM家族中的其他成员（如MiniCPM-DPO、MiniCPM-128K和MiniCPM-MoE）有哪些具体功能和表现？"></a><strong>问题3：MiniCPM家族中的其他成员（如MiniCPM-DPO、MiniCPM-128K和MiniCPM-MoE）有哪些具体功能和表现？</strong></h3><p>MiniCPM家族中的其他成员包括MiniCPM-DPO、MiniCPM-128K和MiniCPM-MoE，它们各自具有不同的功能和表现：</p><ol><li><strong>MiniCPM-DPO</strong>：在SFT阶段之后，MiniCPM-DPO通过DPO（Direct Preference Optimization）进行人类偏好对齐。使用UltraFeedback作为主要对齐数据集，并构建了一个专有的偏好数据集以增强模型的代码和数学能力。在MTBench基准测试中，MiniCPM-DPO的性能从SFT后的6.89提高到7.25，超过了多个较大的模型。</li><li><strong>MiniCPM-128K</strong>：将MiniCPM-2.4B的上下文长度从4096扩展到128,000 tokens，展示了小型语言模型在处理长上下文任务中的能力。通过调整基础频率和使用NTK-Aware RoPE Scaling等技术，MiniCPM-128K在∞Bench基准测试中取得了与Mistral-7B-Instruct-v0.2相当的结果，尽管其模型规模较小。</li><li><strong>MiniCPM-MoE</strong>：通过引入Mixture-of-Expert（MoE），MiniCPM-MoE进一步扩展了MiniCPM的能力。MoE模型在每个token上激活两个专家，并使用Router参数进行路由。在MBPP基准测试中，MiniCPM-MoE的性能与Llama2-34B相当，表明其在多任务语言理解方面的强大能力。</li></ol><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink">https://github.com/chongzicbo/ReadWriteThink</a></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>multi-modal</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>多模态</tag>
      
      <tag>论文阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>C++ 右值语义详解：从基础到实战</title>
    <link href="/2024/12/19/%E5%BC%80%E5%8F%91/cpp/C++001%EF%BC%9AC++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E8%AF%A6%E8%A7%A3%EF%BC%9A%E4%BB%8E%E5%9F%BA%E7%A1%80%E5%88%B0%E5%AE%9E%E6%88%98/"/>
    <url>/2024/12/19/%E5%BC%80%E5%8F%91/cpp/C++001%EF%BC%9AC++%E5%8F%B3%E5%80%BC%E8%AF%AD%E4%B9%89%E8%AF%A6%E8%A7%A3%EF%BC%9A%E4%BB%8E%E5%9F%BA%E7%A1%80%E5%88%B0%E5%AE%9E%E6%88%98/</url>
    
    <content type="html"><![CDATA[<h1 id="C-右值语义详解：从基础到实战"><a href="#C-右值语义详解：从基础到实战" class="headerlink" title="C++ 右值语义详解：从基础到实战"></a>C++ 右值语义详解：从基础到实战</h1><p>在现代 C++ 中，右值语义是一个非常重要的概念，它涉及到右值引用、移动语义、完美转发等核心特性。本文将结合代码实例，详细讲解与右值语义相关的所有知识点，帮助你全面掌握这一主题。</p><h2 id="1-左值与右值的基本概念"><a href="#1-左值与右值的基本概念" class="headerlink" title="1. 左值与右值的基本概念"></a>1. 左值与右值的基本概念</h2><h3 id="1-1-左值-Lvalue"><a href="#1-1-左值-Lvalue" class="headerlink" title="1.1 左值 (Lvalue)"></a>1.1 左值 (Lvalue)</h3><p>左值是可以取地址的表达式，通常表示一个对象或变量。左值具有持久性，可以被赋值。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-type">int</span> a = <span class="hljs-number">10</span>; <span class="hljs-comment">// &#x27;a&#x27; 是一个左值</span><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Address of a: &quot;</span> &lt;&lt; &amp;a &lt;&lt; std::endl; <span class="hljs-comment">// 可以取地址</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="1-2-右值-Rvalue"><a href="#1-2-右值-Rvalue" class="headerlink" title="1.2 右值 (Rvalue)"></a>1.2 右值 (Rvalue)</h3><p>右值是不能取地址的表达式，通常是<strong>临时对象或字面量</strong>。右值具有<strong>短暂性</strong>，不能被赋值。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-type">int</span>&amp;&amp; r = <span class="hljs-number">42</span>; <span class="hljs-comment">// &#x27;42&#x27; 是一个右值</span><br>    <span class="hljs-comment">// std::cout &lt;&lt; &quot;Address of 42: &quot; &lt;&lt; &amp;42 &lt;&lt; std::endl; // 错误：不能取地址</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="1-3-纯右值-PRvalue-与将亡值-Xvalue"><a href="#1-3-纯右值-PRvalue-与将亡值-Xvalue" class="headerlink" title="1.3 纯右值 (PRvalue) 与将亡值 (Xvalue)"></a>1.3 纯右值 (PRvalue) 与将亡值 (Xvalue)</h3><ul><li><strong>纯右值</strong>：临时对象或字面量，如 <code>42</code>、<code>std::string(&quot;hello&quot;)</code>。</li><li><strong>将亡值</strong>：即将被销毁的对象，通常是右值引用的结果，如 <code>std::move(x)</code>。</li></ul><hr><h2 id="2-右值引用-Rvalue-Reference"><a href="#2-右值引用-Rvalue-Reference" class="headerlink" title="2. 右值引用 (Rvalue Reference)"></a>2. 右值引用 (Rvalue Reference)</h2><h3 id="2-1-右值引用的语法"><a href="#2-1-右值引用的语法" class="headerlink" title="2.1 右值引用的语法"></a>2.1 右值引用的语法</h3><p>右值引用使用 <code>T&amp;&amp;</code> 语法，表示对右值的引用。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-type">int</span>&amp;&amp; r = <span class="hljs-number">42</span>; <span class="hljs-comment">// 右值引用</span><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;r = &quot;</span> &lt;&lt; r &lt;&lt; std::endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="2-2-右值引用的作用"><a href="#2-2-右值引用的作用" class="headerlink" title="2.2 右值引用的作用"></a>2.2 右值引用的作用</h3><p>右值引用主要用于支持移动语义和完美转发。</p><hr><h2 id="3-移动语义-Move-Semantics"><a href="#3-移动语义-Move-Semantics" class="headerlink" title="3. 移动语义 (Move Semantics)"></a>3. 移动语义 (Move Semantics)</h2><h3 id="3-1-移动构造函数-Move-Constructor"><a href="#3-1-移动构造函数-Move-Constructor" class="headerlink" title="3.1 移动构造函数 (Move Constructor)"></a>3.1 移动构造函数 (Move Constructor)</h3><p>移动构造函数接受一个右值引用参数，用于将资源从一个对象“移动”到新对象。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyVector</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; data;<br><br>    <span class="hljs-comment">// 移动构造函数</span><br>    <span class="hljs-built_in">MyVector</span>(MyVector&amp;&amp; other) <span class="hljs-keyword">noexcept</span> : <span class="hljs-built_in">data</span>(std::<span class="hljs-built_in">move</span>(other.data)) &#123;<br>        std::cout &lt;&lt; <span class="hljs-string">&quot;Move Constructor called&quot;</span> &lt;&lt; std::endl;<br>    &#125;<br><br>    <span class="hljs-built_in">MyVector</span>(<span class="hljs-type">const</span> std::vector&lt;<span class="hljs-type">int</span>&gt;&amp; d) : <span class="hljs-built_in">data</span>(d) &#123;&#125;<br>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    MyVector v1&#123;std::vector&lt;<span class="hljs-type">int</span>&gt;&#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>&#125;&#125;;<br>    MyVector v2 = std::<span class="hljs-built_in">move</span>(v1); <span class="hljs-comment">// 调用移动构造函数</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="3-2-移动赋值运算符-Move-Assignment-Operator"><a href="#3-2-移动赋值运算符-Move-Assignment-Operator" class="headerlink" title="3.2 移动赋值运算符 (Move Assignment Operator)"></a>3.2 移动赋值运算符 (Move Assignment Operator)</h3><p>移动赋值运算符接受一个右值引用参数，用于将资源从一个对象“移动”到现有对象。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyVector</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; data;<br><br>    <span class="hljs-comment">// 移动赋值运算符</span><br>    MyVector&amp; <span class="hljs-keyword">operator</span>=(MyVector&amp;&amp; other) <span class="hljs-keyword">noexcept</span> &#123;<br>        <span class="hljs-keyword">if</span> (<span class="hljs-keyword">this</span> != &amp;other) &#123;<br>            data = std::<span class="hljs-built_in">move</span>(other.data);<br>            std::cout &lt;&lt; <span class="hljs-string">&quot;Move Assignment Operator called&quot;</span> &lt;&lt; std::endl;<br>        &#125;<br>        <span class="hljs-keyword">return</span> *<span class="hljs-keyword">this</span>;<br>    &#125;<br><br>    <span class="hljs-built_in">MyVector</span>(<span class="hljs-type">const</span> std::vector&lt;<span class="hljs-type">int</span>&gt;&amp; d) : <span class="hljs-built_in">data</span>(d) &#123;&#125;<br>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    MyVector v1&#123;std::vector&lt;<span class="hljs-type">int</span>&gt;&#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>&#125;&#125;;<br>    MyVector v2&#123;std::vector&lt;<span class="hljs-type">int</span>&gt;&#123;<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>&#125;&#125;;<br>    v2 = std::<span class="hljs-built_in">move</span>(v1); <span class="hljs-comment">// 调用移动赋值运算符</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="3-3-std-move"><a href="#3-3-std-move" class="headerlink" title="3.3 std::move"></a>3.3 <code>std::move</code></h3><p><code>std::move</code> 将一个左值转换为右值引用，以便调用移动构造函数或移动赋值运算符。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyVector</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; data;<br><br>    <span class="hljs-built_in">MyVector</span>(MyVector&amp;&amp; other) <span class="hljs-keyword">noexcept</span> : <span class="hljs-built_in">data</span>(std::<span class="hljs-built_in">move</span>(other.data)) &#123;<br>        std::cout &lt;&lt; <span class="hljs-string">&quot;Move Constructor called&quot;</span> &lt;&lt; std::endl;<br>    &#125;<br><br>    <span class="hljs-built_in">MyVector</span>(<span class="hljs-type">const</span> std::vector&lt;<span class="hljs-type">int</span>&gt;&amp; d) : <span class="hljs-built_in">data</span>(d) &#123;&#125;<br>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    MyVector v1&#123;std::vector&lt;<span class="hljs-type">int</span>&gt;&#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>&#125;&#125;;<br>    MyVector v2 = std::<span class="hljs-built_in">move</span>(v1); <span class="hljs-comment">// 使用 std::move 调用移动构造函数</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><hr><h2 id="4-完美转发-Perfect-Forwarding"><a href="#4-完美转发-Perfect-Forwarding" class="headerlink" title="4. 完美转发 (Perfect Forwarding)"></a>4. 完美转发 (Perfect Forwarding)</h2><h3 id="4-1-问题背景"><a href="#4-1-问题背景" class="headerlink" title="4.1 问题背景"></a>4.1 问题背景</h3><p>在模板编程中，如何将参数的值类别（左值或右值）保持不变地传递给其他函数。</p><h3 id="4-2-std-forward"><a href="#4-2-std-forward" class="headerlink" title="4.2 std::forward"></a>4.2 <code>std::forward</code></h3><p><code>std::forward</code> 在模板中保持参数的值类别，实现完美转发。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;utility&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-type">int</span>&amp; x)</span> </span>&#123;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Lvalue: &quot;</span> &lt;&lt; x &lt;&lt; std::endl;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-type">int</span>&amp;&amp; x)</span> </span>&#123;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Rvalue: &quot;</span> &lt;&lt; x &lt;&lt; std::endl;<br>&#125;<br><br><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T&gt;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">wrapper</span><span class="hljs-params">(T&amp;&amp; arg)</span> </span>&#123;<br>    <span class="hljs-built_in">print</span>(std::forward&lt;T&gt;(arg)); <span class="hljs-comment">// 完美转发</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-type">int</span> a = <span class="hljs-number">10</span>;<br>    <span class="hljs-built_in">wrapper</span>(a);  <span class="hljs-comment">// 调用 print(int&amp;)</span><br>    <span class="hljs-built_in">wrapper</span>(<span class="hljs-number">20</span>); <span class="hljs-comment">// 调用 print(int&amp;&amp;)</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="4-3-引用折叠-Reference-Collapsing"><a href="#4-3-引用折叠-Reference-Collapsing" class="headerlink" title="4.3 引用折叠 (Reference Collapsing)"></a>4.3 引用折叠 (Reference Collapsing)</h3><p>引用折叠规则：</p><ul><li><code>T&amp; &amp;</code> 折叠为 <code>T&amp;</code></li><li><code>T&amp; &amp;&amp;</code> 折叠为 <code>T&amp;</code></li><li><code>T&amp;&amp; &amp;</code> 折叠为 <code>T&amp;</code></li><li><code>T&amp;&amp; &amp;&amp;</code> 折叠为 <code>T&amp;&amp;</code></li></ul><hr><h2 id="5-特殊成员函数与规则"><a href="#5-特殊成员函数与规则" class="headerlink" title="5. 特殊成员函数与规则"></a>5. 特殊成员函数与规则</h2><h3 id="5-1-特殊成员函数"><a href="#5-1-特殊成员函数" class="headerlink" title="5.1 特殊成员函数"></a>5.1 特殊成员函数</h3><ul><li><strong>移动构造函数</strong>：<code>ClassName(ClassName&amp;&amp;);</code></li><li><strong>移动赋值运算符</strong>：<code>ClassName&amp; operator=(ClassName&amp;&amp;);</code></li></ul><h3 id="5-2-编译器生成的移动操作"><a href="#5-2-编译器生成的移动操作" class="headerlink" title="5.2 编译器生成的移动操作"></a>5.2 编译器生成的移动操作</h3><p>如果用户显式定义了拷贝构造函数、拷贝赋值运算符或析构函数，编译器不会自动生成移动操作。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NoMove</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-built_in">NoMove</span>() = <span class="hljs-keyword">default</span>;<br>    <span class="hljs-built_in">NoMove</span>(<span class="hljs-type">const</span> NoMove&amp;) &#123; std::cout &lt;&lt; <span class="hljs-string">&quot;Copy Constructor&quot;</span> &lt;&lt; std::endl; &#125;<br>    NoMove&amp; <span class="hljs-keyword">operator</span>=(<span class="hljs-type">const</span> NoMove&amp;) &#123; std::cout &lt;&lt; <span class="hljs-string">&quot;Copy Assignment&quot;</span> &lt;&lt; std::endl; <span class="hljs-keyword">return</span> *<span class="hljs-keyword">this</span>; &#125;<br>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    NoMove a;<br>    NoMove b = std::<span class="hljs-built_in">move</span>(a); <span class="hljs-comment">// 调用拷贝构造函数，因为移动操作未生成</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="5-3-删除的移动操作"><a href="#5-3-删除的移动操作" class="headerlink" title="5.3 删除的移动操作"></a>5.3 删除的移动操作</h3><p>如果移动操作被显式删除或不可访问，对象将无法移动。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DeletedMove</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-built_in">DeletedMove</span>() = <span class="hljs-keyword">default</span>;<br>    <span class="hljs-built_in">DeletedMove</span>(DeletedMove&amp;&amp;) = <span class="hljs-keyword">delete</span>; <span class="hljs-comment">// 显式删除移动构造函数</span><br>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    DeletedMove a;<br>    <span class="hljs-comment">// DeletedMove b = std::move(a); // 错误：移动构造函数被删除</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><hr><h2 id="6-右值语义的应用场景"><a href="#6-右值语义的应用场景" class="headerlink" title="6. 右值语义的应用场景"></a>6. 右值语义的应用场景</h2><h3 id="6-1-资源管理类"><a href="#6-1-资源管理类" class="headerlink" title="6.1 资源管理类"></a>6.1 资源管理类</h3><p>在自定义的资源管理类中，使用移动语义避免不必要的资源拷贝。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;memory&gt;</span></span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Resource</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    std::unique_ptr&lt;<span class="hljs-type">int</span>&gt; ptr;<br><br>    <span class="hljs-built_in">Resource</span>(<span class="hljs-type">int</span> value) : <span class="hljs-built_in">ptr</span>(std::<span class="hljs-built_in">make_unique</span>&lt;<span class="hljs-type">int</span>&gt;(value)) &#123;&#125;<br><br>    <span class="hljs-built_in">Resource</span>(Resource&amp;&amp; other) <span class="hljs-keyword">noexcept</span> : <span class="hljs-built_in">ptr</span>(std::<span class="hljs-built_in">move</span>(other.ptr)) &#123;<br>        std::cout &lt;&lt; <span class="hljs-string">&quot;Resource moved&quot;</span> &lt;&lt; std::endl;<br>    &#125;<br>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-function">Resource <span class="hljs-title">r1</span><span class="hljs-params">(<span class="hljs-number">42</span>)</span></span>;<br>    Resource r2 = std::<span class="hljs-built_in">move</span>(r1); <span class="hljs-comment">// 移动资源</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="6-2-标准库中的右值语义"><a href="#6-2-标准库中的右值语义" class="headerlink" title="6.2 标准库中的右值语义"></a>6.2 标准库中的右值语义</h3><p><code>std::unique_ptr</code>、<code>std::vector</code> 等标准库容器和智能指针广泛使用右值语义。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v1&#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>&#125;;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v2 = std::<span class="hljs-built_in">move</span>(v1); <span class="hljs-comment">// 移动 vector</span><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;v2 size: &quot;</span> &lt;&lt; v<span class="hljs-number">2.</span><span class="hljs-built_in">size</span>() &lt;&lt; std::endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="6-3-函数返回值优化-RVO-与移动语义"><a href="#6-3-函数返回值优化-RVO-与移动语义" class="headerlink" title="6.3 函数返回值优化 (RVO) 与移动语义"></a>6.3 函数返回值优化 (RVO) 与移动语义</h3><p>在函数返回值时，编译器可能使用 RVO 或移动语义优化性能。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-function">std::vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">createVector</span><span class="hljs-params">()</span> </span>&#123;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v&#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>&#125;;<br>    <span class="hljs-keyword">return</span> v; <span class="hljs-comment">// 可能触发 RVO 或移动语义</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v = <span class="hljs-built_in">createVector</span>();<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;v size: &quot;</span> &lt;&lt; v.<span class="hljs-built_in">size</span>() &lt;&lt; std::endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><hr><h2 id="7-常见问题与注意事项"><a href="#7-常见问题与注意事项" class="headerlink" title="7. 常见问题与注意事项"></a>7. 常见问题与注意事项</h2><h3 id="7-1-移动后对象的状态"><a href="#7-1-移动后对象的状态" class="headerlink" title="7.1 移动后对象的状态"></a>7.1 移动后对象的状态</h3><p>移动后的对象处于有效但未定义的状态，通常不应再使用。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v1&#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>&#125;;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v2 = std::<span class="hljs-built_in">move</span>(v1);<br>    <span class="hljs-comment">// v1.size(); // 未定义行为，v1 已被移动</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="7-2-避免不必要的-std-move"><a href="#7-2-避免不必要的-std-move" class="headerlink" title="7.2 避免不必要的 std::move"></a>7.2 避免不必要的 <code>std::move</code></h3><p>在返回局部变量时，编译器会自动选择移动或拷贝，无需显式调用 <code>std::move</code>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-function">std::vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">createVector</span><span class="hljs-params">()</span> </span>&#123;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v&#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>&#125;;<br>    <span class="hljs-keyword">return</span> std::<span class="hljs-built_in">move</span>(v); <span class="hljs-comment">// 不必要的 std::move</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v = <span class="hljs-built_in">createVector</span>();<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;v size: &quot;</span> &lt;&lt; v.<span class="hljs-built_in">size</span>() &lt;&lt; std::endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="7-3-右值引用的陷阱"><a href="#7-3-右值引用的陷阱" class="headerlink" title="7.3 右值引用的陷阱"></a>7.3 右值引用的陷阱</h3><p>右值引用本身是左值，因此需要使用 <code>std::move</code> 或 <code>std::forward</code> 来保持其右值特性。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-type">int</span>&amp;&amp; x)</span> </span>&#123;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Rvalue: &quot;</span> &lt;&lt; x &lt;&lt; std::endl;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-type">int</span>&amp;&amp; r = <span class="hljs-number">42</span>;<br>    <span class="hljs-comment">// print(r); // 错误：r 是左值</span><br>    <span class="hljs-built_in">print</span>(std::<span class="hljs-built_in">move</span>(r)); <span class="hljs-comment">// 正确</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><hr><h2 id="8-为什么需要右值"><a href="#8-为什么需要右值" class="headerlink" title="8.为什么需要右值"></a>8.为什么需要右值</h2><p>在 C++ 中，右值（Rvalue）是一个非常重要的概念，它的引入主要是为了解决以下几个核心问题：</p><hr><h3 id="8-1-避免不必要的拷贝"><a href="#8-1-避免不必要的拷贝" class="headerlink" title="8.1 避免不必要的拷贝"></a>8.1 避免不必要的拷贝</h3><p>在传统的 C++ 中，对象的拷贝操作可能会带来性能问题，尤其是在处理大对象或资源密集型对象时。例如，当你将一个对象从一个地方移动到另一个地方时，如果使用拷贝操作，会浪费大量的时间和资源。</p><h4 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-function">std::vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">createVector</span><span class="hljs-params">()</span> </span>&#123;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v&#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>&#125;;<br>    <span class="hljs-keyword">return</span> v; <span class="hljs-comment">// 返回局部对象</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v = <span class="hljs-built_in">createVector</span>(); <span class="hljs-comment">// 拷贝构造</span><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;v size: &quot;</span> &lt;&lt; v.<span class="hljs-built_in">size</span>() &lt;&lt; std::endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>在这个例子中，<code>createVector</code> 返回一个局部对象 <code>v</code>，如果编译器没有优化（如 RVO 或 NRVO），那么 <code>v</code> 会被拷贝到 <code>main</code> 中的 <code>v</code>。对于大对象来说，拷贝操作的代价非常高。</p><p><strong>右值的引入</strong>：通过右值引用和移动语义，可以将对象的资源“移动”到目标对象，而不是拷贝，从而避免不必要的开销。</p><hr><h3 id="8-2-支持移动语义"><a href="#8-2-支持移动语义" class="headerlink" title="8.2 支持移动语义"></a>8.2 支持移动语义</h3><p>移动语义是 C++11 引入的一个重要特性，它允许将资源从一个对象“移动”到另一个对象，而不是拷贝。移动语义的核心是右值引用（<code>T&amp;&amp;</code>）。</p><h4 id="例子：-1"><a href="#例子：-1" class="headerlink" title="例子："></a>例子：</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyVector</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; data;<br><br>    <span class="hljs-comment">// 移动构造函数</span><br>    <span class="hljs-built_in">MyVector</span>(MyVector&amp;&amp; other) <span class="hljs-keyword">noexcept</span> : <span class="hljs-built_in">data</span>(std::<span class="hljs-built_in">move</span>(other.data)) &#123;<br>        std::cout &lt;&lt; <span class="hljs-string">&quot;Move Constructor called&quot;</span> &lt;&lt; std::endl;<br>    &#125;<br><br>    <span class="hljs-built_in">MyVector</span>(<span class="hljs-type">const</span> std::vector&lt;<span class="hljs-type">int</span>&gt;&amp; d) : <span class="hljs-built_in">data</span>(d) &#123;&#125;<br>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    MyVector v1&#123;std::vector&lt;<span class="hljs-type">int</span>&gt;&#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>&#125;&#125;;<br>    MyVector v2 = std::<span class="hljs-built_in">move</span>(v1); <span class="hljs-comment">// 调用移动构造函数</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>在这个例子中，<code>v1</code> 的资源被“移动”到 <code>v2</code>，而不是拷贝。移动操作的代价非常低，通常只是指针的交换。</p><hr><h3 id="8-3-支持完美转发"><a href="#8-3-支持完美转发" class="headerlink" title="8.3 支持完美转发"></a>8.3 支持完美转发</h3><p>在模板编程中，函数参数的值类别（左值或右值）可能会丢失，导致无法正确地传递参数。完美转发（Perfect Forwarding）通过右值引用和 <code>std::forward</code> 解决了这个问题。</p><h4 id="例子：-2"><a href="#例子：-2" class="headerlink" title="例子："></a>例子：</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;utility&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-type">int</span>&amp; x)</span> </span>&#123;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Lvalue: &quot;</span> &lt;&lt; x &lt;&lt; std::endl;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-type">int</span>&amp;&amp; x)</span> </span>&#123;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Rvalue: &quot;</span> &lt;&lt; x &lt;&lt; std::endl;<br>&#125;<br><br><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T&gt;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">wrapper</span><span class="hljs-params">(T&amp;&amp; arg)</span> </span>&#123;<br>    <span class="hljs-built_in">print</span>(std::forward&lt;T&gt;(arg)); <span class="hljs-comment">// 完美转发</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-type">int</span> a = <span class="hljs-number">10</span>;<br>    <span class="hljs-built_in">wrapper</span>(a);  <span class="hljs-comment">// 调用 print(int&amp;)</span><br>    <span class="hljs-built_in">wrapper</span>(<span class="hljs-number">20</span>); <span class="hljs-comment">// 调用 print(int&amp;&amp;)</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>在这个例子中，<code>wrapper</code> 函数能够正确地转发参数的值类别，无论是左值还是右值。</p><hr><h3 id="8-4-优化资源管理"><a href="#8-4-优化资源管理" class="headerlink" title="8.4 优化资源管理"></a>8.4 优化资源管理</h3><p>在资源管理类（如智能指针、文件句柄等）中，右值引用和移动语义可以显著提高性能。例如，<code>std::unique_ptr</code> 是一个典型的例子，它只能通过移动语义来传递所有权，而不能拷贝。</p><h4 id="例子：-3"><a href="#例子：-3" class="headerlink" title="例子："></a>例子：</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;memory&gt;</span></span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Resource</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    std::unique_ptr&lt;<span class="hljs-type">int</span>&gt; ptr;<br><br>    <span class="hljs-built_in">Resource</span>(<span class="hljs-type">int</span> value) : <span class="hljs-built_in">ptr</span>(std::<span class="hljs-built_in">make_unique</span>&lt;<span class="hljs-type">int</span>&gt;(value)) &#123;&#125;<br><br>    <span class="hljs-built_in">Resource</span>(Resource&amp;&amp; other) <span class="hljs-keyword">noexcept</span> : <span class="hljs-built_in">ptr</span>(std::<span class="hljs-built_in">move</span>(other.ptr)) &#123;<br>        std::cout &lt;&lt; <span class="hljs-string">&quot;Resource moved&quot;</span> &lt;&lt; std::endl;<br>    &#125;<br>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-function">Resource <span class="hljs-title">r1</span><span class="hljs-params">(<span class="hljs-number">42</span>)</span></span>;<br>    Resource r2 = std::<span class="hljs-built_in">move</span>(r1); <span class="hljs-comment">// 移动资源</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>在这个例子中，<code>std::unique_ptr</code> 的资源被移动到 <code>r2</code>，而不是拷贝。这确保了资源的唯一所有权。</p><hr><h3 id="8-5-提高代码的表达能力"><a href="#8-5-提高代码的表达能力" class="headerlink" title="8.5 提高代码的表达能力"></a>8.5 提高代码的表达能力</h3><p>右值引用的引入使得 C++ 的表达能力更强。通过移动语义和完美转发，开发者可以编写更高效、更简洁的代码。例如，标准库中的容器（如 <code>std::vector</code>）和算法（如 <code>std::sort</code>）都广泛使用了右值语义。</p><h4 id="例子：-4"><a href="#例子：-4" class="headerlink" title="例子："></a>例子：</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v1&#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>&#125;;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v2 = std::<span class="hljs-built_in">move</span>(v1); <span class="hljs-comment">// 移动 vector</span><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;v2 size: &quot;</span> &lt;&lt; v<span class="hljs-number">2.</span><span class="hljs-built_in">size</span>() &lt;&lt; std::endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>在这个例子中，<code>v1</code> 的资源被移动到 <code>v2</code>，而不是拷贝。这使得代码更加高效。</p><hr><h3 id="8-6-解决临时对象的资源浪费"><a href="#8-6-解决临时对象的资源浪费" class="headerlink" title="8.6 解决临时对象的资源浪费"></a>8.6 解决临时对象的资源浪费</h3><p>在传统的 C++ 中，临时对象（如函数返回值）的生命周期很短，但它们的资源可能会被浪费。通过右值引用和移动语义，可以有效地利用这些临时对象的资源。</p><h4 id="例子：-5"><a href="#例子：-5" class="headerlink" title="例子："></a>例子：</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-function">std::vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">createVector</span><span class="hljs-params">()</span> </span>&#123;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v&#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>&#125;;<br>    <span class="hljs-keyword">return</span> v; <span class="hljs-comment">// 返回局部对象</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    std::vector&lt;<span class="hljs-type">int</span>&gt; v = <span class="hljs-built_in">createVector</span>(); <span class="hljs-comment">// 可能触发 RVO 或移动语义</span><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;v size: &quot;</span> &lt;&lt; v.<span class="hljs-built_in">size</span>() &lt;&lt; std::endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>在这个例子中，<code>createVector</code> 返回的临时对象 <code>v</code> 的资源被移动到 <code>main</code> 中的 <code>v</code>，而不是拷贝。</p><hr><h2 id="9-总结"><a href="#9-总结" class="headerlink" title="9.总结"></a>9.总结</h2><p>右值的引入解决了以下几个核心问题：</p><ol><li><strong>避免不必要的拷贝</strong>：通过移动语义，减少大对象或资源密集型对象的拷贝开销。</li><li><strong>支持移动语义</strong>：允许将资源从一个对象“移动”到另一个对象，而不是拷贝。</li><li><strong>支持完美转发</strong>：在模板编程中，保持参数的值类别，确保参数能够正确传递。</li><li><strong>优化资源管理</strong>：在智能指针和资源管理类中，确保资源的唯一所有权。</li><li><strong>提高代码的表达能力</strong>：使代码更加高效、简洁。</li><li><strong>解决临时对象的资源浪费</strong>：利用临时对象的资源，避免浪费。</li></ol><p>通过右值引用和移动语义，C++ 的性能和表达能力得到了显著提升，使得现代 C++ 代码更加高效和现代化。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="公众号"></p>]]></content>
    
    
    <categories>
      
      <category>开发</category>
      
      <category>cpp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>c++</tag>
      
      <tag>cpp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>C++ 常量详解</title>
    <link href="/2024/12/18/%E5%BC%80%E5%8F%91/cpp/C++%E5%9F%BA%E7%A1%80%EF%BC%9AC++%E5%B8%B8%E9%87%8F%E8%AF%A6%E8%A7%A3/"/>
    <url>/2024/12/18/%E5%BC%80%E5%8F%91/cpp/C++%E5%9F%BA%E7%A1%80%EF%BC%9AC++%E5%B8%B8%E9%87%8F%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="C-常量详解"><a href="#C-常量详解" class="headerlink" title="C++ 常量详解"></a>C++ 常量详解</h1><h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><p>在 C++ 编程中，常量是不可或缺的一部分。常量是指在程序运行期间其值不能被改变的量。使用常量可以提高代码的可读性、可维护性和安全性。通过定义常量，我们可以避免在代码中直接使用硬编码的数值或字符串，从而减少错误并使代码更易于理解。</p><h2 id="二、常量的定义"><a href="#二、常量的定义" class="headerlink" title="二、常量的定义"></a>二、常量的定义</h2><h3 id="常量的概念"><a href="#常量的概念" class="headerlink" title="常量的概念"></a>常量的概念</h3><p>常量是指在程序运行期间其值不能被改变的量。与变量不同，常量的值一旦被定义，就不能在程序的其他部分被修改。</p><h3 id="常量的类型"><a href="#常量的类型" class="headerlink" title="常量的类型"></a>常量的类型</h3><p>常量可以分为两大类：字面常量和符号常量。</p><h4 id="1-字面常量-Literal-Constants"><a href="#1-字面常量-Literal-Constants" class="headerlink" title="1. 字面常量 (Literal Constants)"></a>1. 字面常量 (Literal Constants)</h4><p>字面常量是指直接写在代码中的常量值。它们没有名称，直接表示一个固定的值。</p><ul><li>**整型常量 (Integer Constants)**：表示整数值，可以是十进制、八进制或十六进制。</li><li>**浮点型常量 (Floating-point Constants)**：表示浮点数值，可以是小数形式或指数形式。</li><li>**字符常量 (Character Constants)**：表示单个字符，用单引号括起来。</li><li>**字符串常量 (String Constants)**：表示字符序列，用双引号括起来。</li></ul><h4 id="2-符号常量-Symbolic-Constants"><a href="#2-符号常量-Symbolic-Constants" class="headerlink" title="2. 符号常量 (Symbolic Constants)"></a>2. 符号常量 (Symbolic Constants)</h4><p>符号常量是指通过某种方式定义的具有名称的常量。它们有名称，可以在代码中多次使用。</p><ul><li><strong>使用 <code>#define</code> 预处理器指令定义的常量</strong>：通过预处理器指令 <code>#define</code> 定义的常量。</li><li><strong>使用 <code>const</code> 关键字定义的常量</strong>：通过 <code>const</code> 关键字定义的常量，具有类型检查。</li><li><strong>使用 <code>constexpr</code> 关键字定义的常量</strong>：通过 <code>constexpr</code> 关键字定义的常量，要求在编译时求值。</li></ul><h2 id="三、常量的使用"><a href="#三、常量的使用" class="headerlink" title="三、常量的使用"></a>三、常量的使用</h2><h3 id="1-字面常量的使用"><a href="#1-字面常量的使用" class="headerlink" title="1. 字面常量的使用"></a>1. 字面常量的使用</h3><h4 id="整型常量的表示方法"><a href="#整型常量的表示方法" class="headerlink" title="整型常量的表示方法"></a>整型常量的表示方法</h4><p>整型常量可以用十进制、八进制或十六进制表示。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-comment">// 十进制整型常量</span><br>    <span class="hljs-type">int</span> decimal = <span class="hljs-number">10</span>;<br>    <br>    <span class="hljs-comment">// 八进制整型常量（以0开头）</span><br>    <span class="hljs-type">int</span> octal = <span class="hljs-number">012</span>; <span class="hljs-comment">// 相当于十进制的10</span><br>    <br>    <span class="hljs-comment">// 十六进制整型常量（以0x或0X开头）</span><br>    <span class="hljs-type">int</span> hexadecimal = <span class="hljs-number">0xA</span>; <span class="hljs-comment">// 相当于十进制的10</span><br><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Decimal: &quot;</span> &lt;&lt; decimal &lt;&lt; std::endl;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Octal: &quot;</span> &lt;&lt; octal &lt;&lt; std::endl;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Hexadecimal: &quot;</span> &lt;&lt; hexadecimal &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="浮点型常量的表示方法"><a href="#浮点型常量的表示方法" class="headerlink" title="浮点型常量的表示方法"></a>浮点型常量的表示方法</h4><p>浮点型常量可以用小数形式或指数形式表示。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-comment">// 小数形式的浮点型常量</span><br>    <span class="hljs-type">double</span> pi = <span class="hljs-number">3.14159</span>;<br>    <br>    <span class="hljs-comment">// 指数形式的浮点型常量</span><br>    <span class="hljs-type">double</span> e = <span class="hljs-number">2.71828e0</span>; <span class="hljs-comment">// 相当于2.71828</span><br><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Pi: &quot;</span> &lt;&lt; pi &lt;&lt; std::endl;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;e: &quot;</span> &lt;&lt; e &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="字符常量的表示方法"><a href="#字符常量的表示方法" class="headerlink" title="字符常量的表示方法"></a>字符常量的表示方法</h4><p>字符常量用单引号括起来。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-comment">// 字符常量</span><br>    <span class="hljs-type">char</span> ch = <span class="hljs-string">&#x27;A&#x27;</span>;<br><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Character: &quot;</span> &lt;&lt; ch &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="字符串常量的表示方法"><a href="#字符串常量的表示方法" class="headerlink" title="字符串常量的表示方法"></a>字符串常量的表示方法</h4><p>字符串常量用双引号括起来。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-comment">// 字符串常量</span><br>    std::string str = <span class="hljs-string">&quot;Hello, World!&quot;</span>;<br><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;String: &quot;</span> &lt;&lt; str &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="2-符号常量的使用"><a href="#2-符号常量的使用" class="headerlink" title="2. 符号常量的使用"></a>2. 符号常量的使用</h3><h4 id="使用-define-预处理器指令定义的常量"><a href="#使用-define-预处理器指令定义的常量" class="headerlink" title="使用 #define 预处理器指令定义的常量"></a>使用 <code>#define</code> 预处理器指令定义的常量</h4><p><code>#define</code> 是预处理器指令，用于定义符号常量。它的优点是简单易用，缺点是没有类型检查。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-comment">// 使用 #define 定义常量</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> PI 3.14159</span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;PI: &quot;</span> &lt;&lt; PI &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="使用-const-关键字定义的常量"><a href="#使用-const-关键字定义的常量" class="headerlink" title="使用 const 关键字定义的常量"></a>使用 <code>const</code> 关键字定义的常量</h4><p><code>const</code> 关键字用于定义常量，具有类型检查，推荐在现代 C++ 中使用。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-comment">// 使用 const 定义常量</span><br>    <span class="hljs-type">const</span> <span class="hljs-type">double</span> PI = <span class="hljs-number">3.14159</span>;<br><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;PI: &quot;</span> &lt;&lt; PI &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="使用-constexpr-关键字定义的常量"><a href="#使用-constexpr-关键字定义的常量" class="headerlink" title="使用 constexpr 关键字定义的常量"></a>使用 <code>constexpr</code> 关键字定义的常量</h4><p><code>constexpr</code> 关键字用于定义编译时常量，要求在编译时求值。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-comment">// 使用 constexpr 定义常量</span><br>    <span class="hljs-keyword">constexpr</span> <span class="hljs-type">double</span> PI = <span class="hljs-number">3.14159</span>;<br><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;PI: &quot;</span> &lt;&lt; PI &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="3-常量的命名规范"><a href="#3-常量的命名规范" class="headerlink" title="3. 常量的命名规范"></a>3. 常量的命名规范</h3><p>常量的命名应遵循一定的规范，以提高代码的可读性。</p><ul><li><strong>使用全大写字母和下划线命名常量</strong>：例如 <code>MAX_VALUE</code>。</li><li><strong>避免使用保留字和关键字作为常量名</strong>：例如不要使用 <code>int</code> 或 <code>const</code> 作为常量名。</li></ul><h2 id="四、常量的应用场景"><a href="#四、常量的应用场景" class="headerlink" title="四、常量的应用场景"></a>四、常量的应用场景</h2><h3 id="1-定义程序中不变的值"><a href="#1-定义程序中不变的值" class="headerlink" title="1. 定义程序中不变的值"></a>1. 定义程序中不变的值</h3><p>常量常用于定义程序中不变的值，例如数学常数、程序配置参数和错误代码。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-comment">// 定义数学常数</span><br><span class="hljs-keyword">constexpr</span> <span class="hljs-type">double</span> PI = <span class="hljs-number">3.14159</span>;<br><span class="hljs-keyword">constexpr</span> <span class="hljs-type">double</span> E = <span class="hljs-number">2.71828</span>;<br><br><span class="hljs-comment">// 定义程序配置参数</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> MAX_CONNECTIONS = <span class="hljs-number">100</span>;<br><br><span class="hljs-comment">// 定义错误代码</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> ERROR_CODE_FILE_NOT_FOUND = <span class="hljs-number">404</span>;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;PI: &quot;</span> &lt;&lt; PI &lt;&lt; std::endl;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;E: &quot;</span> &lt;&lt; E &lt;&lt; std::endl;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Max Connections: &quot;</span> &lt;&lt; MAX_CONNECTIONS &lt;&lt; std::endl;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Error Code: &quot;</span> &lt;&lt; ERROR_CODE_FILE_NOT_FOUND &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="2-提高代码的可读性和可维护性"><a href="#2-提高代码的可读性和可维护性" class="headerlink" title="2. 提高代码的可读性和可维护性"></a>2. 提高代码的可读性和可维护性</h3><p>使用常量可以避免在代码中直接使用魔法数字，从而提高代码的可读性和可维护性。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-comment">// 使用常量代替魔法数字</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> DAYS_IN_WEEK = <span class="hljs-number">7</span>;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Days in a week: &quot;</span> &lt;&lt; DAYS_IN_WEEK &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="3-提高程序的安全性"><a href="#3-提高程序的安全性" class="headerlink" title="3. 提高程序的安全性"></a>3. 提高程序的安全性</h3><p>使用 <code>const</code> 关键字可以防止变量被意外修改，从而提高程序的安全性。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">printValue</span><span class="hljs-params">(<span class="hljs-type">const</span> <span class="hljs-type">int</span> value)</span> </span>&#123;<br>    <span class="hljs-comment">// value = 10; // 错误：不能修改 const 变量</span><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Value: &quot;</span> &lt;&lt; value &lt;&lt; std::endl;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-type">const</span> <span class="hljs-type">int</span> value = <span class="hljs-number">42</span>;<br>    <span class="hljs-built_in">printValue</span>(value);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="五、常量的注意事项"><a href="#五、常量的注意事项" class="headerlink" title="五、常量的注意事项"></a>五、常量的注意事项</h2><h3 id="1-define-预处理器指令定义的常量没有类型检查"><a href="#1-define-预处理器指令定义的常量没有类型检查" class="headerlink" title="1. #define 预处理器指令定义的常量没有类型检查"></a>1. <code>#define</code> 预处理器指令定义的常量没有类型检查</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> VALUE 42</span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-type">double</span> value = VALUE; <span class="hljs-comment">// 没有类型检查，可能会导致错误</span><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Value: &quot;</span> &lt;&lt; value &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="2-constexpr-关键字定义的常量必须能够在编译时求值"><a href="#2-constexpr-关键字定义的常量必须能够在编译时求值" class="headerlink" title="2. constexpr 关键字定义的常量必须能够在编译时求值"></a>2. <code>constexpr</code> 关键字定义的常量必须能够在编译时求值</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-keyword">constexpr</span> <span class="hljs-type">int</span> <span class="hljs-title">getValue</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">42</span>; <span class="hljs-comment">// 必须在编译时求值</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-keyword">constexpr</span> <span class="hljs-type">int</span> value = <span class="hljs-built_in">getValue</span>();<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Value: &quot;</span> &lt;&lt; value &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="3-避免过度使用常量，影响代码的可读性"><a href="#3-避免过度使用常量，影响代码的可读性" class="headerlink" title="3. 避免过度使用常量，影响代码的可读性"></a>3. 避免过度使用常量，影响代码的可读性</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> A = <span class="hljs-number">1</span>;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> B = <span class="hljs-number">2</span>;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> C = <span class="hljs-number">3</span>;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-comment">// 过度使用常量可能会影响代码的可读性</span><br>    <span class="hljs-type">int</span> result = A + B + C;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Result: &quot;</span> &lt;&lt; result &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h2><p>本文详细介绍了 C++ 中常量的定义、使用和应用场景。常量在提高代码质量方面起着重要作用，能够提高代码的可读性、可维护性和安全性。通过合理使用常量，我们可以避免硬编码的数值和字符串，从而使代码更加清晰和易于维护。</p><h2 id="七、参考资料"><a href="#七、参考资料" class="headerlink" title="七、参考资料"></a>七、参考资料</h2><ul><li><a href="https://www.amazon.com/C-Primer-5th-Stanley-Lippman/dp/0321714113">C++ Primer</a></li><li><a href="https://en.cppreference.com/w/">cppreference.com</a></li></ul><h2 id="八、附录"><a href="#八、附录" class="headerlink" title="八、附录"></a>八、附录</h2><h3 id="常见常量示例代码"><a href="#常见常量示例代码" class="headerlink" title="常见常量示例代码"></a>常见常量示例代码</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-comment">// 使用 #define 定义常量</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> MAX_VALUE 100</span><br><br><span class="hljs-comment">// 使用 const 定义常量</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> MIN_VALUE = <span class="hljs-number">0</span>;<br><br><span class="hljs-comment">// 使用 constexpr 定义常量</span><br><span class="hljs-keyword">constexpr</span> <span class="hljs-type">double</span> PI = <span class="hljs-number">3.14159</span>;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Max Value: &quot;</span> &lt;&lt; MAX_VALUE &lt;&lt; std::endl;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Min Value: &quot;</span> &lt;&lt; MIN_VALUE &lt;&lt; std::endl;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;PI: &quot;</span> &lt;&lt; PI &lt;&lt; std::endl;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="常量相关的常见问题解答"><a href="#常量相关的常见问题解答" class="headerlink" title="常量相关的常见问题解答"></a>常量相关的常见问题解答</h3><ol><li><p><strong>问：<code>const</code> 和 <code>constexpr</code> 有什么区别？</strong></p><ul><li>答：<code>const</code> 用于定义运行时常量，而 <code>constexpr</code> 用于定义编译时常量，要求在编译时求值。</li></ul></li><li><p><strong>问：为什么推荐使用 <code>const</code> 而不是 <code>#define</code>？</strong></p><ul><li>答：<code>const</code> 具有类型检查，更安全且更易于调试，而 <code>#define</code> 没有类型检查，容易出错。</li></ul></li><li><p><strong>问：<code>constexpr</code> 函数有什么要求？</strong></p><ul><li>答：<code>constexpr</code> 函数必须在编译时求值，且不能包含复杂的逻辑或运行时才能确定的值。</li></ul></li></ol><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>开发</category>
      
      <category>cpp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>c++</tag>
      
      <tag>cpp</tag>
      
      <tag>c++基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OCR算法、模型综述</title>
    <link href="/2024/12/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal/OCR/%E5%A4%9A%E6%A8%A1%E6%80%81003%EF%BC%9AOCR%E7%AE%97%E6%B3%95%E3%80%81%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/"/>
    <url>/2024/12/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal/OCR/%E5%A4%9A%E6%A8%A1%E6%80%81003%EF%BC%9AOCR%E7%AE%97%E6%B3%95%E3%80%81%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h1><h3 id="1-1-什么是OCR"><a href="#1-1-什么是OCR" class="headerlink" title="1.1 什么是OCR"></a>1.1 什么是OCR</h3><p>OCR俗称光学字符识别，英文全称是Optical Charater Recognition（简称OCR）,它是利用光学技术和计算机技术把印刷在或者写在图纸上的文字以文本形式提取出来，并转换成一种计算机能够接受、人又可以理解的格式。OCR技术是实现文字快速录入的一项关键技术。在信息社会时代，每天会产生大量的票据、表单、证件数据，这些数据要电子化，需要利用OCR技术进行提取录入。在深度学习没有全面推广之前，大部分OCR识别都是基于传统的方法进行检测识别。在背景单一、数据场景简单的情况下，传统OCR一般都能达到好的效果，但在一些场景复杂、干扰多的情况下，识别效果不好，这个时候深度学习OCR就能体现出巨大的优势。</p><h3 id="1-2-传统OCR与深度学习OCR"><a href="#1-2-传统OCR与深度学习OCR" class="headerlink" title="1.2 传统OCR与深度学习OCR"></a>1.2 传统OCR与深度学习OCR</h3><p>传统OCR（Optical Character Recognition，光学字符识别）方法是指在深度学习兴起之前广泛使用的一种将图片中的文字转化为计算机可识别文字的技术手段。它主要通过图像处理和统计机器学习方法来达成这一目的。传统OCR技术能够对纸上打印的字符进行识别，支持多场景、任意版型（如英文、字母、数字等）的文字识别，最初只能将图片中的文字转为纯文本，随着技术发展后来也能识别表格以及将一些固定排版的图片（如证件、票据等）转为结构化的数据 。</p><p>从技术实现角度看，传统OCR技术涵盖了多个处理阶段，例如文字区域定位、文字矫正、文字分割、文字识别以及后处理等环节。它利用opencv算法库等工具，运用连通区域分析、MSER（Maximally Stable Extremal Regions，最大稳定极值区域）等技术进行文字区域定位，通过旋转、仿射变换进行文字矫正，采用二值化、过滤噪声来分割文字，再利用逻辑回归、SVM（Support Vector Machine，支持向量机）、Adaboost等分类器识别文字，最后结合规则、语言模型（如HMM - Hidden Markov Model，隐马尔可夫模型）等进行后处理，从而提高识别的准确性和有效性</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241209122335916.png" alt="image-20241209122335916"></p><p>深度学习OCR方法可以分为两阶段算法和端到端的算法。两阶段OCR算法分为文本检测和识别算法，文本检测算法从图像中得到文本行的检测框，然后识别算法识别文本框中的内容。</p><p>端对端OCR算法使用一个模型同时完成文字检测和文字识别，因此端对端模型更小，速度更快。</p><p>相对于传统OCR方法，深度学习OCR方法准确率较高，速度可能慢一点。</p><h4 id="传统OCR方法的优缺点"><a href="#传统OCR方法的优缺点" class="headerlink" title="传统OCR方法的优缺点"></a>传统OCR方法的优缺点</h4><h5 id="（一）优点"><a href="#（一）优点" class="headerlink" title="（一）优点"></a>（一）优点</h5><ol><li>处理简单场景效果好<ul><li>对于简单场景下的图片，传统OCR已经取得了很好的识别效果。例如在一些格式规范、文字清晰、背景简单的文档识别中，传统OCR能够准确地识别出文字内容。像标准格式的发票识别，发票上的文字排版相对固定，字体规范，传统OCR可以有效地提取出发票号码、金额、日期等关键信息。在这种简单场景下，传统OCR的识别准确率可以达到较高水平，满足基本的业务需求，如财务报销中的发票信息录入等 <a href="https://cloud.tencent.com/developer/information/ocr%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95">8</a>。</li></ul></li><li>运行速度较快<ul><li>传统OCR技术通常比基于深度学习的OCR（aiOCR）更快，因为它不需要进行大量的训练和学习。在处理一些常规的、规则排列的文字时，传统OCR可以迅速地进行识别。例如在识别大量的、格式统一的表格数据时，传统OCR能够快速地对表格中的文字进行定位、分割和识别，不需要像深度学习模型那样进行复杂的神经网络前向传播和反向传播计算。这使得传统OCR在一些对速度要求较高、数据格式相对简单的应用场景中具有优势，如快递单的信息识别，在快递分拣过程中，需要快速地识别出寄件人和收件人的姓名、地址、电话号码等信息，传统OCR可以在较短的时间内完成识别任务 <a href="https://www.11pdf.com/Tag/2192136.html">16</a>。</li></ul></li><li>成本较低<ul><li>传统OCR技术通常比较便宜，并且可以在低端硬件上运行。它不需要高端的图形处理单元（GPU）等昂贵的硬件设备来支持计算。对于一些预算有限的企业或个人用户来说，传统OCR技术是一种经济实惠的选择。例如，一些小型企业在进行文档数字化时，如果只是处理一些简单的文档类型，采用传统OCR技术可以在较低的成本下实现文字的识别和转换。而且，传统OCR技术在软件授权方面也相对较为便宜，不需要像一些基于深度学习的OCR软件那样支付高额的软件使用费用 <a href="https://www.11pdf.com/Tag/2192136.html">16</a>。</li></ul></li></ol><h5 id="（二）缺点"><a href="#（二）缺点" class="headerlink" title="（二）缺点"></a>（二）缺点</h5><ol><li>对复杂场景适应性差<ul><li>传统OCR方法是针对特定场景的图像进行建模的，一旦跳出当前场景，模型就会失效。例如，在处理自然场景中的文字时，如街景中的招牌、车身广告上的文字等，这些文字可能存在字体多样、大小不一、排列不规则、背景复杂（包含各种图案、光影变化等）的情况，传统OCR技术往往难以准确识别。再如手写文字的识别，由于每个人的书写风格差异很大，传统OCR很难对各种手写字体进行准确识别，其识别准确率会大幅下降 <a href="https://cloud.tencent.com/developer/information/ocr%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95">8</a>。</li></ul></li><li>识别精度有限<ul><li>对于复杂的文本，如包含多种字体、字号、颜色变化以及特殊排版（如艺术字、斜体字与正体字混合排版等）的文本，传统OCR技术可能会出现错误。在处理一些低质量的图像，如模糊、有污渍或者光照不均匀的图像时，传统OCR的识别准确性也会受到严重影响。例如，在识别一张被水浸湿过的纸质文档上的文字时，由于纸张的变形、文字的模糊等原因，传统OCR可能无法正确识别出所有的文字内容 <a href="https://www.11pdf.com/Tag/2192136.html">16</a>。</li></ul></li><li>需要专门培训人员<ul><li>传统OCR技术需要专门培训人员进行使用和维护。其参数调整、算法优化等操作需要一定的专业知识和经验。例如，在使用传统OCR软件进行特定文档类型的识别时，可能需要根据文档的特点（如字体类型、排版格式等）对识别算法的参数进行调整，这就要求操作人员具备相关的技术知识，而这种专业人才相对较少，增加了企业或组织使用传统OCR技术的人力成本和难度 <a href="https://www.11pdf.com/Tag/2192136.html">16</a>。</li></ul></li><li>对格式保留困难<ul><li>传统OCR技术在将纸质文档转换为电子文本时，可能无法完全保留原始文档的格式、排版和图表等信息。例如，在识别一份包含表格和图片的文档时，传统OCR可能只能识别出表格中的文字内容，而无法准确还原表格的结构和样式，对于图片更是无法直接转换为电子文档中的可编辑元素，这在一定程度上影响了文档转换的完整性和可用性。</li></ul></li></ol><h1 id="2-传统OCR方法"><a href="#2-传统OCR方法" class="headerlink" title="2. 传统OCR方法"></a>2. 传统OCR方法</h1><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/1577105446728504.jpeg" alt="传统OCR方法"></p><p>传统OCR方法主要分为文字区域定位、文字图像矫正、行列分割、分类器识别和后处理。按处理方式划分为三个阶段：预处理阶段、识别阶段和后处理阶段。</p><h3 id="2-1-文字区域定位"><a href="#2-1-文字区域定位" class="headerlink" title="2.1 文字区域定位"></a>2.1 文字区域定位</h3><ul><li>传统OCR方法在处理图像时，首先要进行文字区域定位。这一过程常采用连通区域分析和MSER算法。连通区域分析是基于图像中像素的连通性，将具有相似特征（如颜色、灰度值等）的像素划分为不同的区域。例如，在一张包含文字和背景的图像中，文字部分的像素与背景像素在灰度或颜色上可能存在差异，通过连通区域分析可以初步找出可能是文字的区域。</li><li>MSER算法则是通过寻找图像中的极值区域，这些区域在一定的阈值变化范围内是最稳定的。对于文字来说，其字符形状相对稳定，在不同的灰度阈值下，字符所在区域往往表现出极大值或极小值的特性。通过这种方式，可以更精确地定位文字所在的区域</li></ul><h3 id="2-2-文字图像矫正"><a href="#2-2-文字图像矫正" class="headerlink" title="2.2 文字图像矫正"></a>2.2 文字图像矫正</h3><ul><li>当图像中的文字存在倾斜或扭曲时，就需要进行文字矫正。这一阶段通常使用旋转和仿射变换。旋转是针对文字整体存在一定角度倾斜的情况，通过计算倾斜角度，将文字区域旋转到水平或垂直方向。例如，在扫描纸质文档时，如果文档放置稍有倾斜，扫描得到的图像中的文字也会倾斜，此时就可以根据文字的边界或者特定的算法来确定倾斜角度并进行旋转操作。</li><li>仿射变换则更通用，它可以处理文字的拉伸、压缩以及更复杂的变形情况。它通过对图像中的坐标进行线性变换，将变形后的文字映射回正常的形状，从而为后续的文字分割和识别提供更有利的条件。</li></ul><h3 id="2-3-行列分割"><a href="#2-3-行列分割" class="headerlink" title="2.3 行列分割"></a>2.3 行列分割</h3><ul><li>在文字分割阶段，传统OCR采用二值化和过滤噪声等操作。二值化是将图像的像素值转换为只有0和1的二值形式，这样可以突出文字与背景的对比。例如，将文字部分设为1（白色），背景设为0（黑色）或者反之。通过这种方式，可以简化图像的复杂度，使得文字的轮廓更加清晰。</li><li>过滤噪声则是去除图像中的干扰因素，如扫描过程中产生的小斑点、划痕等。这些噪声可能会被误识别为文字的一部分，通过滤波算法（如中值滤波、高斯滤波等）可以去除这些不必要的干扰，提高文字分割的准确性。</li></ul><h3 id="2-4-分类器识别"><a href="#2-4-分类器识别" class="headerlink" title="2.4 分类器识别"></a>2.4 分类器识别</h3><p>分类器识别过程是在提取了字符图像之后，对其进行分析和分类，以确定每个字符对应的符号或文字。以下是这一过程的详细说明：</p><h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><p>在进行分类之前，OCR系统首先需要从图像中提取出字符的特征。这可能包括：</p><ul><li><strong>几何特征</strong>：如字符的高度、宽度、笔画数量等。</li><li><strong>统计特征</strong>：例如像素分布、边缘检测结果等。</li><li><strong>拓扑结构</strong>：字符内部的连通性或断开情况。</li><li><strong>频域特征</strong>：通过傅里叶变换或其他频域分析方法获得的特征。</li></ul><p>这些特征能够描述字符的独特属性，有助于后续的分类工作。</p><h4 id="分类器训练"><a href="#分类器训练" class="headerlink" title="分类器训练"></a>分类器训练</h4><p>为了实现准确的分类，OCR系统通常会使用机器学习算法来构建分类器。训练阶段涉及以下几个方面：</p><ul><li><strong>数据准备</strong>：收集大量带有标签的字符图像作为训练样本。每个样本都包含一个字符图像及其对应的正确标签（即该字符的真实值）。</li><li><strong>选择模型</strong>：根据任务需求选择合适的分类算法，如支持向量机（SVM）、K近邻（KNN）、神经网络等。</li><li><strong>训练过程</strong>：利用训练集中的数据对选定的模型进行训练，调整模型参数，使得模型能够学习到如何基于输入的特征预测正确的字符标签。</li></ul><h4 id="分类与决策"><a href="#分类与决策" class="headerlink" title="分类与决策"></a>分类与决策</h4><p>一旦分类器被训练好，就可以用于实际的字符识别：</p><ul><li><strong>输入处理</strong>：当一个新的字符图像输入时，系统首先会提取其特征。</li><li><strong>分类预测</strong>：然后使用训练好的分类器对这些特征进行评估，输出最有可能的字符标签。</li><li><strong>后处理</strong>：有时还会有一个后处理步骤，比如语言模型的应用，来修正识别结果中的拼写错误或者不符合语法的地方，提高整体准确性。</li></ul><p>分类器识别是传统OCR方法的核心部分，它决定了系统能否准确地将图像中的字符转换成文本。通过精心设计的特征提取、有效的分类算法和充足的训练数据，可以显著提高OCR系统的准确性和鲁棒性。</p><h3 id="2-5-后处理"><a href="#2-5-后处理" class="headerlink" title="2.5 后处理"></a>2.5 后处理</h3><ul><li>传统OCR的后处理阶段会利用规则和语言模型来提高识别的准确性。规则可以是基于语法、格式等方面的规定。例如，在识别身份证号码时，根据身份证号码的固定格式（18位数字，特定的编码规则等），对识别结果进行校验和修正。</li><li>语言模型如HMM也会被应用。HMM是一种统计模型，它基于马尔可夫链假设，对文字的序列进行建模。在文字识别中，它可以根据语言的概率分布来纠正识别结果中的错误。例如，在识别一段英文句子时，如果某个单词被识别为不符合语法规则的形式，HMM可以根据语言的统计规律，推荐最可能的正确单词。</li></ul><h1 id="3-深度学习OCR方法"><a href="#3-深度学习OCR方法" class="headerlink" title="3. 深度学习OCR方法"></a>3. 深度学习OCR方法</h1><h3 id="3-1-两阶段OCR"><a href="#3-1-两阶段OCR" class="headerlink" title="3.1 两阶段OCR"></a>3.1 两阶段OCR</h3><h4 id=""><a href="#" class="headerlink" title=""></a><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/68747470733a2f2f61692d73747564696f2d7374617469632d6f6e6c696e652e63646e2e626365626f732e636f6d2f34663465613635353738333834393030393039656666663933643062373338366538366563653134346438633436373762376263393462346630333337636662" alt="img"></h4><table><thead><tr><th>文本检测算法模型</th><th>文字识别算法模型</th><th></th></tr></thead><tbody><tr><td>CTPN</td><td>CRNN</td><td></td></tr><tr><td>SegLink</td><td>RARE</td><td></td></tr><tr><td>EAST</td><td>RAN</td><td></td></tr><tr><td>PSENet</td><td>ASTER</td><td></td></tr><tr><td>DBNet</td><td>MORAN</td><td></td></tr><tr><td>FCENet</td><td>SRN</td><td></td></tr><tr><td>Texboxes</td><td>STAR-Net</td><td></td></tr><tr><td>CRAFT</td><td>Rosetta</td><td></td></tr><tr><td>LOMO</td><td>SAR</td><td></td></tr><tr><td>SPCNet</td><td>R2AM</td><td></td></tr><tr><td>PAN</td><td></td><td></td></tr><tr><td>DB</td><td></td><td></td></tr></tbody></table><h3 id="3-2-端到端OCR"><a href="#3-2-端到端OCR" class="headerlink" title="3.2 端到端OCR"></a>3.2 端到端OCR</h3><p>与两阶段OCR不同，端到端OCR不需要先进行文字检测然后再进行文字识别这样明确的分阶段操作。它直接在整个图像上进行操作，将图像中的文字信息直接转换为文本。这样做的好处是避免了由于分阶段处理带来的误差累积问题。例如，在两阶段OCR中，如果文字检测阶段出现错误，那么在识别阶段就会基于错误的检测结果进行操作，而端到端OCR则不存在这种情况，因为它是从图像到文本的直接映射。并且，将检测和识别统一到一个模型里面，就使得图像的feature可以被共享利用。</p><table><thead><tr><th>OCR算法模型</th><th>Title</th></tr></thead><tbody><tr><td></td><td><a href="https://arxiv.org/pdf/1707.03985.pdf">Towards End-to-end Text Spotting with Convolution Recurrent Neural Network</a></td></tr><tr><td>FOTS</td><td><a href="https://arxiv.org/pdf/1801.01671.pdf">FOTS: Fast Oriented Text Spotting with a Unified Network</a></td></tr><tr><td>Mask TextSpotter</td><td><a href="https://arxiv.org/pdf/1807.02242.pdf">Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes</a></td></tr><tr><td>CharNet</td><td><a href="https://arxiv.org/abs/1910.07954">Convolutional Character Networks</a></td></tr><tr><td>Mask TextSpotterV3</td><td><a href="https://arxiv.org/abs/2007.09482">Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting</a></td></tr><tr><td>PGNet</td><td>[<a href="https://arxiv.org/abs/2104.05458">2104.05458] PGNet: Real-time Arbitrarily-Shaped Text Spotting with Point Gathering Network</a></td></tr><tr><td>ABCNet</td><td></td></tr><tr><td>TrOCR</td><td>TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models</td></tr><tr><td>SVTRv2</td><td>SVTRv2: CTC Beats Encoder-Decoder Models in Scene Text Recognition</td></tr><tr><td>GOT-OCR</td><td><a href="https://github.com/Ucas-HaoranWei/GOT-OCR2.0">Ucas-HaoranWei&#x2F;GOT-OCR2.0: Official code implementation of General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model</a></td></tr></tbody></table><h3 id="3-3-多模态OCR"><a href="#3-3-多模态OCR" class="headerlink" title="3.3 多模态OCR"></a>3.3 多模态OCR</h3><table><thead><tr><th>算法模型</th><th></th></tr></thead><tbody><tr><td>Vary</td><td>[<a href="https://arxiv.org/abs/2312.06109">2312.06109] Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models</a></td></tr><tr><td>TextHarmony</td><td>[<a href="https://arxiv.org/abs/2407.16364">2407.16364] Harmonizing Visual Text Comprehension and Generation</a></td></tr><tr><td><a href="https://www.aisharenet.com/got-ocr20jiyu/">GOT-OCR2.0</a></td><td><a href="https://github.com/Ucas-HaoranWei/GOT-OCR2.0">Ucas-HaoranWei&#x2F;GOT-OCR2.0: Official code implementation of General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model</a></td></tr><tr><td>其它大部分多模态大模型都支持OCR任务</td><td><a href="https://github.com/Yuliang-Liu/MultimodalOCR">Yuliang-Liu&#x2F;MultimodalOCR: On the Hidden Mystery of OCR in Large Multimodal Models (OCRBench)</a></td></tr></tbody></table><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/4b4650c9d035fbd912d56823b451c035.png" alt="多模态OCR方法"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241209145707320.png" alt="多模型OCR结果"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241209145800157.png" alt="多模态OCR结果"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241209145845645.png" alt="多模态OCR结果"></p><h1 id="4-OCR应用方向"><a href="#4-OCR应用方向" class="headerlink" title="4. OCR应用方向"></a>4. OCR应用方向</h1><h3 id="4-1-文字识别"><a href="#4-1-文字识别" class="headerlink" title="4.1 文字识别"></a>4.1 文字识别</h3><ul><li><p>普通文字识别</p></li><li><p>场景文字识别</p></li><li><p>数学公式识别</p><p>从图片中提取数学公式，转换为markdown或者Latex</p></li></ul><h3 id="4-2-文档结构化识别（版面分析）"><a href="#4-2-文档结构化识别（版面分析）" class="headerlink" title="4.2 文档结构化识别（版面分析）"></a>4.2 文档结构化识别（版面分析）</h3><ul><li><p>页眉</p></li><li><p>页脚</p></li><li><p>数学公式</p></li><li><p>图表的Caption</p></li><li><p>图片</p></li><li><p>表格</p></li><li><p>段落文本</p></li><li><p>段落标题</p></li><li><p>列表</p></li></ul><h3 id="4-3-关键信息抽取"><a href="#4-3-关键信息抽取" class="headerlink" title="4.3 关键信息抽取"></a>4.3 关键信息抽取</h3><p>关键信息提取（Key Information Extraction，KIE）是Document VQA中的一个重要任务，主要从图像中提取所需要的关键信息，如从身份证中提取出姓名和公民身份号码信息，这类信息的种类往往在特定任务下是固定的，但是在不同任务间是不同的。</p><p>KIE通常分为两个子任务进行研究：</p><ul><li>SER: 语义实体识别 (Semantic Entity Recognition)，对每一个检测到的文本进行分类，如将其分为姓名，身份证。如下图中的黑色框和红色框。</li><li>RE: 关系抽取 (Relation Extraction)，对每一个检测到的文本进行分类，如将其分为问题和的答案。然后对每一个问题找到对应的答案。如下图中的红色框和黑色框分别代表问题和答案，黄色线代表问题和答案之间的对应关系。</li></ul><p>一般的KIE方法基于命名实体识别(Named Entity Recognition,NER)[4]来研究，但是这类方法只利用了图像中的文本信息，缺少对视觉和结构信息的使用，因此精度不高。在此基础上，近几年的方法都开始将视觉和结构信息与文本信息融合到一起，按照对多模态信息进行融合时所采用的原理可以将这些方法分为下面四种：</p><ul><li>基于Grid的方法</li><li>基于Token的方法</li><li>基于GCN的方法</li><li>基于End to End 的方法</li></ul><h1 id="5-OCR相关工具"><a href="#5-OCR相关工具" class="headerlink" title="5. OCR相关工具"></a>5. OCR相关工具</h1><ul><li><p>PaddleOCR:通用OCR工具</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/68747470733a2f2f61692d73747564696f2d7374617469632d6f6e6c696e652e63646e2e626365626f732e636f6d2f65303939323962346133316534346639623565336435343264313234313133333236363964326531613231643435616438386231646439313134326563383663" alt="PaddleOCR"></p></li><li><p>Tesseract: 普通文字识别</p></li><li><p>Umi-OCR: 普通文字识别、文档解析</p></li><li><p>MinerU:将PDF转换成Markdown和JSON格式</p></li><li><p>LaTeX-OCR: 数学公式识别</p></li></ul><p><a href="https://developer.aliyun.com/article/1054626">OCR文字识别方法综述-阿里云开发者社区</a></p><p><a href="https://bbs.huaweicloud.com/blogs/140398#H10">传统OCR识别综述-云社区-华为云</a></p><p><a href="https://github.com/PaddleOCR-Community/Dive-into-OCR/blob/main/notebook_ch/1.introduction/OCR%E6%8A%80%E6%9C%AF%E5%AF%BC%E8%AE%BA.ipynb">Dive-into-OCR&#x2F;notebook_ch&#x2F;1.introduction&#x2F;OCR技术导论.ipynb at main · PaddleOCR-Community&#x2F;Dive-into-OCR</a></p><p><a href="https://blog.csdn.net/jiaoyangwm/article/details/138414709">【多模态】29、OCRBench | 为大型多模态模型提供一个 OCR 任务测评基准-CSDN博客</a></p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink">https://github.com/chongzicbo/ReadWriteThink</a></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="二维码"></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>multi-modal</category>
      
      <category>OCR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>多模态</tag>
      
      <tag>OCR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>C++ 中的 static 关键字：深入理解与应用</title>
    <link href="/2024/12/16/%E5%BC%80%E5%8F%91/cpp/C++%E5%9F%BA%E7%A1%80%EF%BC%9AC++%20%E4%B8%AD%E7%9A%84%20static%20%E5%85%B3%E9%94%AE%E5%AD%97%EF%BC%9A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <url>/2024/12/16/%E5%BC%80%E5%8F%91/cpp/C++%E5%9F%BA%E7%A1%80%EF%BC%9AC++%20%E4%B8%AD%E7%9A%84%20static%20%E5%85%B3%E9%94%AE%E5%AD%97%EF%BC%9A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E4%B8%8E%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="C-中的-static-关键字：深入理解与应用"><a href="#C-中的-static-关键字：深入理解与应用" class="headerlink" title="C++ 中的 static 关键字：深入理解与应用"></a>C++ 中的 static 关键字：深入理解与应用</h1><h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><p>C++ 语言以其强大的功能和灵活性著称，能够满足从底层系统编程到高层应用开发的广泛需求。在 C++ 中，<code>static</code> 关键字是一个非常重要且常用的关键字，它具有多种用途，能够帮助开发者更好地管理内存、控制作用域以及实现一些特定的编程模式。本文将详细探讨 <code>static</code> 关键字在 C++ 中的多种用途，并通过代码示例帮助读者深入理解其应用。</p><h2 id="二、static-关键字的基本概念"><a href="#二、static-关键字的基本概念" class="headerlink" title="二、static 关键字的基本概念"></a>二、static 关键字的基本概念</h2><h3 id="1-基本含义"><a href="#1-基本含义" class="headerlink" title="1. 基本含义"></a>1. 基本含义</h3><p><code>static</code> 关键字在 C++ 中有两个基本含义：<strong>静态存储期</strong>和<strong>内部链接性</strong>。</p><ul><li><strong>静态存储期</strong>：静态存储期的变量在程序的整个运行期间都存在，不会在作用域结束时被销毁。</li><li><strong>内部链接性</strong>：具有内部链接性的变量或函数只能在声明它的文件内部访问，不能被其他文件访问。</li></ul><h3 id="2-static-与非-static-变量-函数的对比"><a href="#2-static-与非-static-变量-函数的对比" class="headerlink" title="2. static 与非 static 变量&#x2F;函数的对比"></a>2. static 与非 static 变量&#x2F;函数的对比</h3><table><thead><tr><th>特性</th><th>非 static 变量&#x2F;函数</th><th>static 变量&#x2F;函数</th></tr></thead><tbody><tr><td><strong>存储位置</strong></td><td>栈或堆</td><td>静态存储区</td></tr><tr><td><strong>生命周期</strong></td><td>作用域结束时销毁</td><td>程序运行期间一直存在</td></tr><tr><td><strong>作用域</strong></td><td>局部作用域或全局作用域</td><td>局部作用域或文件内部作用域</td></tr></tbody></table><h2 id="三、static-关键字在不同场景下的应用"><a href="#三、static-关键字在不同场景下的应用" class="headerlink" title="三、static 关键字在不同场景下的应用"></a>三、static 关键字在不同场景下的应用</h2><h3 id="1-静态局部变量"><a href="#1-静态局部变量" class="headerlink" title="1. 静态局部变量"></a>1. 静态局部变量</h3><h4 id="定义和语法"><a href="#定义和语法" class="headerlink" title="定义和语法"></a>定义和语法</h4><p>静态局部变量是在局部作用域中使用 <code>static</code> 关键字声明的变量。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">exampleFunction</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-type">static</span> <span class="hljs-type">int</span> count = <span class="hljs-number">0</span>;  <span class="hljs-comment">// 静态局部变量</span><br>    count++;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Count: &quot;</span> &lt;&lt; count &lt;&lt; std::endl;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h4><ul><li><strong>只初始化一次</strong>：静态局部变量在第一次进入该作用域时进行初始化，之后不再初始化。</li><li><strong>生命周期贯穿整个程序运行期间</strong>：即使函数执行结束，静态局部变量的值仍然保留。</li><li><strong>作用域仅限于声明它的局部作用域</strong>：静态局部变量只能在声明它的函数内部访问。</li></ul><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><ul><li><strong>实现单例模式</strong>：静态局部变量可以用于实现线程安全的单例模式。</li><li><strong>保存函数调用之间的状态</strong>：静态局部变量可以用于保存函数调用之间的状态信息。</li></ul><h4 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">exampleFunction</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-type">static</span> <span class="hljs-type">int</span> count = <span class="hljs-number">0</span>;  <span class="hljs-comment">// 静态局部变量，只初始化一次</span><br>    count++;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;Count: &quot;</span> &lt;&lt; count &lt;&lt; std::endl;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-built_in">exampleFunction</span>();  <span class="hljs-comment">// 输出: Count: 1</span><br>    <span class="hljs-built_in">exampleFunction</span>();  <span class="hljs-comment">// 输出: Count: 2</span><br>    <span class="hljs-built_in">exampleFunction</span>();  <span class="hljs-comment">// 输出: Count: 3</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="2-静态全局变量和静态函数"><a href="#2-静态全局变量和静态函数" class="headerlink" title="2. 静态全局变量和静态函数"></a>2. 静态全局变量和静态函数</h3><h4 id="定义和语法-1"><a href="#定义和语法-1" class="headerlink" title="定义和语法"></a>定义和语法</h4><p>静态全局变量和静态函数是在全局作用域中使用 <code>static</code> 关键字声明的变量或函数。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">static</span> <span class="hljs-type">int</span> globalVar = <span class="hljs-number">10</span>;  <span class="hljs-comment">// 静态全局变量</span><br><br><span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">void</span> <span class="hljs-title">staticFunction</span><span class="hljs-params">()</span> </span>&#123;<br>    std::cout &lt;&lt; <span class="hljs-string">&quot;This is a static function.&quot;</span> &lt;&lt; std::endl;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="特性-1"><a href="#特性-1" class="headerlink" title="特性"></a>特性</h4><ul><li><strong>限制作用域</strong>：静态全局变量和静态函数的作用域仅限于声明它的文件内部。</li><li><strong>避免命名冲突</strong>：静态全局变量和静态函数可以避免与其他文件中的同名变量或函数发生冲突。</li></ul><h4 id="应用场景-1"><a href="#应用场景-1" class="headerlink" title="应用场景"></a>应用场景</h4><ul><li><strong>实现文件级别的私有变量和函数</strong>：静态全局变量和静态函数可以用于实现文件内部的私有变量和函数。</li><li><strong>组织代码</strong>：静态全局变量和静态函数可以用于组织代码，提高代码的可读性和可维护性。</li></ul><h4 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// file1.cpp</span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-type">static</span> <span class="hljs-type">int</span> globalVar = <span class="hljs-number">10</span>;  <span class="hljs-comment">// 静态全局变量，仅在 file1.cpp 中可见</span><br><br><span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">void</span> <span class="hljs-title">staticFunction</span><span class="hljs-params">()</span> </span>&#123;  <span class="hljs-comment">// 静态函数，仅在 file1.cpp 中可见</span><br>    std::cout &lt;&lt; <span class="hljs-string">&quot;GlobalVar: &quot;</span> &lt;&lt; globalVar &lt;&lt; std::endl;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">callStaticFunction</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-built_in">staticFunction</span>();  <span class="hljs-comment">// 调用静态函数</span><br>&#125;<br><br><span class="hljs-comment">// main.cpp</span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-keyword">extern</span> <span class="hljs-type">void</span> <span class="hljs-title">callStaticFunction</span><span class="hljs-params">()</span></span>;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-built_in">callStaticFunction</span>();  <span class="hljs-comment">// 输出: GlobalVar: 10</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="3-静态成员变量和静态成员函数"><a href="#3-静态成员变量和静态成员函数" class="headerlink" title="3. 静态成员变量和静态成员函数"></a>3. 静态成员变量和静态成员函数</h3><h4 id="定义和语法-2"><a href="#定义和语法-2" class="headerlink" title="定义和语法"></a>定义和语法</h4><p>静态成员变量和静态成员函数是在类中使用 <code>static</code> 关键字声明的成员变量或成员函数。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyClass</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-type">static</span> <span class="hljs-type">int</span> staticVar;  <span class="hljs-comment">// 静态成员变量</span><br><br>    <span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">void</span> <span class="hljs-title">staticFunction</span><span class="hljs-params">()</span> </span>&#123;  <span class="hljs-comment">// 静态成员函数</span><br>        std::cout &lt;&lt; <span class="hljs-string">&quot;StaticVar: &quot;</span> &lt;&lt; staticVar &lt;&lt; std::endl;<br>    &#125;<br>&#125;;<br><br><span class="hljs-type">int</span> MyClass::staticVar = <span class="hljs-number">20</span>;  <span class="hljs-comment">// 静态成员变量的初始化</span><br></code></pre></td></tr></table></figure><h4 id="特性-2"><a href="#特性-2" class="headerlink" title="特性"></a>特性</h4><ul><li><strong>属于类本身</strong>：静态成员变量和静态成员函数属于类本身，而非类的实例，所有对象共享同一个静态成员变量。</li><li><strong>只能访问静态成员</strong>：静态成员函数只能访问静态成员变量和静态成员函数，不能访问非静态成员变量和非静态成员函数。</li><li><strong>必须在类外初始化</strong>：静态成员变量必须在类外进行初始化。</li></ul><h4 id="应用场景-2"><a href="#应用场景-2" class="headerlink" title="应用场景"></a>应用场景</h4><ul><li><strong>实现类级别的计数器、共享资源等</strong>：静态成员变量可以用于实现类级别的计数器或共享资源。</li><li><strong>提供与对象无关的类级别接口</strong>：静态成员函数可以用于提供与对象无关的类级别接口。</li></ul><h4 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyClass</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-type">static</span> <span class="hljs-type">int</span> staticVar;  <span class="hljs-comment">// 静态成员变量</span><br><br>    <span class="hljs-function"><span class="hljs-type">static</span> <span class="hljs-type">void</span> <span class="hljs-title">staticFunction</span><span class="hljs-params">()</span> </span>&#123;  <span class="hljs-comment">// 静态成员函数</span><br>        std::cout &lt;&lt; <span class="hljs-string">&quot;StaticVar: &quot;</span> &lt;&lt; staticVar &lt;&lt; std::endl;<br>    &#125;<br>&#125;;<br><br><span class="hljs-type">int</span> MyClass::staticVar = <span class="hljs-number">20</span>;  <span class="hljs-comment">// 静态成员变量的初始化</span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    MyClass::<span class="hljs-built_in">staticFunction</span>();  <span class="hljs-comment">// 输出: StaticVar: 20</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="四、static-关键字的注意事项"><a href="#四、static-关键字的注意事项" class="headerlink" title="四、static 关键字的注意事项"></a>四、static 关键字的注意事项</h2><h3 id="1-静态局部变量的初始化顺序问题"><a href="#1-静态局部变量的初始化顺序问题" class="headerlink" title="1. 静态局部变量的初始化顺序问题"></a>1. 静态局部变量的初始化顺序问题</h3><p>静态局部变量的初始化顺序是按照它们在代码中出现的顺序进行的。如果多个静态局部变量之间存在依赖关系，可能会导致未定义行为。</p><h3 id="2-静态成员变量的初始化顺序问题"><a href="#2-静态成员变量的初始化顺序问题" class="headerlink" title="2. 静态成员变量的初始化顺序问题"></a>2. 静态成员变量的初始化顺序问题</h3><p>静态成员变量的初始化顺序是按照它们在类中声明的顺序进行的。如果多个静态成员变量之间存在依赖关系，可能会导致未定义行为。</p><h3 id="3-静态成员函数不能访问非静态成员变量和非静态成员函数的原因"><a href="#3-静态成员函数不能访问非静态成员变量和非静态成员函数的原因" class="headerlink" title="3. 静态成员函数不能访问非静态成员变量和非静态成员函数的原因"></a>3. 静态成员函数不能访问非静态成员变量和非静态成员函数的原因</h3><p>静态成员函数没有 <code>this</code> 指针，因此无法访问非静态成员变量和非静态成员函数。</p><h3 id="4-避免滥用-static-关键字"><a href="#4-避免滥用-static-关键字" class="headerlink" title="4. 避免滥用 static 关键字"></a>4. 避免滥用 static 关键字</h3><p>滥用 <code>static</code> 关键字可能会导致代码的可读性和可维护性下降。因此，在使用 <code>static</code> 关键字时，应谨慎考虑其适用场景。</p><h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>本文详细探讨了 <code>static</code> 关键字在 C++ 中的多种用途，包括静态局部变量、静态全局变量和静态函数、静态成员变量和静态成员函数。通过代码示例，我们深入理解了 <code>static</code> 关键字的工作原理和应用场景。</p><p>理解 <code>static</code> 关键字的重要性不言而喻，它能够帮助我们更好地管理内存、控制作用域以及实现一些特定的编程模式。在实际编程中，合理使用 <code>static</code> 关键字可以提高代码的可读性和可维护性。</p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>开发</category>
      
      <category>cpp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>c++</tag>
      
      <tag>cpp</tag>
      
      <tag>c++基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 日志处理最佳实践：使用logging模块构建高效日志系统</title>
    <link href="/2024/12/15/%E5%BC%80%E5%8F%91/Python/Python-003%EF%BC%9APython%20%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%9A%E4%BD%BF%E7%94%A8logging%E6%A8%A1%E5%9D%97%E6%9E%84%E5%BB%BA%E9%AB%98%E6%95%88%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/"/>
    <url>/2024/12/15/%E5%BC%80%E5%8F%91/Python/Python-003%EF%BC%9APython%20%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%9A%E4%BD%BF%E7%94%A8logging%E6%A8%A1%E5%9D%97%E6%9E%84%E5%BB%BA%E9%AB%98%E6%95%88%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="Python-日志处理最佳实践：使用-logging-模块构建高效日志系统"><a href="#Python-日志处理最佳实践：使用-logging-模块构建高效日志系统" class="headerlink" title="Python 日志处理最佳实践：使用 logging 模块构建高效日志系统"></a>Python 日志处理最佳实践：使用 <code>logging</code> 模块构建高效日志系统</h1><p>在现代软件开发中，日志记录是不可或缺的一部分。它不仅可以帮助我们调试和排查问题，还可以为系统的运行状态提供有价值的信息。Python 的标准库 <code>logging</code> 是一个强大且灵活的日志记录工具，但在实际项目中，如何高效地使用它却是一个值得探讨的话题。本文将结合实际项目经验，总结使用 <code>logging</code> 模块的最佳实践，并提供一个完整的代码示例。</p><hr><h2 id="1-为什么选择-logging-模块？"><a href="#1-为什么选择-logging-模块？" class="headerlink" title="1. 为什么选择 logging 模块？"></a>1. 为什么选择 <code>logging</code> 模块？</h2><p>相比于简单的 <code>print</code> 语句，<code>logging</code> 模块提供了以下优势：</p><ul><li><strong>日志级别</strong>：支持 <code>DEBUG</code>、<code>INFO</code>、<code>WARNING</code>、<code>ERROR</code> 和 <code>CRITICAL</code> 五种日志级别，便于控制日志的详细程度。</li><li><strong>日志格式化</strong>：可以自定义日志的输出格式，包括时间、日志级别、模块名称等。</li><li><strong>日志处理器</strong>：支持将日志输出到文件、控制台、网络等多种目标。</li><li><strong>日志轮转</strong>：支持日志文件的自动轮转，避免日志文件过大。</li><li><strong>日志过滤器</strong>：可以过滤敏感信息，确保日志安全。</li></ul><hr><h2 id="2-最佳实践概述"><a href="#2-最佳实践概述" class="headerlink" title="2. 最佳实践概述"></a>2. 最佳实践概述</h2><p>在实际项目中，日志记录的最佳实践包括以下几点：</p><ol><li><strong>统一的日志配置</strong>：通过一个统一的日志配置函数，避免在每个模块中重复编写日志配置代码。</li><li><strong>根据模块名称自动命名日志文件</strong>：每个模块的日志存储在独立的文件中，便于管理和分析。</li><li><strong>日志轮转</strong>：使用 <code>RotatingFileHandler</code> 或 <code>TimedRotatingFileHandler</code> 管理日志文件的大小和数量。</li><li><strong>日志级别控制</strong>：在开发环境中使用 <code>DEBUG</code> 级别，在生产环境中使用 <code>INFO</code> 或 <code>WARNING</code> 级别。</li><li><strong>日志格式化</strong>：使用统一的日志格式，便于阅读和分析。</li><li><strong>日志安全</strong>：避免记录敏感信息，如密码、密钥等。</li></ol><hr><h2 id="3-示例项目结构"><a href="#3-示例项目结构" class="headerlink" title="3. 示例项目结构"></a>3. 示例项目结构</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus">my_project/<br>├── <span class="hljs-selector-tag">main</span><span class="hljs-selector-class">.py</span><br>├── module_a<span class="hljs-selector-class">.py</span><br>├── module_b<span class="hljs-selector-class">.py</span><br>├── logger_config<span class="hljs-selector-class">.py</span><br>└── logs/<br>    ├── module_a<span class="hljs-selector-class">.log</span><br>    ├── module_b<span class="hljs-selector-class">.log</span><br>    └── app.log<br></code></pre></td></tr></table></figure><hr><h2 id="4-统一的日志配置函数"><a href="#4-统一的日志配置函数" class="headerlink" title="4. 统一的日志配置函数"></a>4. 统一的日志配置函数</h2><p>为了减少重复代码，我们可以在一个单独的模块中定义一个统一的日志配置函数 <code>setup_logger</code>，并在每个模块中调用它。</p><h3 id="logger-config-py"><a href="#logger-config-py" class="headerlink" title="logger_config.py"></a><code>logger_config.py</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">from</span> logging.handlers <span class="hljs-keyword">import</span> RotatingFileHandler<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_logger</span>(<span class="hljs-params">module_name, log_dir=<span class="hljs-string">&quot;logs&quot;</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据模块名称配置日志记录器，并自动生成日志文件名。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    :param module_name: 模块名称，用于生成日志文件名</span><br><span class="hljs-string">    :param log_dir: 日志文件存储目录，默认为 &quot;logs&quot;</span><br><span class="hljs-string">    :return: 配置好的日志记录器</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 创建日志记录器</span><br>    logger = logging.getLogger(module_name)<br>    logger.setLevel(logging.DEBUG)  <span class="hljs-comment"># 设置全局日志级别为 DEBUG</span><br><br>    <span class="hljs-comment"># 配置日志格式化</span><br>    formatter = logging.Formatter(<span class="hljs-string">&quot;%(asctime)s - %(name)s - %(levelname)s - %(message)s&quot;</span>)<br><br>    <span class="hljs-comment"># 配置控制台处理器（输出到终端）</span><br>    console_handler = logging.StreamHandler()<br>    console_handler.setLevel(logging.INFO)  <span class="hljs-comment"># 控制台日志级别为 INFO</span><br>    console_handler.setFormatter(formatter)<br>    logger.addHandler(console_handler)<br><br>    <span class="hljs-comment"># 配置文件处理器（根据模块名称生成日志文件名）</span><br>    log_file = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;log_dir&#125;</span>/<span class="hljs-subst">&#123;module_name&#125;</span>.log&quot;</span><br>    file_handler = RotatingFileHandler(<br>        log_file,  <span class="hljs-comment"># 日志文件路径</span><br>        maxBytes=<span class="hljs-number">1024</span> * <span class="hljs-number">1024</span>,  <span class="hljs-comment"># 每个日志文件的最大大小（1MB）</span><br>        backupCount=<span class="hljs-number">5</span>  <span class="hljs-comment"># 保留的旧日志文件数量</span><br>    )<br>    file_handler.setLevel(logging.DEBUG)  <span class="hljs-comment"># 文件日志级别为 DEBUG</span><br>    file_handler.setFormatter(formatter)<br>    logger.addHandler(file_handler)<br><br>    <span class="hljs-keyword">return</span> logger<br></code></pre></td></tr></table></figure><hr><h2 id="5-主模块：main-py"><a href="#5-主模块：main-py" class="headerlink" title="5. 主模块：main.py"></a>5. 主模块：<code>main.py</code></h2><p>在主模块中，我们调用 <code>setup_logger</code> 函数来配置日志记录器，并记录日志。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> logger_config <span class="hljs-keyword">import</span> setup_logger<br><span class="hljs-keyword">import</span> module_a<br><span class="hljs-keyword">import</span> module_b<br><br><span class="hljs-comment"># 配置主模块的日志记录器</span><br>logger = setup_logger(<span class="hljs-string">&quot;main&quot;</span>)<br><br><span class="hljs-comment"># 记录日志</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    logger.info(<span class="hljs-string">&quot;主模块启动&quot;</span>)<br>    module_a.run()<br>    module_b.run()<br>    logger.info(<span class="hljs-string">&quot;主模块结束&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure><hr><h2 id="6-模块-A：module-a-py"><a href="#6-模块-A：module-a-py" class="headerlink" title="6. 模块 A：module_a.py"></a>6. 模块 A：<code>module_a.py</code></h2><p>在模块 A 中，我们同样调用 <code>setup_logger</code> 函数来配置日志记录器，并记录日志。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> logger_config <span class="hljs-keyword">import</span> setup_logger<br><br><span class="hljs-comment"># 配置模块 A 的日志记录器</span><br>logger = setup_logger(<span class="hljs-string">&quot;module_a&quot;</span>)<br><br><span class="hljs-comment"># 记录日志</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>():<br>    logger.debug(<span class="hljs-string">&quot;模块 A 的调试信息&quot;</span>)<br>    logger.info(<span class="hljs-string">&quot;模块 A 的普通信息&quot;</span>)<br>    logger.warning(<span class="hljs-string">&quot;模块 A 的警告信息&quot;</span>)<br>    logger.error(<span class="hljs-string">&quot;模块 A 的错误信息&quot;</span>)<br>    logger.critical(<span class="hljs-string">&quot;模块 A 的严重错误信息&quot;</span>)<br></code></pre></td></tr></table></figure><hr><h2 id="7-模块-B：module-b-py"><a href="#7-模块-B：module-b-py" class="headerlink" title="7. 模块 B：module_b.py"></a>7. 模块 B：<code>module_b.py</code></h2><p>在模块 B 中，我们同样调用 <code>setup_logger</code> 函数来配置日志记录器，并记录日志。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> logger_config <span class="hljs-keyword">import</span> setup_logger<br><br><span class="hljs-comment"># 配置模块 B 的日志记录器</span><br>logger = setup_logger(<span class="hljs-string">&quot;module_b&quot;</span>)<br><br><span class="hljs-comment"># 记录日志</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>():<br>    logger.debug(<span class="hljs-string">&quot;模块 B 的调试信息&quot;</span>)<br>    logger.info(<span class="hljs-string">&quot;模块 B 的普通信息&quot;</span>)<br>    logger.warning(<span class="hljs-string">&quot;模块 B 的警告信息&quot;</span>)<br>    logger.error(<span class="hljs-string">&quot;模块 B 的错误信息&quot;</span>)<br>    logger.critical(<span class="hljs-string">&quot;模块 B 的严重错误信息&quot;</span>)<br></code></pre></td></tr></table></figure><hr><h2 id="8-运行结果"><a href="#8-运行结果" class="headerlink" title="8. 运行结果"></a>8. 运行结果</h2><h3 id="控制台输出"><a href="#控制台输出" class="headerlink" title="控制台输出"></a>控制台输出</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - main - INFO - 主模块启动<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_a - INFO - 模块 A 的普通信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_a - WARNING - 模块 A 的警告信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_a - ERROR - 模块 A 的错误信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_a - CRITICAL - 模块 A 的严重错误信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_b - INFO - 模块 B 的普通信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_b - WARNING - 模块 B 的警告信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_b - ERROR - 模块 B 的错误信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_b - CRITICAL - 模块 B 的严重错误信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - main - INFO - 主模块结束<br></code></pre></td></tr></table></figure><h3 id="日志文件内容"><a href="#日志文件内容" class="headerlink" title="日志文件内容"></a>日志文件内容</h3><h4 id="logs-app-log（主模块日志）："><a href="#logs-app-log（主模块日志）：" class="headerlink" title="logs/app.log（主模块日志）："></a><code>logs/app.log</code>（主模块日志）：</h4><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - main - INFO - 主模块启动<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - main - INFO - 主模块结束<br></code></pre></td></tr></table></figure><h4 id="logs-module-a-log（模块-A-日志）："><a href="#logs-module-a-log（模块-A-日志）：" class="headerlink" title="logs/module_a.log（模块 A 日志）："></a><code>logs/module_a.log</code>（模块 A 日志）：</h4><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_a - DEBUG - 模块 A 的调试信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_a - INFO - 模块 A 的普通信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_a - WARNING - 模块 A 的警告信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_a - ERROR - 模块 A 的错误信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_a - CRITICAL - 模块 A 的严重错误信息<br></code></pre></td></tr></table></figure><h4 id="logs-module-b-log（模块-B-日志）："><a href="#logs-module-b-log（模块-B-日志）：" class="headerlink" title="logs/module_b.log（模块 B 日志）："></a><code>logs/module_b.log</code>（模块 B 日志）：</h4><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_b - DEBUG - 模块 B 的调试信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_b - INFO - 模块 B 的普通信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_b - WARNING - 模块 B 的警告信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_b - ERROR - 模块 B 的错误信息<br><span class="hljs-attribute">2023</span>-<span class="hljs-number">10</span>-<span class="hljs-number">01</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> - module_b - CRITICAL - 模块 B 的严重错误信息<br></code></pre></td></tr></table></figure><hr><h2 id="9-总结"><a href="#9-总结" class="headerlink" title="9. 总结"></a>9. 总结</h2><p>通过统一的日志配置函数 <code>setup_logger</code>，我们实现了以下目标：</p><ol><li><strong>自动生成日志文件名</strong>：根据模块名称动态生成日志文件名，避免手动指定文件名。</li><li><strong>减少重复代码</strong>：在每个模块中只需调用 <code>setup_logger</code> 函数，无需重复编写日志配置代码。</li><li><strong>灵活的日志管理</strong>：每个模块的日志存储在独立的文件中，便于管理和分析。</li><li><strong>日志轮转</strong>：通过 <code>RotatingFileHandler</code>，可以自动管理日志文件的大小和数量。</li><li><strong>日志级别控制</strong>：在开发环境中使用 <code>DEBUG</code> 级别，在生产环境中使用 <code>INFO</code> 或 <code>WARNING</code> 级别。</li><li><strong>日志格式化</strong>：使用统一的日志格式，便于阅读和分析。</li><li><strong>日志安全</strong>：避免记录敏感信息，如密码、密钥等。</li></ol><p>这种设计模式非常适合大型项目，能够有效提高日志管理的灵活性和可维护性。</p>]]></content>
    
    
    <categories>
      
      <category>开发</category>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python开发中常用工具函数</title>
    <link href="/2024/12/14/%E5%BC%80%E5%8F%91/Python/Python-002%EF%BC%9Apython%E5%BC%80%E5%8F%91%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E5%B7%A5%E5%85%B7%E5%87%BD%E6%95%B0/"/>
    <url>/2024/12/14/%E5%BC%80%E5%8F%91/Python/Python-002%EF%BC%9Apython%E5%BC%80%E5%8F%91%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E5%B7%A5%E5%85%B7%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="1-函数执行时间统计装饰器"><a href="#1-函数执行时间统计装饰器" class="headerlink" title="1.函数执行时间统计装饰器"></a>1.函数执行时间统计装饰器</h2><h3 id="功能："><a href="#功能：" class="headerlink" title="功能："></a>功能：</h3><p>用于统计函数执行的时间，常用于性能优化。</p><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> perf_counter<br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> wraps<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">timeit</span>(<span class="hljs-params">loop: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    函数执行失败时，重试</span><br><span class="hljs-string"></span><br><span class="hljs-string">    :param loop: 循环执行次数</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># 校验参数，参数值不正确时使用默认参数</span><br>    <span class="hljs-keyword">if</span> loop &lt; <span class="hljs-number">1</span>:<br>        loop = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decorator</span>(<span class="hljs-params">func</span>):<br><span class="hljs-meta">        @wraps(<span class="hljs-params">func</span>)</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">wrapper</span>(<span class="hljs-params">*args, **kwargs</span>):<br>            sum_time: <span class="hljs-built_in">float</span> = <span class="hljs-number">0.0</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(loop):<br>                start_time: <span class="hljs-built_in">float</span> = perf_counter()<br>                ret = func(*args, **kwargs)<br>                end_time: <span class="hljs-built_in">float</span> = perf_counter()<br>                sum_time += end_time - start_time<br><br>            <span class="hljs-built_in">print</span>(<br>                <span class="hljs-string">f&quot;函数(<span class="hljs-subst">&#123;func.__name__&#125;</span>)共执行<span class="hljs-subst">&#123;loop&#125;</span>次，平均执行时间 <span class="hljs-subst">&#123;(sum_time/loop):<span class="hljs-number">.3</span>f&#125;</span> 秒&quot;</span><br>            )<br>            <span class="hljs-keyword">return</span> ret<br><br>        <span class="hljs-keyword">return</span> wrapper<br><br>    <span class="hljs-keyword">return</span> decorator<br><br></code></pre></td></tr></table></figure><p>在 Python 开发中，工具函数（Utility Functions）是提高代码效率和可维护性的重要组成部分。这些函数可以帮助我们完成常见的任务，如时间统计、类型检查、字符串处理、文件操作等。以下是一些常用的工具函数及其使用场景。</p><hr><h2 id="2-类型检查装饰器"><a href="#2-类型检查装饰器" class="headerlink" title="2. 类型检查装饰器"></a>2. 类型检查装饰器</h2><h3 id="功能：-1"><a href="#功能：-1" class="headerlink" title="功能："></a>功能：</h3><p>用于检查函数参数的类型是否符合预期。</p><h3 id="示例代码：-1"><a href="#示例代码：-1" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> wraps<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">type_check</span>(<span class="hljs-params">func</span>):<br><span class="hljs-meta">    @wraps(<span class="hljs-params">func</span>)</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">wrapper</span>(<span class="hljs-params">*args, **kwargs</span>):<br>        sig = inspect.signature(func)<br>        params = sig.parameters<br>        <span class="hljs-keyword">for</span> i, (arg_name, arg_value) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(params, args)):<br>            param = params[arg_name]<br>            <span class="hljs-keyword">if</span> param.annotation != inspect.Parameter.empty <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(arg_value, param.annotation):<br>                <span class="hljs-keyword">raise</span> TypeError(<span class="hljs-string">f&quot;参数 <span class="hljs-subst">&#123;arg_name&#125;</span> 的类型应为 <span class="hljs-subst">&#123;param.annotation&#125;</span>，但传入的是 <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(arg_value)&#125;</span>&quot;</span>)<br>        <span class="hljs-keyword">return</span> func(*args, **kwargs)<br>    <span class="hljs-keyword">return</span> wrapper<br><br><span class="hljs-meta">@type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">a: <span class="hljs-built_in">int</span>, b: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>    <span class="hljs-keyword">return</span> a + b<br><br><span class="hljs-built_in">print</span>(add(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))  <span class="hljs-comment"># 正常</span><br><span class="hljs-built_in">print</span>(add(<span class="hljs-number">1</span>, <span class="hljs-string">&quot;2&quot;</span>))  <span class="hljs-comment"># 抛出 TypeError</span><br></code></pre></td></tr></table></figure><hr><h2 id="3-重试装饰器"><a href="#3-重试装饰器" class="headerlink" title="3. 重试装饰器"></a>3. 重试装饰器</h2><h3 id="功能：-2"><a href="#功能：-2" class="headerlink" title="功能："></a>功能：</h3><p>用于在函数执行失败时自动重试。</p><h3 id="示例代码：-2"><a href="#示例代码：-2" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> wraps<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">retry</span>(<span class="hljs-params">max_retries=<span class="hljs-number">3</span>, delay=<span class="hljs-number">1</span></span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decorator</span>(<span class="hljs-params">func</span>):<br><span class="hljs-meta">        @wraps(<span class="hljs-params">func</span>)</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">wrapper</span>(<span class="hljs-params">*args, **kwargs</span>):<br>            retries = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">while</span> retries &lt; max_retries:<br>                <span class="hljs-keyword">try</span>:<br>                    <span class="hljs-keyword">return</span> func(*args, **kwargs)<br>                <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>                    retries += <span class="hljs-number">1</span><br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;重试 <span class="hljs-subst">&#123;retries&#125;</span>/<span class="hljs-subst">&#123;max_retries&#125;</span>: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br>                    time.sleep(delay)<br>            <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">f&quot;达到最大重试次数 <span class="hljs-subst">&#123;max_retries&#125;</span>&quot;</span>)<br>        <span class="hljs-keyword">return</span> wrapper<br>    <span class="hljs-keyword">return</span> decorator<br><br><span class="hljs-meta">@retry(<span class="hljs-params">max_retries=<span class="hljs-number">3</span>, delay=<span class="hljs-number">1</span></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">flaky_function</span>():<br>    <span class="hljs-keyword">if</span> random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>) == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&quot;随机失败&quot;</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;成功&quot;</span><br><br><span class="hljs-built_in">print</span>(flaky_function())<br></code></pre></td></tr></table></figure><hr><h2 id="4-批量处理函数"><a href="#4-批量处理函数" class="headerlink" title="4. 批量处理函数"></a>4. 批量处理函数</h2><h3 id="功能：-3"><a href="#功能：-3" class="headerlink" title="功能："></a>功能：</h3><p>用于将一个列表或迭代器分批处理。</p><h3 id="示例代码：-3"><a href="#示例代码：-3" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">batch_process</span>(<span class="hljs-params">data, batch_size=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(data), batch_size):<br>        <span class="hljs-keyword">yield</span> data[i:i + batch_size]<br><br>data = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">101</span>))<br><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> batch_process(data, batch_size=<span class="hljs-number">20</span>):<br>    <span class="hljs-built_in">print</span>(batch)<br></code></pre></td></tr></table></figure><h3 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h3><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">[1, 2, 3, <span class="hljs-string">...</span>, 20]<br>[21, 22, 33, <span class="hljs-string">...</span>, 40]<br><span class="hljs-string">...</span><br>[81, 82, 83, <span class="hljs-string">...</span>, 100]<br></code></pre></td></tr></table></figure><hr><h2 id="5-文件操作工具函数"><a href="#5-文件操作工具函数" class="headerlink" title="5. 文件操作工具函数"></a>5. 文件操作工具函数</h2><h3 id="功能：-4"><a href="#功能：-4" class="headerlink" title="功能："></a>功能：</h3><p>用于读取、写入和处理文件。</p><h3 id="示例代码：-4"><a href="#示例代码：-4" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_file</span>(<span class="hljs-params">file_path</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">return</span> f.read()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">write_file</span>(<span class="hljs-params">file_path, content</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        f.write(content)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">append_file</span>(<span class="hljs-params">file_path, content</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;a&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        f.write(content)<br><br><span class="hljs-comment"># 示例</span><br>write_file(<span class="hljs-string">&quot;example.txt&quot;</span>, <span class="hljs-string">&quot;Hello, World!\n&quot;</span>)<br>append_file(<span class="hljs-string">&quot;example.txt&quot;</span>, <span class="hljs-string">&quot;This is a new line.\n&quot;</span>)<br><span class="hljs-built_in">print</span>(read_file(<span class="hljs-string">&quot;example.txt&quot;</span>))<br></code></pre></td></tr></table></figure><hr><h2 id="6-字符串处理工具函数"><a href="#6-字符串处理工具函数" class="headerlink" title="6. 字符串处理工具函数"></a>6. 字符串处理工具函数</h2><h3 id="功能：-5"><a href="#功能：-5" class="headerlink" title="功能："></a>功能：</h3><p>用于字符串的格式化、分割、替换等操作。</p><h3 id="示例代码：-5"><a href="#示例代码：-5" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">format_string</span>(<span class="hljs-params">template, **kwargs</span>):<br>    <span class="hljs-keyword">return</span> template.<span class="hljs-built_in">format</span>(**kwargs)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">split_string</span>(<span class="hljs-params">text, delimiter=<span class="hljs-string">&quot;,&quot;</span></span>):<br>    <span class="hljs-keyword">return</span> text.split(delimiter)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">replace_string</span>(<span class="hljs-params">text, old, new</span>):<br>    <span class="hljs-keyword">return</span> text.replace(old, new)<br><br><span class="hljs-comment"># 示例</span><br>template = <span class="hljs-string">&quot;My name is &#123;name&#125; and I am &#123;age&#125; years old.&quot;</span><br><span class="hljs-built_in">print</span>(format_string(template, name=<span class="hljs-string">&quot;Alice&quot;</span>, age=<span class="hljs-number">30</span>))<br><br>text = <span class="hljs-string">&quot;apple,banana,cherry&quot;</span><br><span class="hljs-built_in">print</span>(split_string(text))<br><br>text = <span class="hljs-string">&quot;Hello, World!&quot;</span><br><span class="hljs-built_in">print</span>(replace_string(text, <span class="hljs-string">&quot;World&quot;</span>, <span class="hljs-string">&quot;Python&quot;</span>))<br></code></pre></td></tr></table></figure><hr><h2 id="7-日志记录工具函数"><a href="#7-日志记录工具函数" class="headerlink" title="7. 日志记录工具函数"></a>7. 日志记录工具函数</h2><h3 id="功能：-6"><a href="#功能：-6" class="headerlink" title="功能："></a>功能：</h3><p>用于记录程序的运行日志。</p><h3 id="示例代码：-6"><a href="#示例代码：-6" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">from</span> logging.handlers <span class="hljs-keyword">import</span> RotatingFileHandler<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_logger</span>(<span class="hljs-params">module_name, log_dir=<span class="hljs-string">&quot;logs&quot;</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据模块名称配置日志记录器，并自动生成日志文件名。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    :param module_name: 模块名称，用于生成日志文件名</span><br><span class="hljs-string">    :param log_dir: 日志文件存储目录，默认为 &quot;logs&quot;</span><br><span class="hljs-string">    :return: 配置好的日志记录器</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 创建日志记录器</span><br>    logger = logging.getLogger(module_name)<br>    logger.setLevel(logging.DEBUG)  <span class="hljs-comment"># 设置全局日志级别为 DEBUG</span><br><br>    <span class="hljs-comment"># 配置日志格式化</span><br>    formatter = logging.Formatter(<span class="hljs-string">&quot;%(asctime)s - %(name)s - %(levelname)s - %(message)s&quot;</span>)<br><br>    <span class="hljs-comment"># 配置控制台处理器（输出到终端）</span><br>    console_handler = logging.StreamHandler()<br>    console_handler.setLevel(logging.INFO)  <span class="hljs-comment"># 控制台日志级别为 INFO</span><br>    console_handler.setFormatter(formatter)<br>    logger.addHandler(console_handler)<br><br>    <span class="hljs-comment"># 配置文件处理器（根据模块名称生成日志文件名）</span><br>    log_file = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;log_dir&#125;</span>/<span class="hljs-subst">&#123;module_name&#125;</span>.log&quot;</span><br>    file_handler = RotatingFileHandler(<br>        log_file,  <span class="hljs-comment"># 日志文件路径</span><br>        maxBytes=<span class="hljs-number">1024</span> * <span class="hljs-number">1024</span>,  <span class="hljs-comment"># 每个日志文件的最大大小（1MB）</span><br>        backupCount=<span class="hljs-number">5</span>  <span class="hljs-comment"># 保留的旧日志文件数量</span><br>    )<br>    file_handler.setLevel(logging.DEBUG)  <span class="hljs-comment"># 文件日志级别为 DEBUG</span><br>    file_handler.setFormatter(formatter)<br>    logger.addHandler(file_handler)<br><br>    <span class="hljs-keyword">return</span> logger<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> loguru <span class="hljs-keyword">import</span> logger<br><span class="hljs-keyword">import</span> sys<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_logger</span>(<span class="hljs-params">module_name, log_dir=<span class="hljs-string">&quot;logs&quot;</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据模块名称配置日志记录器，并自动生成日志文件名。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    :param module_name: 模块名称，用于生成日志文件名</span><br><span class="hljs-string">    :param log_dir: 日志文件存储目录，默认为 &quot;logs&quot;</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 配置日志格式</span><br>    log_format = (<br>        <span class="hljs-string">&quot;&lt;green&gt;&#123;time:YYYY-MM-DD HH:mm:ss&#125;&lt;/green&gt; | &quot;</span><br>        <span class="hljs-string">&quot;&lt;level&gt;&#123;level: &lt;8&#125;&lt;/level&gt; | &quot;</span><br>        <span class="hljs-string">&quot;&lt;cyan&gt;&#123;name&#125;&lt;/cyan&gt;:&lt;cyan&gt;&#123;function&#125;&lt;/cyan&gt;:&lt;cyan&gt;&#123;line&#125;&lt;/cyan&gt; - &lt;level&gt;&#123;message&#125;&lt;/level&gt;&quot;</span><br>    )<br><br>    <span class="hljs-comment"># 配置控制台日志</span><br>    logger.remove()  <span class="hljs-comment"># 移除默认的日志处理器</span><br>    logger.add(sys.stdout, <span class="hljs-built_in">format</span>=log_format, level=<span class="hljs-string">&quot;INFO&quot;</span>)<br><br>    <span class="hljs-comment"># 配置文件日志（根据模块名称生成日志文件名）</span><br>    log_file = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;log_dir&#125;</span>/<span class="hljs-subst">&#123;module_name&#125;</span>.log&quot;</span><br>    logger.add(<br>        log_file,  <span class="hljs-comment"># 日志文件路径</span><br>        <span class="hljs-built_in">format</span>=log_format,<br>        level=<span class="hljs-string">&quot;DEBUG&quot;</span>,<br>        rotation=<span class="hljs-string">&quot;10 MB&quot;</span>,  <span class="hljs-comment"># 日志文件轮转大小</span><br>        retention=<span class="hljs-string">&quot;7 days&quot;</span>,  <span class="hljs-comment"># 保留日志文件的时间</span><br>        compression=<span class="hljs-string">&quot;zip&quot;</span>,  <span class="hljs-comment"># 日志文件压缩格式</span><br>    )<br><br>    <span class="hljs-keyword">return</span> logger<br></code></pre></td></tr></table></figure><hr><h2 id="8-缓存工具函数"><a href="#8-缓存工具函数" class="headerlink" title="8. 缓存工具函数"></a>8. 缓存工具函数</h2><h3 id="功能：-7"><a href="#功能：-7" class="headerlink" title="功能："></a>功能：</h3><p>用于缓存函数的返回值，避免重复计算。</p><h3 id="示例代码：-7"><a href="#示例代码：-7" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> lru_cache<br><br><span class="hljs-meta">@lru_cache(<span class="hljs-params">maxsize=<span class="hljs-number">128</span></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fibonacci</span>(<span class="hljs-params">n</span>):<br>    <span class="hljs-keyword">if</span> n &lt;= <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> n<br>    <span class="hljs-keyword">return</span> fibonacci(n - <span class="hljs-number">1</span>) + fibonacci(n - <span class="hljs-number">2</span>)<br><br><span class="hljs-built_in">print</span>(fibonacci(<span class="hljs-number">10</span>))  <span class="hljs-comment"># 计算并缓存</span><br><span class="hljs-built_in">print</span>(fibonacci(<span class="hljs-number">10</span>))  <span class="hljs-comment"># 直接从缓存中获取</span><br></code></pre></td></tr></table></figure><hr><h2 id="9-随机数据生成工具函数"><a href="#9-随机数据生成工具函数" class="headerlink" title="9. 随机数据生成工具函数"></a>9. 随机数据生成工具函数</h2><h3 id="功能：-8"><a href="#功能：-8" class="headerlink" title="功能："></a>功能：</h3><p>用于生成随机数据，如随机字符串、随机数等。</p><h3 id="示例代码：-8"><a href="#示例代码：-8" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> string<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_string</span>(<span class="hljs-params">length=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;&#x27;</span>.join(random.choices(string.ascii_letters + string.digits, k=length))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_number</span>(<span class="hljs-params">start=<span class="hljs-number">1</span>, end=<span class="hljs-number">100</span></span>):<br>    <span class="hljs-keyword">return</span> random.randint(start, end)<br><br><span class="hljs-comment"># 示例</span><br><span class="hljs-built_in">print</span>(random_string(<span class="hljs-number">15</span>))<br><span class="hljs-built_in">print</span>(random_number(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><hr><h2 id="10-并发执行工具函数"><a href="#10-并发执行工具函数" class="headerlink" title="10. 并发执行工具函数"></a>10. 并发执行工具函数</h2><h3 id="功能：-9"><a href="#功能：-9" class="headerlink" title="功能："></a>功能：</h3><p>用于并发执行多个任务。</p><h3 id="示例代码：-9"><a href="#示例代码：-9" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> concurrent.futures<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">task</span>(<span class="hljs-params">n</span>):<br>    <span class="hljs-keyword">return</span> n * n<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parallel_execute</span>(<span class="hljs-params">tasks</span>):<br>    <span class="hljs-keyword">with</span> concurrent.futures.ThreadPoolExecutor() <span class="hljs-keyword">as</span> executor:<br>        results = <span class="hljs-built_in">list</span>(executor.<span class="hljs-built_in">map</span>(task, tasks))<br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># 示例</span><br>tasks = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]<br><span class="hljs-built_in">print</span>(parallel_execute(tasks))<br></code></pre></td></tr></table></figure><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上是 Python 中常用的一些工具函数，涵盖了时间统计、类型检查、重试机制、文件操作、字符串处理、日志记录、缓存、随机数据生成和并发执行等多个方面。这些工具函数可以帮助我们提高代码的可读性、可维护性和性能。</p><p>根据具体需求，你可以选择合适的工具函数，或者将它们组合起来，构建更复杂的工具链。</p>]]></content>
    
    
    <categories>
      
      <category>开发</category>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深入解析 `xml.dom.minidom`：从入门到精通</title>
    <link href="/2024/12/13/%E5%BC%80%E5%8F%91/Python/Python-001%EF%BC%9A%E4%BD%BF%E7%94%A8xml.dom.minidom%E8%A7%A3%E6%9E%90xml%E6%96%87%E4%BB%B6/"/>
    <url>/2024/12/13/%E5%BC%80%E5%8F%91/Python/Python-001%EF%BC%9A%E4%BD%BF%E7%94%A8xml.dom.minidom%E8%A7%A3%E6%9E%90xml%E6%96%87%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="深入解析-xml-dom-minidom：从入门到精通"><a href="#深入解析-xml-dom-minidom：从入门到精通" class="headerlink" title="深入解析 xml.dom.minidom：从入门到精通"></a>深入解析 <code>xml.dom.minidom</code>：从入门到精通</h1><p>在 Python 中，处理 XML 文件是一个常见的需求。Python 提供了多种库来解析和操作 XML，其中 <code>xml.dom.minidom</code> 是一个轻量级的 DOM（Document Object Model）解析器，适合处理小型 XML 文件。本文将通过实际例子，详细解释 <code>xml.dom.minidom</code> 的用法，具体到每个方法的功能和使用场景。</p><hr><h2 id="1-什么是-xml-dom-minidom？"><a href="#1-什么是-xml-dom-minidom？" class="headerlink" title="1. 什么是 xml.dom.minidom？"></a>1. 什么是 <code>xml.dom.minidom</code>？</h2><p><code>xml.dom.minidom</code> 是 Python 标准库中的一个模块，用于解析和操作 XML 文档。它实现了 W3C 的 DOM Level 2 规范，提供了一种将 XML 文档表示为树结构的方式。通过 <code>xml.dom.minidom</code>，你可以轻松地读取、修改和生成 XML 文件。</p><hr><h2 id="2-安装与导入"><a href="#2-安装与导入" class="headerlink" title="2. 安装与导入"></a>2. 安装与导入</h2><p><code>xml.dom.minidom</code> 是 Python 标准库的一部分，因此无需安装，直接导入即可使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> xml.dom.minidom<br></code></pre></td></tr></table></figure><hr><h2 id="3-核心方法详解"><a href="#3-核心方法详解" class="headerlink" title="3. 核心方法详解"></a>3. 核心方法详解</h2><h3 id="3-1-parse-file-解析-XML-文件"><a href="#3-1-parse-file-解析-XML-文件" class="headerlink" title="3.1 parse(file) - 解析 XML 文件"></a>3.1 <code>parse(file)</code> - 解析 XML 文件</h3><p><code>parse(file)</code> 方法用于解析一个 XML 文件，并返回一个 <code>Document</code> 对象。</p><h4 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 解析 XML 文件</span><br>dom = xml.dom.minidom.parse(<span class="hljs-string">&quot;example.xml&quot;</span>)<br><span class="hljs-built_in">print</span>(dom.toxml())  <span class="hljs-comment"># 输出整个 XML 文档</span><br></code></pre></td></tr></table></figure><h4 id="解释："><a href="#解释：" class="headerlink" title="解释："></a>解释：</h4><ul><li><code>parse(file)</code>：传入一个文件路径或文件对象，返回一个 <code>Document</code> 对象。</li><li><code>toxml()</code>：将 <code>Document</code> 对象转换为字符串形式的 XML。</li></ul><hr><h3 id="3-2-parseString-string-解析-XML-字符串"><a href="#3-2-parseString-string-解析-XML-字符串" class="headerlink" title="3.2 parseString(string) - 解析 XML 字符串"></a>3.2 <code>parseString(string)</code> - 解析 XML 字符串</h3><p><code>parseString(string)</code> 方法用于解析一个 XML 字符串，并返回一个 <code>Document</code> 对象。</p><h4 id="示例：-1"><a href="#示例：-1" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> xml.dom <span class="hljs-keyword">import</span> minidom<br><span class="hljs-comment"># 解析 XML 字符串</span><br>xml_string = <span class="hljs-string">&quot;&quot;&quot;&lt;library&gt;</span><br><span class="hljs-string">    &lt;book id=&quot;1&quot;&gt;</span><br><span class="hljs-string">        &lt;title&gt;Python Basics&lt;/title&gt;</span><br><span class="hljs-string">        &lt;author&gt;John Doe&lt;/author&gt;</span><br><span class="hljs-string">    &lt;/book&gt;</span><br><span class="hljs-string">    &lt;book id=&quot;2&quot;&gt;</span><br><span class="hljs-string">        &lt;title&gt;Advanced Python&lt;/title&gt;</span><br><span class="hljs-string">        &lt;author&gt;Jane Smith&lt;/author&gt;</span><br><span class="hljs-string">    &lt;/book&gt;</span><br><span class="hljs-string">&lt;/library&gt;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>dom = minidom.parseString(xml_string)<br><span class="hljs-built_in">print</span>(dom.toxml())  <span class="hljs-comment"># 输出整个 XML 文档</span><br></code></pre></td></tr></table></figure><h4 id="解释：-1"><a href="#解释：-1" class="headerlink" title="解释："></a>解释：</h4><ul><li><code>parseString(string)</code>：传入一个 XML 字符串，返回一个 <code>Document</code> 对象。</li><li><code>toxml()</code>：将 <code>Document</code> 对象转换为字符串形式的 XML。</li></ul><hr><h3 id="3-3-getElementsByTagName-tagname-获取指定标签的元素"><a href="#3-3-getElementsByTagName-tagname-获取指定标签的元素" class="headerlink" title="3.3 getElementsByTagName(tagname) - 获取指定标签的元素"></a>3.3 <code>getElementsByTagName(tagname)</code> - 获取指定标签的元素</h3><p><code>getElementsByTagName(tagname)</code> 方法用于获取所有指定标签名的元素，返回一个包含 <code>Element</code> 对象的列表。</p><h4 id="示例：-2"><a href="#示例：-2" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 获取所有 &lt;book&gt; 元素</span><br>books = dom.getElementsByTagName(<span class="hljs-string">&quot;book&quot;</span>)<br><span class="hljs-keyword">for</span> book <span class="hljs-keyword">in</span> books:<br>    <span class="hljs-built_in">print</span>(book.toxml())  <span class="hljs-comment"># 输出每个 &lt;book&gt; 元素的 XML</span><br></code></pre></td></tr></table></figure><h4 id="解释：-2"><a href="#解释：-2" class="headerlink" title="解释："></a>解释：</h4><ul><li><code>getElementsByTagName(tagname)</code>：传入标签名，返回一个包含所有匹配元素的列表。</li><li><code>toxml()</code>：将元素转换为字符串形式的 XML。</li></ul><hr><h3 id="3-4-getAttribute-name-获取元素的属性值"><a href="#3-4-getAttribute-name-获取元素的属性值" class="headerlink" title="3.4 getAttribute(name) - 获取元素的属性值"></a>3.4 <code>getAttribute(name)</code> - 获取元素的属性值</h3><p><code>getAttribute(name)</code> 方法用于获取元素的指定属性值。</p><h4 id="示例：-3"><a href="#示例：-3" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 获取 &lt;book&gt; 元素的 &quot;id&quot; 属性</span><br>book = dom.getElementsByTagName(<span class="hljs-string">&quot;book&quot;</span>)[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(book.getAttribute(<span class="hljs-string">&quot;id&quot;</span>))  <span class="hljs-comment"># 输出 &quot;1&quot;</span><br></code></pre></td></tr></table></figure><h4 id="解释：-3"><a href="#解释：-3" class="headerlink" title="解释："></a>解释：</h4><ul><li><code>getAttribute(name)</code>：传入属性名，返回属性值。</li></ul><hr><h3 id="3-5-setAttribute-name-value-设置元素的属性值"><a href="#3-5-setAttribute-name-value-设置元素的属性值" class="headerlink" title="3.5 setAttribute(name, value) - 设置元素的属性值"></a>3.5 <code>setAttribute(name, value)</code> - 设置元素的属性值</h3><p><code>setAttribute(name, value)</code> 方法用于设置元素的属性值。</p><h4 id="示例：-4"><a href="#示例：-4" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 设置 &lt;book&gt; 元素的 &quot;id&quot; 属性</span><br>book = dom.getElementsByTagName(<span class="hljs-string">&quot;book&quot;</span>)[<span class="hljs-number">0</span>]<br>book.setAttribute(<span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;2&quot;</span>)<br><span class="hljs-built_in">print</span>(book.getAttribute(<span class="hljs-string">&quot;id&quot;</span>))  <span class="hljs-comment"># 输出 &quot;2&quot;</span><br></code></pre></td></tr></table></figure><h4 id="解释：-4"><a href="#解释：-4" class="headerlink" title="解释："></a>解释：</h4><ul><li><code>setAttribute(name, value)</code>：传入属性名和属性值，设置元素的属性。</li></ul><hr><h3 id="3-6-createElement-tagName-创建新元素"><a href="#3-6-createElement-tagName-创建新元素" class="headerlink" title="3.6 createElement(tagName) - 创建新元素"></a>3.6 <code>createElement(tagName)</code> - 创建新元素</h3><p><code>createElement(tagName)</code> 方法用于创建一个新的元素节点。</p><h4 id="示例：-5"><a href="#示例：-5" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建一个新的 &lt;book&gt; 元素</span><br>new_book = dom.createElement(<span class="hljs-string">&quot;book&quot;</span>)<br>new_book.setAttribute(<span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;3&quot;</span>)<br>dom.documentElement.appendChild(new_book)<br><span class="hljs-built_in">print</span>(dom.toxml())  <span class="hljs-comment"># 输出更新后的 XML</span><br></code></pre></td></tr></table></figure><h4 id="解释：-5"><a href="#解释：-5" class="headerlink" title="解释："></a>解释：</h4><ul><li><code>createElement(tagName)</code>：传入标签名，创建一个新的元素节点。</li><li><code>appendChild(node)</code>：将新元素添加到文档中。</li></ul><hr><h3 id="3-7-createTextNode-data-创建文本节点"><a href="#3-7-createTextNode-data-创建文本节点" class="headerlink" title="3.7 createTextNode(data) - 创建文本节点"></a>3.7 <code>createTextNode(data)</code> - 创建文本节点</h3><p><code>createTextNode(data)</code> 方法用于创建一个包含文本内容的节点。</p><h4 id="示例：-6"><a href="#示例：-6" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建一个文本节点并添加到 &lt;book&gt; 元素中</span><br>text_node = dom.createTextNode(<span class="hljs-string">&quot;Python Programming&quot;</span>)<br>new_book = dom.createElement(<span class="hljs-string">&quot;book&quot;</span>)<br>new_book.appendChild(text_node)<br>dom.documentElement.appendChild(new_book)<br><span class="hljs-built_in">print</span>(dom.toxml())  <span class="hljs-comment"># 输出更新后的 XML</span><br></code></pre></td></tr></table></figure><h4 id="解释：-6"><a href="#解释：-6" class="headerlink" title="解释："></a>解释：</h4><ul><li><code>createTextNode(data)</code>：传入文本内容，创建一个文本节点。</li><li><code>appendChild(node)</code>：将文本节点添加到元素中。</li></ul><hr><h3 id="3-8-removeChild-node-删除子节点"><a href="#3-8-removeChild-node-删除子节点" class="headerlink" title="3.8 removeChild(node) - 删除子节点"></a>3.8 <code>removeChild(node)</code> - 删除子节点</h3><p><code>removeChild(node)</code> 方法用于删除指定的子节点。</p><h4 id="示例：-7"><a href="#示例：-7" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 删除第一个 &lt;book&gt; 元素</span><br>book = dom.getElementsByTagName(<span class="hljs-string">&quot;book&quot;</span>)[<span class="hljs-number">0</span>]<br>dom.documentElement.removeChild(book)<br><span class="hljs-built_in">print</span>(dom.toxml())  <span class="hljs-comment"># 输出更新后的 XML</span><br></code></pre></td></tr></table></figure><h4 id="解释：-7"><a href="#解释：-7" class="headerlink" title="解释："></a>解释：</h4><ul><li><code>removeChild(node)</code>：传入要删除的节点，从父节点中移除该节点。</li></ul><hr><h3 id="3-9-toxml-和-toprettyxml-生成-XML-字符串"><a href="#3-9-toxml-和-toprettyxml-生成-XML-字符串" class="headerlink" title="3.9 toxml() 和 toprettyxml() - 生成 XML 字符串"></a>3.9 <code>toxml()</code> 和 <code>toprettyxml()</code> - 生成 XML 字符串</h3><p><code>toxml()</code> 和 <code>toprettyxml()</code> 方法用于将 <code>Document</code> 对象转换为字符串形式的 XML。</p><h4 id="示例：-8"><a href="#示例：-8" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 生成 XML 字符串</span><br><span class="hljs-built_in">print</span>(dom.toxml())  <span class="hljs-comment"># 紧凑格式</span><br><span class="hljs-built_in">print</span>(dom.toprettyxml())  <span class="hljs-comment"># 带缩进和换行的格式</span><br></code></pre></td></tr></table></figure><h4 id="解释：-8"><a href="#解释：-8" class="headerlink" title="解释："></a>解释：</h4><ul><li><code>toxml()</code>：生成紧凑的 XML 字符串。</li><li><code>toprettyxml()</code>：生成带缩进和换行的格式化 XML 字符串。</li></ul><hr><h2 id="4-实际案例：图书管理系统"><a href="#4-实际案例：图书管理系统" class="headerlink" title="4. 实际案例：图书管理系统"></a>4. 实际案例：图书管理系统</h2><p>假设我们有一个 XML 文件 <code>books.xml</code>，内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">library</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">book</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;1&quot;</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>Python Basics<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">author</span>&gt;</span>John Doe<span class="hljs-tag">&lt;/<span class="hljs-name">author</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">book</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">book</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;2&quot;</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>Advanced Python<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">author</span>&gt;</span>Jane Smith<span class="hljs-tag">&lt;/<span class="hljs-name">author</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">book</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">library</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="4-1-读取-XML-文件"><a href="#4-1-读取-XML-文件" class="headerlink" title="4.1 读取 XML 文件"></a>4.1 读取 XML 文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> xml.dom.minidom<br><br><span class="hljs-comment"># 解析 XML 文件</span><br>dom = xml.dom.minidom.parse(<span class="hljs-string">&quot;books.xml&quot;</span>)<br><br><span class="hljs-comment"># 获取所有 &lt;book&gt; 元素</span><br>books = dom.getElementsByTagName(<span class="hljs-string">&quot;book&quot;</span>)<br><span class="hljs-keyword">for</span> book <span class="hljs-keyword">in</span> books:<br>    title = book.getElementsByTagName(<span class="hljs-string">&quot;title&quot;</span>)[<span class="hljs-number">0</span>].firstChild.data<br>    author = book.getElementsByTagName(<span class="hljs-string">&quot;author&quot;</span>)[<span class="hljs-number">0</span>].firstChild.data<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Title: <span class="hljs-subst">&#123;title&#125;</span>, Author: <span class="hljs-subst">&#123;author&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">Title:</span> <span class="hljs-string">Python</span> <span class="hljs-string">Basics,</span> <span class="hljs-attr">Author:</span> <span class="hljs-string">John</span> <span class="hljs-string">Doe</span><br><span class="hljs-attr">Title:</span> <span class="hljs-string">Advanced</span> <span class="hljs-string">Python,</span> <span class="hljs-attr">Author:</span> <span class="hljs-string">Jane</span> <span class="hljs-string">Smith</span><br></code></pre></td></tr></table></figure><h3 id="4-2-修改-XML-文件"><a href="#4-2-修改-XML-文件" class="headerlink" title="4.2 修改 XML 文件"></a>4.2 修改 XML 文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 修改第一个 &lt;book&gt; 的标题</span><br>book = dom.getElementsByTagName(<span class="hljs-string">&quot;book&quot;</span>)[<span class="hljs-number">0</span>]<br>title = book.getElementsByTagName(<span class="hljs-string">&quot;title&quot;</span>)[<span class="hljs-number">0</span>]<br>title.firstChild.data = <span class="hljs-string">&quot;Python for Beginners&quot;</span><br><br><span class="hljs-comment"># 保存修改后的 XML</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;books_updated.xml&quot;</span>, <span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    f.write(dom.toprettyxml())<br></code></pre></td></tr></table></figure><h4 id="输出（books-updated-xml）："><a href="#输出（books-updated-xml）：" class="headerlink" title="输出（books_updated.xml）："></a>输出（<code>books_updated.xml</code>）：</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">library</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">book</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;1&quot;</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>Python for Beginners<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">author</span>&gt;</span>John Doe<span class="hljs-tag">&lt;/<span class="hljs-name">author</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">book</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">book</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;2&quot;</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>Advanced Python<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">author</span>&gt;</span>Jane Smith<span class="hljs-tag">&lt;/<span class="hljs-name">author</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">book</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">library</span>&gt;</span><br></code></pre></td></tr></table></figure><hr><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p><code>xml.dom.minidom</code> 是一个功能强大且易于使用的 XML 解析库。通过本文的详细讲解和实际案例，你应该已经掌握了如何使用 <code>xml.dom.minidom</code> 来解析、修改和生成 XML 文件。无论是读取 XML 数据，还是动态生成 XML 文档，<code>xml.dom.minidom</code> 都能满足你的需求。</p><p>希望本文对你理解和使用 <code>xml.dom.minidom</code> 有所帮助！如果你有任何问题或需要进一步的帮助，请随时留言。</p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>开发</category>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>音视频开发09：SRT、ASS、SAA、VTT字幕介绍</title>
    <link href="/2024/12/12/%E5%BC%80%E5%8F%91/%E9%9F%B3%E8%A7%86%E9%A2%91/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%9109%EF%BC%9A%E5%AD%97%E5%B9%95%E4%BB%8B%E7%BB%8D/"/>
    <url>/2024/12/12/%E5%BC%80%E5%8F%91/%E9%9F%B3%E8%A7%86%E9%A2%91/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%9109%EF%BC%9A%E5%AD%97%E5%B9%95%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241209171329186.png" alt="image-20241209171329186"></p><h1 id="1-定义"><a href="#1-定义" class="headerlink" title="1. 定义"></a>1. 定义</h1><p>音视频的字幕是指以文本形式显示在屏幕上的内容，这些内容通常代表了音频中的对话、叙述以及其他重要的声音元素。字幕不仅有助于提高视频内容的理解度，而且对于那些有听力障碍或是在静音环境中观看视频的人来说尤为重要。此外，字幕还可以用于翻译不同语言的内容，从而扩大视频的受众范围</p><h1 id="2-字幕的种类"><a href="#2-字幕的种类" class="headerlink" title="2. 字幕的种类"></a>2. 字幕的种类</h1><p>根据其功能和使用场景，字幕可以分为几种不同类型：</p><ul><li><strong>开放式字幕（硬编码字幕）</strong>：这类字幕直接嵌入到视频帧中，成为视频的一部分，无法关闭。它们适用于需要永久性地将字幕信息与视频结合的情况。</li><li><strong>隐藏式字幕（软字幕&#x2F;外挂字幕）</strong>：用户可以选择是否开启这类字幕，并且可以根据需要调整样式。它们通常作为独立文件存在，如SRT、ASS等格式，便于管理和更新。</li><li><strong>实时字幕</strong>：这种字幕是在直播或其他即时活动中生成的，能够快速响应现场发生的口语交流，为观众提供即时的文字反馈</li></ul><h1 id="3-字幕格式"><a href="#3-字幕格式" class="headerlink" title="3. 字幕格式"></a>3. 字幕格式</h1><p>常见的字幕格式包括：</p><ol><li>**SRT (SubRip Text)**：最常见的字幕格式之一，文本文件，每行包含时间码和字幕内容。</li><li>**ASS (Advanced SubStation Alpha)**：功能强大的字幕格式，支持复杂的字幕样式和特效。</li><li>**SSA (SubStation Alpha)**：早期的字幕格式，功能不如ASS强大，较少使用。</li><li>**VTT (WebVTT)**：用于网页视频的字幕格式，类似于SRT<a href="https://blog.csdn.net/AlvinCasper/article/details/113066446">5</a>。</li></ol><h3 id="3-1-SRT字幕"><a href="#3-1-SRT字幕" class="headerlink" title="3.1 SRT字幕"></a>3.1 SRT字幕</h3><p>SRT（SubRip Text）字幕格式以其简洁性和广泛的兼容性著称，它由纯文本构成，每个字幕段落包括四个主要部分：序号、时间码、字幕文本以及一个空白行来分隔不同的字幕片段。下面我们将通过具体的例子详细介绍SRT字幕文件的结构和使用方法。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241209172234191.png" alt="image-20241209172234191"></p><h4 id="SRT-字幕的基本结构"><a href="#SRT-字幕的基本结构" class="headerlink" title="SRT 字幕的基本结构"></a>SRT 字幕的基本结构</h4><h5 id="序号"><a href="#序号" class="headerlink" title="序号"></a>序号</h5><p>每个字幕段都有一个唯一的序号，这个数字从1开始递增，表示字幕出现的顺序。虽然序号本身并不直接影响播放时的显示效果，但在实际应用中，它有助于保持字幕的逻辑顺序正确无误。</p><h5 id="时间码"><a href="#时间码" class="headerlink" title="时间码"></a>时间码</h5><p>紧跟在序号之后的是时间码，它定义了字幕何时出现及消失。时间码遵循<code>hh:mm:ss,mmm --&gt; hh:mm:ss,mmm</code>的格式，其中<code>hh</code>代表小时，<code>mm</code>代表分钟，<code>ss</code>代表秒，而<code>mmm</code>则表示毫秒。例如：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs dns"><span class="hljs-number">1</span><br><span class="hljs-number">00</span>:<span class="hljs-number">00:29,740</span> --&gt; <span class="hljs-number">00</span>:<span class="hljs-number">00:31,280</span><br>福姬套餐~<br></code></pre></td></tr></table></figure><p>在这个例子中，字幕“福姬套餐~”将在视频播放到第29秒740毫秒时出现，并持续到第31秒280毫秒后消失。</p><h5 id="字幕文本"><a href="#字幕文本" class="headerlink" title="字幕文本"></a>字幕文本</h5><p>接下来是具体的字幕文本，它可以是一行或多行。对于多语言字幕，不同语言的内容可以通过换行分隔开来。例如，在同一时间段内可以先显示英文再显示中文翻译：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs dns"><span class="hljs-number">2</span><br><span class="hljs-number">00</span>:<span class="hljs-number">00:31,400</span> --&gt; <span class="hljs-number">00:00:32,240</span><br>炒炸酱面<br></code></pre></td></tr></table></figure><p>这里，“炒炸酱面”会在指定的时间范围内显示在屏幕上。</p><h5 id="空白行"><a href="#空白行" class="headerlink" title="空白行"></a>空白行</h5><p>每一组字幕之间必须有一个空行来分隔不同的字幕片段，这标志着上一段字幕的结束和新一段字幕的开始。空白行的存在确保了字幕文件的可读性和结构清晰度。</p><h4 id="样式与格式化示例"><a href="#样式与格式化示例" class="headerlink" title="样式与格式化示例"></a>样式与格式化示例</h4><p>尽管SRT本质上是一个简单的文本文件，但它也支持一些基本的HTML标签来进行格式化，如粗体（<code>&lt;b&gt;</code>）、斜体（<code>&lt;i&gt;</code>）、下划线（<code>&lt;u&gt;</code>）以及字体颜色（<code>&lt;font color=&quot;...&quot;&gt;</code>）。需要注意的是，并非所有的播放器都支持这些样式标记，因此在实际使用时应该考虑目标平台的支持情况。例如：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs xml">3<br>00:00:32,360 --&gt; 00:00:33,300<br><span class="hljs-tag">&lt;<span class="hljs-name">b</span>&gt;</span>炒拉面<span class="hljs-tag">&lt;/<span class="hljs-name">b</span>&gt;</span><br></code></pre></td></tr></table></figure><p>这段代码会让“炒拉面”以粗体形式显示。</p><h4 id="实际应用中的SRT文件示例"><a href="#实际应用中的SRT文件示例" class="headerlink" title="实际应用中的SRT文件示例"></a>实际应用中的SRT文件示例</h4><p>以下是一个完整的SRT字幕文件示例，摘自电影《阿凡达》的中英双语字幕：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ada"><span class="hljs-number">3</span><br><span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">39</span>,<span class="hljs-number">770</span> <span class="hljs-comment">--&gt; 00:00:41,880</span><br>在经历了一场人生巨变之后<br><span class="hljs-keyword">When</span> I was lying there <span class="hljs-keyword">in</span> the VA hospital ...<br></code></pre></td></tr></table></figure><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-number">4</span><br><span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">42</span>,<span class="hljs-number">550</span> <span class="hljs-comment">--&gt; 00:00:44,690</span><br>我被送进了退伍军人管理局医院 ...<br><span class="hljs-keyword">with</span> a big hole blown <span class="hljs-keyword">through</span> <span class="hljs-keyword">the</span> <span class="hljs-keyword">middle</span> <span class="hljs-keyword">of</span> <span class="hljs-keyword">my</span> life,<br></code></pre></td></tr></table></figure><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs dns"><span class="hljs-number">5</span><br><span class="hljs-number">00</span>:<span class="hljs-number">00:45,590</span> --&gt; <span class="hljs-number">00:00:48,120</span><br>那段时间我经常会梦到自己在飞翔 ...<br>I started having these dreams of flying.<br></code></pre></td></tr></table></figure><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs dns"><span class="hljs-number">6</span><br><span class="hljs-number">00</span>:<span class="hljs-number">00:49,740</span> --&gt; <span class="hljs-number">00</span>:<span class="hljs-number">00:51,520</span><br>终获自由<br>I was free.<br></code></pre></td></tr></table></figure><p>在这个例子中，我们可以看到每一段字幕都有其对应的序号、时间码和文本内容，并且通过空白行分隔开各个字幕段。</p><h4 id="编码方式"><a href="#编码方式" class="headerlink" title="编码方式"></a>编码方式</h4><p>SRT文件默认采用Windows-1252编码，但也可以保存为其他编码格式，如UTF-8或UTF-16，带或不带字节顺序标记（BOM）。为了确保跨平台兼容性，建议尽量使用UTF-8编码。特别是当涉及到非ASCII字符时，正确的编码选择尤为重要，因为错误的编码可能导致乱码问题。</p><h4 id="创建与编辑SRT文件"><a href="#创建与编辑SRT文件" class="headerlink" title="创建与编辑SRT文件"></a>创建与编辑SRT文件</h4><p>创建SRT文件的过程相对直接，用户可以直接用文本编辑器手动编写，也可以借助专门的字幕编辑软件，如Aegisub、Subtitle Edit等。此外，还有在线服务和移动应用程序可以帮助生成或调整现有的SRT文件。对于批量处理或者需要高精度的时间轴同步，还可以考虑利用语音识别技术自动生成初步的SRT文件，然后进行人工校对和完善。</p><p>综上所述，SRT字幕格式凭借其易于理解和操作的特点，成为了视频字幕领域不可或缺的一部分。</p><h3 id="3-2-ASS字幕"><a href="#3-2-ASS字幕" class="headerlink" title="3.2 ASS字幕"></a>3.2 ASS字幕</h3><p>ASS（Advanced SubStation Alpha）是一种比SSA更为高级的字幕格式，它不仅支持基本的文本显示，还允许用户添加复杂的样式、特效以及时间轴控制。ASS 文件通常以 <code>.ass</code> 作为文件扩展名，并且是纯文本文件，这意味着它们可以用任何文本编辑器手工编辑，但必须注意遵循特定的规则以确保正确解析。</p><h4 id="ASS-字幕的基本结构"><a href="#ASS-字幕的基本结构" class="headerlink" title="ASS 字幕的基本结构"></a>ASS 字幕的基本结构</h4><p>一个典型的 ASS 文件由几个部分组成：<code>[Script Info]</code>、<code>[V4+ Styles]</code> 和 <code>[Events]</code>。每个部分都有其独特的功能和用途。</p><h5 id="Script-Info"><a href="#Script-Info" class="headerlink" title="[Script Info]"></a>[Script Info]</h5><p>这部分包含了脚本的头部信息和总体描述。例如：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">[<span class="hljs-keyword">Script </span>Info]<br><span class="hljs-comment">; This is a Sub Station Alpha v4 script.</span><br><span class="hljs-symbol">Title:</span> Neon Genesis Evangelion - Episode <span class="hljs-number">26</span> (neutral Spanish)<br><span class="hljs-keyword">Original </span><span class="hljs-keyword">Script: </span>RoRo<br><span class="hljs-keyword">Script </span>Updated <span class="hljs-keyword">By: </span>version <span class="hljs-number">2</span>.<span class="hljs-number">8</span>.<span class="hljs-number">01</span><br><span class="hljs-keyword">ScriptType: </span>v4.<span class="hljs-number">00</span>+<br><span class="hljs-symbol">Collisions:</span> <span class="hljs-keyword">Normal</span><br><span class="hljs-keyword"></span><span class="hljs-symbol">PlayResY:</span> <span class="hljs-number">600</span><br><span class="hljs-symbol">PlayDepth:</span> <span class="hljs-number">0</span><br><span class="hljs-symbol">Timer:</span> <span class="hljs-number">100</span>,<span class="hljs-number">00</span><br></code></pre></td></tr></table></figure><p>这里定义了诸如标题 (<code>Title</code>)、原作者 (<code>Original Script</code>) 等元数据，以及一些技术参数如播放分辨率 (<code>PlayResX</code>, <code>PlayResY</code>) 和颜色深度 (<code>PlayDepth</code>)。<code>Timer</code> 属性用于调整字幕的时间轴速度，不过现代渲染器通常会忽略这一设置。</p><h5 id="V4-Styles"><a href="#V4-Styles" class="headerlink" title="[V4+ Styles]"></a>[V4+ Styles]</h5><p>这是样式定义的部分，其中每一个被脚本使用的样式都应该在此处定义。格式如下：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs dns">[V4+ Styles]<br>Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding<br>Style: Default,Arial,<span class="hljs-number">20</span>,&amp;H00FFFFFF,&amp;H000000FF,&amp;H<span class="hljs-number">00000000</span>,&amp;H<span class="hljs-number">00000000,0</span>,<span class="hljs-number">0,0,0,100</span>,<span class="hljs-number">100,0,0,1</span>,<span class="hljs-number">0.5,0,2</span>,<span class="hljs-number">10,10,10,1</span><br></code></pre></td></tr></table></figure><p>每一项都对应着不同的视觉属性，比如字体名称 (<code>Fontname</code>)、字体大小 (<code>Fontsize</code>)、主要颜色 (<code>PrimaryColour</code>) 等等。<code>Alignment</code> 属性决定了字幕在屏幕上的对齐方式，而 <code>MarginL</code>, <code>MarginR</code>, <code>MarginV</code> 则设定了字幕与屏幕边缘的距离。</p><h5 id="Events"><a href="#Events" class="headerlink" title="[Events]"></a>[Events]</h5><p>这是最重要的部分之一，因为它包含了所有实际出现在屏幕上的内容。每条事件记录了一个具体的字幕或注释，并指定了它的开始时间和结束时间、所使用的样式以及其他可能的效果。例如：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[Events]<br><span class="hljs-keyword">Format</span>: Layer, <span class="hljs-keyword">Start</span>, <span class="hljs-keyword">End</span>, Style, <span class="hljs-type">Name</span>, MarginL, MarginR, MarginV, Effect, <span class="hljs-type">Text</span><br>Dialogue: <span class="hljs-number">0</span>,<span class="hljs-number">0</span>:<span class="hljs-number">00</span>:<span class="hljs-number">09.07</span>,<span class="hljs-number">0</span>:<span class="hljs-number">00</span>:<span class="hljs-number">13.14</span>,<span class="hljs-keyword">Default</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,,我的爱从相遇的那一刻起<br>Dialogue: <span class="hljs-number">0</span>,<span class="hljs-number">0</span>:<span class="hljs-number">00</span>:<span class="hljs-number">13.27</span>,<span class="hljs-number">0</span>:<span class="hljs-number">00</span>:<span class="hljs-number">17.41</span>,<span class="hljs-keyword">Default</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,,我的生活一切都被你占据<br></code></pre></td></tr></table></figure><p><code>Layer</code> 属性控制字幕的叠放顺序；<code>Start</code> 和 <code>End</code> 分别表示字幕出现和消失的时间点；<code>Style</code> 指向之前定义好的样式；<code>Text</code> 包含了要显示的具体文字。</p><h4 id="样式与格式化示例-1"><a href="#样式与格式化示例-1" class="headerlink" title="样式与格式化示例"></a>样式与格式化示例</h4><p>除了上述基础元素外，ASS 还提供了丰富的内联标签来实现更加精细的排版和动画效果。例如：</p><ul><li><code>\b1</code> 开启粗体，<code>\b0</code> 关闭粗体；</li><li><code>\i1</code> 开启斜体，<code>\i0</code> 关闭斜体；</li><li><code>\u1</code> 开启下划线，<code>\u0</code> 关闭下划线；</li><li><code>\k&lt;time&gt;</code> 用于卡拉 OK 效果，指定高亮移动的速度；</li><li><code>\move(&lt;x1&gt;, &lt;y1&gt;, &lt;x2&gt;, &lt;y2&gt;)</code> 实现字幕从一个位置移动到另一个位置；</li><li><code>\fad(&lt;in-time&gt;, &lt;out-time&gt;)</code> 创建淡入淡出效果。</li></ul><p>以下是一个使用这些标签的例子：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Dialogue</span>: <span class="hljs-number">0</span>,<span class="hljs-number">0</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span>.<span class="hljs-number">00</span>,<span class="hljs-number">0</span>:<span class="hljs-number">00</span>:<span class="hljs-number">05</span>.<span class="hljs-number">00</span>,Default,,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,,&#123;\b1\u1\c&amp;HFF0000&amp;&#125;这是一段&#123;\i1&#125;带&#123;\i0&#125;有&#123;\b0\u0&#125;样式的文本。<br><span class="hljs-attribute">Dialogue</span>: <span class="hljs-number">0</span>,<span class="hljs-number">0</span>:<span class="hljs-number">00</span>:<span class="hljs-number">05</span>.<span class="hljs-number">00</span>,<span class="hljs-number">0</span>:<span class="hljs-number">00</span>:<span class="hljs-number">10</span>.<span class="hljs-number">00</span>,Default,,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,,&#123;\move(<span class="hljs-number">100</span>,<span class="hljs-number">100</span>,<span class="hljs-number">500</span>,<span class="hljs-number">100</span>)\fad(<span class="hljs-number">1000</span>,<span class="hljs-number">1000</span>)&#125;这段文字会从左下角滑动到右下角并伴有淡入淡出。<br></code></pre></td></tr></table></figure><p>在这个例子中，第一行应用了粗体和下划线，同时将颜色更改为红色，并且中间有一部分为斜体；第二行则设置了从屏幕左侧到右侧的移动路径，并且加入了1秒的淡入淡出过渡。</p><h4 id="实际应用中的ASS文件示例"><a href="#实际应用中的ASS文件示例" class="headerlink" title="实际应用中的ASS文件示例"></a>实际应用中的ASS文件示例</h4><p>考虑到ASS的强大功能，它可以用来创建非常复杂且富有创意的字幕效果，尤其是在动漫领域。下面给出一段来自《命运石之门》的日语台词及其对应的ASS代码：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Dialogue</span>: <span class="hljs-number">0</span>,<span class="hljs-number">0</span>:<span class="hljs-number">01</span>:<span class="hljs-number">23</span>.<span class="hljs-number">45</span>,<span class="hljs-number">0</span>:<span class="hljs-number">01</span>:<span class="hljs-number">27</span>.<span class="hljs-number">67</span>,CustomStyle,,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,,&#123;\an7\b1\c&amp;HFFFF00&amp;\<span class="hljs-number">3</span>c&amp;H0000FF&amp;&#125;OKERU！<br><span class="hljs-attribute">Dialogue</span>: <span class="hljs-number">0</span>,<span class="hljs-number">0</span>:<span class="hljs-number">01</span>:<span class="hljs-number">27</span>.<span class="hljs-number">67</span>,<span class="hljs-number">0</span>:<span class="hljs-number">01</span>:<span class="hljs-number">31</span>.<span class="hljs-number">89</span>,CustomStyle,,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,,&#123;\an8\i1\k50&#125;我们终于可以一起制作游戏了！<br></code></pre></td></tr></table></figure><p>这里，“CustomStyle” 是预先定义好的一种样式，而 <code>&#123;&#125;</code> 内的内容则是额外添加的临时样式覆盖。第一句台词位于屏幕顶部中央（<code>an7</code>），并且采用了黄色字体配蓝色边框；第二句则位于顶部右侧（<code>an8</code>），带有卡拉 OK 式的逐字高亮效果（<code>\k50</code> 表示每个字符高亮持续50毫秒）。</p><h4 id="编码方式-1"><a href="#编码方式-1" class="headerlink" title="编码方式"></a>编码方式</h4><p>与SRT类似，ASS也推荐使用UTF-8编码，特别是当涉及到多语言支持时。正确的编码选择对于避免乱码至关重要。此外，由于ASS支持更多样化的字体和颜色配置，因此还需要确保目标设备上安装了相应的字体文件，或者考虑嵌入所需字体以便于跨平台播放。</p><h3 id="3-3-SSA字幕"><a href="#3-3-SSA字幕" class="headerlink" title="3.3 SSA字幕"></a>3.3 SSA字幕</h3><p>在上面我们介绍了ASS字幕，ASS字幕其实是SSA字幕格式的高级版本，在SSA的基础上增加了一些额外的功能。下面用一个例子介绍下ASS和SSA之间的区别和联系。</p><p>假设我们要为一段视频添加带有特效的字幕，这段视频中有一句台词：<strong>“欢迎来到未来世界”。</strong></p><h4 id="SSA-字幕文件示例"><a href="#SSA-字幕文件示例" class="headerlink" title="SSA 字幕文件示例"></a>SSA 字幕文件示例</h4><p>首先，我们看看如何用 SSA 格式创建这段字幕。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">[Script Info]<br>; This is a Sub Station Alpha v4 script.<br>Title: Future World - Episode 1<br>Original Script: Example Author<br>ScriptType: v4.00<br>Collisions: Normal<br>PlayResX: 640<br>PlayResY: 480<br>Timer: 100,00<br><br>[V4 Styles]<br>Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, TertiaryColour, BackColour, Bold, Italic, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, AlphaLevel, Encoding<br>Style: Default,Arial,20,&amp;H00FFFFFF,&amp;H000000FF,&amp;H00000000,&amp;H00000000,-1,0,1,2,2,2,10,10,10,0,0<br><br>[Events]<br>Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text<br>Dialogue: 0,0:00:05.00,0:00:07.00,Default,,0,0,0,,&#123;\c&amp;HFF0000&amp;&#125;欢迎&#123;\c&amp;H00FF00&amp;&#125;来到&#123;\c&amp;H0000FF&amp;&#125;未来世界！<br></code></pre></td></tr></table></figure><p>在这个 SSA 文件中：</p><ul><li><strong>[Script Info]</strong> 部分定义了脚本的基本信息，如标题、原作者等。</li><li><strong>[V4 Styles]</strong> 定义了一种默认样式 <code>Default</code>，指定了字体名称、大小、颜色等属性。</li><li><strong>[Events]</strong> 包含了一个对话事件，它设置了开始时间和结束时间，并使用了内联标签 <code>\c</code> 来改变文本的颜色，以实现多彩效果。</li></ul><h4 id="ASS-字幕文件示例"><a href="#ASS-字幕文件示例" class="headerlink" title="ASS 字幕文件示例"></a>ASS 字幕文件示例</h4><p>接下来，我们将相同的台词转换成 ASS 格式，同时增加一些额外的功能。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">[Script Info]<br>; This is an Advanced Sub Station Alpha v4+ script.<br>Title: Future World - Episode 1<br>ScriptType: v4.00+<br>Collisions: Normal<br>PlayResX: 640<br>PlayResY: 480<br>Timer: 100,00<br><br>[V4+ Styles]<br>Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding<br>Style: Default,Arial,20,&amp;H00FFFFFF,&amp;H000000FF,&amp;H00000000,&amp;H00000000,0,0,0,0,100,100,0,0,1,1,1,2,10,10,10,1<br><br>[Events]<br>Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text<br>Dialogue: 0,0:00:05.00,0:00:07.00,Default,,0,0,0,,&#123;\an8\c&amp;HFF0000&amp;\b1&#125;欢迎&#123;\c&amp;H00FF00&amp;\b0&#125;来到&#123;\c&amp;H0000FF&amp;&#125;未来世界！&#123;\fad(500,500)&#125;<br></code></pre></td></tr></table></figure><p>在这个 ASS 文件中：</p><ul><li><strong>[Script Info]</strong> 部分同样提供了脚本信息，但指明这是 v4.00+ 版本。</li><li><strong>[V4+ Styles]</strong> 中增加了更多样式属性，例如 <code>ScaleX</code>, <code>ScaleY</code>, <code>Spacing</code>, <code>Angle</code> 等，允许对文本进行更精细的控制。</li><li><strong>[Events]</strong> 部分不仅实现了与 SSA 相同的多彩效果，还加入了粗体切换 (<code>\b1</code>, <code>\b0</code>) 和淡入淡出效果 (<code>\fad</code>)。</li></ul><h4 id="区别和联系"><a href="#区别和联系" class="headerlink" title="区别和联系"></a>区别和联系</h4><h5 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h5><ol><li><strong>版本</strong>：ASS 是基于 SSA v4.00+ 的改进版本，具有更多的功能和支持。</li><li><strong>样式定义</strong>：ASS 在 <code>[V4+ Styles]</code> 中引入了额外的样式属性，如 <code>ScaleX</code>, <code>ScaleY</code>, <code>Spacing</code>, <code>Angle</code> 等。</li><li><strong>内联标签支持</strong>：ASS 扩展了 SSA 的内联标签功能，新增了如 <code>\t</code> (临时样式覆盖) 等标签，并增强了现有标签的效果，比如卡拉 OK 效果 (<code>\k</code>)。</li><li><strong>高级特效与动画</strong>：ASS 支持更复杂的动画效果，包括路径上的移动 (\move)，旋转，缩放变换等。</li><li><strong>字符编码</strong>：ASS 设计之初就考虑到了更好的国际化支持，对于非拉丁字符集有更好的处理方式。</li></ol><h5 id="联系"><a href="#联系" class="headerlink" title="联系"></a>联系</h5><ul><li><strong>结构相似</strong>：两者都遵循类似的文件结构，包括 <code>[Script Info]</code>, <code>[V4/V4+ Styles]</code>, <code>[Events]</code> 等部分。</li><li><strong>向后兼容</strong>：大多数现代播放器能够解析 SSA 和 ASS 文件，因为 ASS 向后兼容 SSA。</li><li><strong>基本功能一致</strong>：在基础功能上，SSA 和 ASS 提供了相同的核心特性，如设置字体、颜色、对齐方式等。</li></ul><p>综上所述，尽管 SSA 和 ASS 在细节上有许多不同之处，但它们共享很多共同点，特别是对于普通用户来说，两者都能很好地满足日常字幕制作的需求。然而，如果你需要利用更先进的特效或希望确保最佳的跨平台兼容性，那么选择 ASS 可能会是一个更好的决定。</p><h3 id="3-4-VTT字幕"><a href="#3-4-VTT字幕" class="headerlink" title="3.4 VTT字幕"></a>3.4 VTT字幕</h3><p>WebVTT（Web Video Text Tracks）是一种专门为网络视频设计的字幕格式，它被广泛应用于 HTML5 <code>&lt;video&gt;</code> 元素中。WebVTT 文件通常以 <code>.vtt</code> 作为扩展名，并且是纯文本文件，易于编辑和解析。与 SSA&#x2F;ASS 等格式相比，WebVTT 更加简洁，专注于提供基本的字幕功能，如时间戳、文本内容以及简单的样式控制。下面将通过一个具体的例子来详细介绍 WebVTT 的结构和用法。</p><h4 id="WebVTT-文件的基本结构"><a href="#WebVTT-文件的基本结构" class="headerlink" title="WebVTT 文件的基本结构"></a>WebVTT 文件的基本结构</h4><p>每个 WebVTT 文件都必须以 <code>WEBVTT</code> 开头，紧接着是一个空行，然后是各个字幕段落。每个段落由以下几部分组成：</p><ol><li><strong>标识符（可选）</strong>：可以为每个字幕段落指定一个唯一的标识符。</li><li><strong>时间码</strong>：定义了字幕显示的时间范围，格式为 <code>HH:MM:SS.mmm --&gt; HH:MM:SS.mmm</code>，其中 <code>HH</code> 表示小时，<code>MM</code> 表示分钟，<code>SS</code> 表示秒，<code>mmm</code> 表示毫秒。</li><li><strong>设置（可选）</strong>：用于指定字幕的位置、对齐方式等属性。</li><li><strong>文本内容</strong>：即实际要显示的字幕文字，可以包含多行。</li></ol><h5 id="示例：简单对话场景"><a href="#示例：简单对话场景" class="headerlink" title="示例：简单对话场景"></a>示例：简单对话场景</h5><p>假设我们有一段视频，其中一个人说：“欢迎来到未来世界。” 我们可以创建如下的 WebVTT 文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">WEBVTT<br><br>1<br>00:00:05.000 --&gt; 00:00:07.000<br>欢迎来到未来世界。<br></code></pre></td></tr></table></figure><p>在这个例子中：</p><ul><li><code>WEBVTT</code> 表明这是一个 WebVTT 文件。</li><li>数字 <code>1</code> 是字幕段落的唯一标识符（可选），有助于管理和引用特定的字幕。</li><li>时间码 <code>00:00:05.000 --&gt; 00:00:07.000</code> 指定了字幕从第 5 秒开始，在第 7 秒结束。</li><li>最后一行是实际要显示的文字内容。</li></ul><h4 id="增强样式的使用"><a href="#增强样式的使用" class="headerlink" title="增强样式的使用"></a>增强样式的使用</h4><p>虽然 WebVTT 主要关注于提供清晰易读的字幕，但它也允许一定程度上的样式定制。例如，你可以使用 HTML 标签或内联 CSS 来改变字体颜色、大小、样式等。此外，WebVTT 还支持一些特殊的标签来实现更复杂的效果。</p><h5 id="示例：带样式的字幕"><a href="#示例：带样式的字幕" class="headerlink" title="示例：带样式的字幕"></a>示例：带样式的字幕</h5><p>考虑同样的台词，但我们想让它更引人注目，比如给“欢迎”两个字加上红色高亮，并让整个句子居中显示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">WEBVTT<br><br>1<br>00:00:05.000 --&gt; 00:00:07.000 line:50% position:50% align:center<br>&lt;c.red&gt;欢迎&lt;/c&gt;来到未来世界。<br></code></pre></td></tr></table></figure><p>在这个例子中：</p><ul><li><code>line:50% position:50% align:center</code> 设置了字幕在屏幕上的位置和对齐方式，使它位于屏幕中央。</li><li><code>&lt;c.red&gt;</code> 和 <code>&lt;/c&gt;</code> 是 WebVTT 特有的标记，用来包裹需要应用样式的文本。这里我们将“欢迎”两个字的颜色设置为红色。</li></ul><h4 id="使用-CSS-类进行样式化"><a href="#使用-CSS-类进行样式化" class="headerlink" title="使用 CSS 类进行样式化"></a>使用 CSS 类进行样式化</h4><p>对于更复杂的样式需求，可以结合 CSS 类来进行处理。首先，在 WebVTT 文件中定义类名，然后在网页上关联相应的 CSS 样式规则。</p><h5 id="示例：使用-CSS-类"><a href="#示例：使用-CSS-类" class="headerlink" title="示例：使用 CSS 类"></a>示例：使用 CSS 类</h5><p>继续上面的例子，这次我们将为“欢迎”添加一个自定义的 CSS 类：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">WEBVTT<br><br>1<br>00:00:05.000 --&gt; 00:00:07.000 line:50% position:50% align:center<br>&lt;c.vwelcome&gt;欢迎&lt;/c&gt;来到未来世界。<br></code></pre></td></tr></table></figure><p>接着，在网页中的 <code>&lt;style&gt;</code> 标签或者外部样式表里添加如下 CSS 规则：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-pseudo">::cue</span>(<span class="hljs-selector-class">.vwelcome</span>) &#123;<br>    <span class="hljs-attribute">color</span>: red;<br>    <span class="hljs-attribute">font-weight</span>: bold;<br>&#125;<br></code></pre></td></tr></table></figure><p>这样，当播放器渲染这个 WebVTT 文件时，“欢迎”这两个字不仅会变成红色，还会显得更加粗重。</p><h4 id="WebVTT-的高级特性"><a href="#WebVTT-的高级特性" class="headerlink" title="WebVTT 的高级特性"></a>WebVTT 的高级特性</h4><p>除了上述基本功能外，WebVTT 还支持其他类型的文本轨道，如章节列表、描述性音轨等。这些功能使得 WebVTT 成为了一个非常灵活且强大的工具，适用于各种多媒体应用场景。</p><h5 id="示例：章节列表"><a href="#示例：章节列表" class="headerlink" title="示例：章节列表"></a>示例：章节列表</h5><p>如果想要为视频添加章节导航，可以使用 WebVTT 创建一个章节轨道文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">WEBVTT<br><br>CHAPTERS<br><br>00:00:00.000 --&gt; 00:01:00.000<br>Introduction<br><br>00:01:00.000 --&gt; 00:02:30.000<br>Main Content<br><br>00:02:30.000 --&gt; 00:03:00.000<br>Conclusion<br></code></pre></td></tr></table></figure><p>这段代码定义了一个包含三个章节的列表，用户可以通过点击章节标题快速跳转到相应的时间点。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>WebVTT 是一种轻量级但功能丰富的字幕格式，特别适合在网络环境中使用。它的语法简单直观，易于学习和实现，同时提供了足够的灵活性来满足大多数字幕制作的需求。无论是简单的对话字幕还是带有复杂样式的提示信息，WebVTT 都能有效地支持。更重要的是，由于它是 HTML5 标准的一部分，因此可以在所有现代浏览器中无缝工作，确保了良好的兼容性和用户体验。</p><h1 id="4-字幕制作"><a href="#4-字幕制作" class="headerlink" title="4. 字幕制作"></a>4. 字幕制作</h1><h3 id="4-1-字幕的制作工具"><a href="#4-1-字幕的制作工具" class="headerlink" title="4.1 字幕的制作工具"></a>4.1 字幕的制作工具</h3><p>有许多专业的字幕制作软件可以帮助用户轻松制作和编辑字幕，例如：</p><ol><li>**KBuilderTools (小灰熊字幕制作软件)**：支持MV字幕制作，操作简单。</li><li><strong>Sayatoo卡拉字幕精灵</strong>：支持自定义设置字幕的字体、颜色、布局等参数。</li><li><strong>Srt字幕制作帮手</strong>：专门针对SRT字幕的编辑软件，操作简便。</li><li>**字幕大师 (OKVoice)**：采用高精准语音识别技术，支持自动匹配音视频中的语音与字幕。</li><li><strong>Arctime Pro</strong>：专业字幕制作软件，支持多种创新技术，如字幕块绑定、自动分轴等。</li><li><strong>Aegisub</strong>：免费、开源、跨平台的字幕编辑软件，支持多种语言编码。</li></ol><h3 id="4-2-字幕的制作步骤"><a href="#4-2-字幕的制作步骤" class="headerlink" title="4.2 字幕的制作步骤"></a>4.2 字幕的制作步骤</h3><ol><li><strong>导入视频和字幕文件</strong>：使用视频编辑软件导入视频文件和字幕文件，确保字幕文件与视频文件的内容对应。</li><li><strong>调整时间轴</strong>：在视频编辑软件中，调整字幕的出现时间和消失时间，使其与配音的时长和内容一致。</li><li><strong>调整字幕样式</strong>：根据需要，调整字幕的字体、字号、颜色、背景等，以使字幕更加清晰明确。</li><li><strong>预览和调整</strong>：预览整个视频，确保字幕与配音同步显示，并且字幕的内容清晰可读。</li><li><strong>导出视频</strong>：完成字幕的调整后，将视频导出为新的文件，确保字幕和配音同步的效果在最终的视频文件中得以保留。</li></ol><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink">https://github.com/chongzicbo/ReadWriteThink</a></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>开发</category>
      
      <category>音视频</category>
      
      <category>基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>音视频开发</tag>
      
      <tag>音视频基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>音视频开发08：音视频开发基本步骤和流程</title>
    <link href="/2024/12/11/%E5%BC%80%E5%8F%91/%E9%9F%B3%E8%A7%86%E9%A2%91/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%9108%EF%BC%9A%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%91%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4%E5%92%8C%E6%B5%81%E7%A8%8B/"/>
    <url>/2024/12/11/%E5%BC%80%E5%8F%91/%E9%9F%B3%E8%A7%86%E9%A2%91/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%9108%EF%BC%9A%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%91%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4%E5%92%8C%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241205155843570.png" alt="image-20241205155843570"></p><h1 id="音视频开发的基本步骤"><a href="#音视频开发的基本步骤" class="headerlink" title="音视频开发的基本步骤"></a>音视频开发的基本步骤</h1><p>音视频开发是一个涉及多个技术领域的复杂过程，其基本步骤涵盖了从数据采集到最终播放展示的各个环节。</p><h3 id="一、数据采集"><a href="#一、数据采集" class="headerlink" title="一、数据采集"></a><strong>一、数据采集</strong></h3><p> 数据采集是音视频开发的起始点，它解决的是数据从哪里来的问题。这一环节涉及到对声音和图像从现实世界转换为数字信号的操作。</p><ul><li><strong>设备选择</strong>：首先要选择合适的音视频采集设备。对于音频采集，麦克风是常见的设备；对于视频采集，摄像头则是常用的选择。不同的设备适用于不同的场景，例如在视频会议场景下，可能会选择高清摄像头来确保图像的清晰，而在语音通话场景中，会选用能够有效降低背景噪音的麦克风。</li><li><strong>配置采集参数</strong>：针对不同的采集设备和采集场景，需要配置不同的采集参数。对于音频，重要的参数有采样率、位宽、声道数等。采样率指每秒从连续信号中提取并组成离散信号的采样个数，例如人耳能听到的最高频率为20kHz，为了满足人耳的听觉要求，采样率通常为44.1kHz或48kHz；位宽涉及到振幅量化，常见的有8位、16位、32位；声道数有单声道、双声道等。对于视频，图像传输格式、图像格式、传输通道、分辨率、采样频率等是关键参数。分辨率如常见的720P（1280×720）、1080P（1920×1080）等，帧率也是一个重要参数，它代表单位时间内帧的数量，单位是fps，像24&#x2F;25fps是一般电影的帧率，30&#x2F;60fps是游戏常见的帧率，30帧可以接受，60帧会感觉更加流畅逼真，85fps以上人眼基本无法察觉出差异，更高帧率在视频里意义不大。</li><li><strong>开启采集设备</strong>：通过相应的API或SDK来开启采集设备。例如在Android系统中，视频采集可以使用Camera类，音频采集可以用AudioRecord来开启采集设备。在开启设备后，就可以进行数据的采集了。</li><li><strong>采集音频和视频数据</strong>：采集设备获取音频信号并将其转换为数字信号，对于视频则是获取图像的数字信号。在采集过程中，可能会面临一些问题，如音频采集时可能会遇到噪音、回声等干扰，视频采集可能会存在延时敏感、图像质量受环境光影响等问题。同时，采集到的数据可能需要进行一些初步的处理，如音频的降噪、视频的格式转换等，以便后续的操作。例如在音频采集流程中，采集端将音频模拟信号转换为数字信号后，进入音频处理模块，会有音频增益、噪声抑制、混音等操作。</li></ul><h3 id="二、数据处理"><a href="#二、数据处理" class="headerlink" title="二、数据处理"></a><strong>二、数据处理</strong></h3><p>数据处理是对采集或获取的音视频数据进行加工，以实现特定的应用需求。</p><ul><li><strong>解码</strong>：如果采集到的数据是经过编码压缩的，那么首先需要进行解码操作，将其转换为原始的音视频信号。例如对于采用H.264编码的视频数据和AAC编码的音频数据，需要对应的解码器将其还原成原始信号，以便进行后续处理。</li><li><strong>数据处理操作</strong>：这一步包含多种操作。在音频处理方面，有混音、降噪、声音特效等操作。例如混音可以将多个音频源混合成一个音频流；降噪可以减少环境噪音对音频质量的影响；声音特效则可以为音频添加特殊的效果，如回声、变声等。在视频处理方面，常见的有美颜、水印、自定义滤镜、自定义处理等。美颜操作可以通过磨皮、美白等手段来改善视频中的人物形象，磨皮可以采用均值模糊、高斯模糊和中值滤波等技术，同时可能结合人脸和皮肤检测技术；水印可以是播放器水印或者视频内嵌水印；自定义滤镜则可以根据需求创建各种独特的视觉效果。</li><li><strong>编码和压缩</strong>：处理后的音视频数据需要进行编码，以将其转换为压缩的音视频数据流，从而减小数据量，便于存储和传输。对于视频编码，其主要作用是将视频像素数据（如RGB，YUV等）压缩成为视频码流，常见的视频编码标准有H.264、H.265等。H.264具有低码率、高质量、高容错的特点，H.265在码率节省上相比H.264有较大优势，在相同RSNR下分别节省了48.3%和75.8%，但H.264在编码时间上有优势。对于音频编码，常见的编码标准有AAC、MP3等。编码的基本原理包括利用空间冗余（图像相邻像素之间有较强的相关性）、时间冗余（视频序的相邻图像之间内容相似）、编码冗余（不同像素值出现的概率不同）、知识冗余（规性的结构可由先验知识和背景知识得到）等来进行压缩。在编码之后，可能还会进行进一步的压缩操作，以进一步减小数据量，提高传输和存储效率。</li><li><strong>重采样和转码</strong>：重采样主要针对音频数据，改变采样率、位深度等参数，以适应不同的应用需求。例如将44100&#x2F;16&#x2F;2转成48000&#x2F;16&#x2F;2。转码是将音视频数据从一种格式转换为另一种格式，以适应不同的设备和应用环境，如将视频从MKV格式转码为MP4格式，以便在更多设备上播放。</li><li><strong>合成操作</strong>：在一些场景下，需要将多个音视频流进行合成，例如将多个音频轨道、视频轨道合并成一个完整的音视频文件。比如在视频编辑软件中，将不同片段的视频和对应的音频合成一个完整的视频作品。</li></ul><h3 id="三、数据传输"><a href="#三、数据传输" class="headerlink" title="三、数据传输"></a><strong>三、数据传输</strong></h3><p> 数据传输是将采集、处理后的音视频数据流传输到远程设备或服务器的过程。</p><ul><li><strong>建立连接</strong>：通过网络协议建立连接，常见的网络协议有TCP和UDP等。TCP协议是一种可靠的面向连接的协议，适用于对数据准确性要求较高的场景，如文件传输；UDP协议是一种无连接的协议，传输速度快但可靠性相对较低，适用于对实时性要求较高的场景，如视频直播中的部分数据传输。</li><li><strong>数据打包</strong>：将采集、处理后的音视频数据流打包为网络传输的格式，例如RTP、RTMP等协议。RTP（Real - time Transport Protocol）是一种实时传输协议，用于在IP网络上传输实时数据，通常与RTCP（RTP Control Protocol）一起使用，RTCP用于监控服务质量并提供反馈；RTMP（Real - Time Messaging Protocol）是基于TCP的实时消息传输协议，广泛用于直播领域。</li><li><strong>压缩（如果需要）</strong>：在传输之前，可能还会对数据流进行压缩，以减小数据量和网络带宽占用。这一步与前面数据处理中的编码压缩类似，但可能会根据传输网络的情况进行进一步的优化，例如根据网络带宽动态调整压缩率。</li><li><strong>传输数据</strong>：通过网络将打包和压缩后的数据流传输到远程设备或服务器。在传输过程中，需要考虑网络的稳定性、带宽等因素。如果网络带宽不足，可能会导致视频卡顿、音频中断等问题。</li></ul><h3 id="四、数据渲染与播放"><a href="#四、数据渲染与播放" class="headerlink" title="四、数据渲染与播放"></a><strong>四、数据渲染与播放</strong></h3><p>这一环节是将音视频数据流转换为可视化的音视频内容并播放的过程。</p><ul><li><strong>解码（再次解码）</strong>：将接收到的音视频数据流解码为原始的音视频信号。这一步与数据处理中的解码类似，但可能会因为传输过程中的一些情况（如数据丢失、错误等）而需要进行一些特殊的处理，例如纠错、数据恢复等。</li><li><strong>帧缓存</strong>：将解码后的视频帧存储到缓存中，以供后续渲染。缓存的大小和管理方式会影响视频播放的流畅性，如果缓存过小，可能会导致视频播放时频繁卡顿；如果缓存过大，可能会增加内存占用和延迟。</li><li><strong>视频渲染</strong>：通过OpenGL、DirectX等图形库将视频帧渲染到屏幕上，并可以添加相应的特效和滤镜等处理。这些图形库提供了强大的图形处理功能，可以实现视频的缩放、旋转、添加字幕等操作。</li><li><strong>音频渲染</strong>：将音频信号转换为声音，并通过扬声器或耳机播放出来。在播放过程中，需要确保音频的音量、音质等符合要求，并且要与视频保持同步。</li><li><strong>同步操作</strong>：将音视频进行同步，以保证音频和视频的时间戳一致，避免出现卡顿、不同步等问题。音视频同步是一个复杂的过程，需要考虑到采集、传输、处理等各个环节可能引入的时间差，通过调整播放速度、缓冲等方式来实现同步。</li></ul><h1 id="音视频开发流程包含哪些环节"><a href="#音视频开发流程包含哪些环节" class="headerlink" title="音视频开发流程包含哪些环节"></a>音视频开发流程包含哪些环节</h1><p>音视频开发流程包含多个环节，这些环节相互协作，共同完成从原始数据到可播放的音视频内容的转换。</p><h3 id="一、芯片与元件相关环节"><a href="#一、芯片与元件相关环节" class="headerlink" title="一、芯片与元件相关环节"></a><strong>一、芯片与元件相关环节</strong></h3><ul><li><strong>主芯片厂商环节</strong>：主芯片厂商，如海思、TI、安霸、联咏等，在音视频开发流程中处于基础地位。他们的核心在于各自的压缩算法，这些算法以SDK的方式开放给开发者使用。这些压缩算法是音视频编码的关键技术支撑，决定了音视频数据在采集、处理和传输过程中的压缩效率和质量。例如，一个好的视频压缩算法可以在保证视频质量的前提下，将视频数据量大大减小，从而节省存储空间和传输带宽。不同的主芯片厂商可能会有不同的技术优势和应用场景，开发者可以根据项目需求选择合适的芯片厂商的SDK进行开发。</li><li><strong>传感器与分立元件厂商环节</strong>：传感器（senor）厂商、镜头等分立元件厂商也是重要的一环。传感器用于采集视频的原始数据，如摄像头中的图像传感器，它的性能直接影响到采集到的视频图像的质量，包括分辨率、色彩还原度、低光性能等。镜头则影响着视频的视角、焦距等参数。这些分立元件与主芯片相互配合，为后续的视频采集和处理提供基础的硬件条件。例如，一个高质量的镜头可以提供更清晰、更广阔的视野，与高分辨率的传感器相结合，可以采集到高质量的视频数据。</li></ul><h3 id="二、模组开发环节"><a href="#二、模组开发环节" class="headerlink" title="二、模组开发环节"></a><strong>二、模组开发环节</strong></h3><p>模组厂商在音视频开发流程中起着承上启下的作用。他们买来芯片、sensor、镜头等进行一些基础的开发，得出一些视频采集的模组，实现视频的采集、编码、传输。在这个环节中，模组厂商会将各种硬件元件集成在一起，并进行软件层面的开发，以实现视频的采集功能。他们需要对采集到的视频数据进行编码，将原始的视频数据转换为适合存储和传输的格式，例如采用H.264或H.265等视频编码标准进行编码。同时，还要实现视频数据的传输功能，确保编码后的视频数据能够在不同的设备之间进行传输。这个环节的开发成果是视频采集模组，它是整个音视频系统的重要组成部分，为后续的视频服务器和上层应用开发提供了基础的视频数据源。</p><h3 id="三、视频服务器相关环节"><a href="#三、视频服务器相关环节" class="headerlink" title="三、视频服务器相关环节"></a><strong>三、视频服务器相关环节</strong></h3><ul><li><strong>视频服务器厂商环节</strong>：视频服务器厂商，如大拿等，在音视频开发流程中负责让编码后的视频能够通过外网传输。他们先将视频推到服务器上，再通过服务器让多个客户端进行多线程的访问。视频服务器需要具备强大的网络处理能力，能够处理大量的视频流数据。在这个环节中，涉及到网络协议的应用，如采用合适的流媒体传输协议（如RTSP、RTMP、HLS等）将视频数据传输到服务器，并在服务器端进行相应的处理，如视频的存储、转发等操作。服务器还需要提供多线程访问的支持，以满足多个客户端同时访问视频数据的需求。</li><li><strong>网络传输协议环节</strong>：网络传输协议在视频服务器相关环节中至关重要。不同的协议适用于不同的场景。RTSP（Real Time Streaming Protocol）是一种实时流传输协议，常用于视频监控等场景，它允许客户端对视频流进行暂停、快进等操作；RTMP（Real - Time Messaging Protocol）是基于TCP的实时消息传输协议，广泛应用于直播领域；HLS（HTTP Live Streaming）是由Apple公司定义的基于HTTP的流媒体实时传输协议，可实现流媒体的直播和点播，主要用于iOS系统。这些协议在视频数据的传输过程中，负责将视频数据从服务器传输到客户端，并且要保证视频的流畅播放和数据的准确性。</li></ul><h3 id="四、上层应用开发环节"><a href="#四、上层应用开发环节" class="headerlink" title="四、上层应用开发环节"></a><strong>四、上层应用开发环节</strong></h3><ul><li><strong>面向解决方案的方案开发商环节</strong>：面向解决方案的方案开发商买来模组、视频服务器，进行一些更上层的开发，如app，web管理等。他们会将模组采集到的视频数据和视频服务器提供的视频流进行整合，开发出各种应用。例如，他们可以整合模组与服务器，做出手机app，如将摄像头放在幼儿园中，家长就可以通过app对校园内的环境进行查看；也可以开发人脸识别门禁、打卡方案，将传感器接入人脸识别功能，连接数据库制作一套系统。在这个环节中，开发者需要具备多种技术能力，包括前端开发（如开发手机app的界面）、后端开发（如与数据库进行交互、处理业务逻辑）以及对音视频数据的处理能力（如在app中实现视频的播放、暂停、截图等功能）。</li><li><strong>工程商或销售商环节</strong>：工程商或销售商在音视频开发流程中更多地关注项目的实施和销售方面。工程商做工程的，大多不懂技术只懂施工，他们大多关心摄像头装在墙上还是天花板；用什么网线，网线怎么接；多久施工完。例如一个工程商接一个项目，买来解决方案进行某工厂、停车场的监控系统的建设。销售商则将智能家居方案卖给个人或家庭。虽然他们不直接参与音视频技术的开发，但他们在将音视频解决方案推向市场和实际应用场景方面起着重要的作用。</li></ul><h3 id="五、用户端播放环节"><a href="#五、用户端播放环节" class="headerlink" title="五、用户端播放环节"></a><strong>五、用户端播放环节</strong></h3><p>在用户端播放环节，涉及到将接收到的音视频数据进行解码、渲染和播放的过程。这一环节需要播放器软件或设备来实现。播放器需要支持多种音视频格式和编码标准，例如能够解码H.264编码的视频和AAC编码的音频。在播放过程中，要进行音视频的同步操作，以保证音频和视频的时间戳一致，避免出现卡顿、不同步等问题。同时，用户端设备（如手机、电脑、智能电视等）的性能也会影响播放效果，如设备的处理器速度、内存大小、显卡性能等会影响视频的解码速度和渲染质量。</p><h1 id="音视频开发的关键步骤有哪些"><a href="#音视频开发的关键步骤有哪些" class="headerlink" title="音视频开发的关键步骤有哪些"></a>音视频开发的关键步骤有哪些</h1><p>音视频开发的关键步骤是整个开发流程中的核心部分，它们对最终的开发效果和用户体验有着决定性的影响。</p><h3 id="一、编码与解码"><a href="#一、编码与解码" class="headerlink" title="一、编码与解码"></a><strong>一、编码与解码</strong></h3><ul><li><strong>编码的重要性和原理</strong>：编码是音视频开发中不可或缺的关键步骤。其主要目的是为了压缩数据，节省带宽和传输时间。原始的音视频数据量通常非常大，例如一个1080P30帧，32bit色彩时长为1秒的视频文件，如果按每一帧画面进行存储的话，数据大小将会达到:32bit * 30 * 1080 * 1920≈237MB的空间，通过编码可以大大减小这个数据量。视频编码的主要作用是将视频像素数据（RGB，YUV等）压缩成为视频码流，从而降低视频的数据量。编码的基本原理包括空间冗余（图像相邻像素之间有较强的相关性）、时间冗余（视频序的相邻图像之间内容相似）、编码冗余（不同像素值出现的概率不同）、知识冗余（规性的结构可由先验知识和背景知识得到）等。例如，在背景色全部是黑色的情况下，我们实际上没有必要按照视频大小（1124 * 772）存储黑色，我们可以将存储黑色的像素点抽离出来记录，只存储其他像素点的颜色即可。常见的视频编码标准有H.264、H.265等，H.264具有低码率、高质量、高容错的特点，H.265对H.264在码率节省上有较大的优势，在相同RSNR下分别节省了48.3%和75.8%，但H.264在编码时间上有聚到优势，对比VP9和H.265，H.265是vp9的6倍，vp9是H.264的将近40倍。对于音频编码，常见的编码标准有AAC、MP3等。</li><li><strong>解码的作用和实现方式</strong>：解码是编码的逆过程，其作用是将视频&#x2F;音频压缩编码数据，解码成为非压缩的视频&#x2F;音频原始数据。在整个音视频系统中，解码是非常重要也是最复杂的一个环节。解码可以使用软解码和硬解码两种方式。软解码就是利用CPU资源去解压缩数据，采用的方式是FFmpeg解码等。硬解码则是利用专门的硬件（如显卡等）来进行解码，对于iOS平台来说，可以使用VideoToolbox.Framework（该框架只能在iOS8.0及以上系统使用）硬解码视频数据。在播放音视频数据时，播放器端需要根据接收到的编码数据类型选择合适的解码方式进行解码，然后才能将解码后的原始数据进行播放。</li></ul><h3 id="二、数据传输相关步骤"><a href="#二、数据传输相关步骤" class="headerlink" title="二、数据传输相关步骤"></a><strong>二、数据传输相关步骤</strong></h3><ul><li><strong>选择合适的流媒体传输协议</strong>：在音视频开发中，选择合适的流媒体传输协议是关键。不同的协议适用于不同的场景和需求。例如RTMP（Real - Time Messaging Protocol）是目前主流的流媒体传输协议，基于TCP，设计用来进行实时数据通信，广泛用于直播领域，市面上绝大多数直播产品都采用了这个协议；HTTP Live Streaming（HLS）是由Apple公司定义的基于HTTP的流媒体实时传输协议，可实现流媒体的直播和点播，主要用于iOS系统，但它的分段推送的特点，决定了HLS的延迟一般会高于普通的流媒体直播协议；WebRTC（webrealtimecommunication）是一个支持网页浏览器进行实时语音或者视频对话的API，适用于网页端的实时音视频通信。在选择协议时，需要考虑到项目的应用场景（如直播、点播、实时通信等）、目标用户群体（如iOS用户、安卓用户、网页用户等）以及对延迟、带宽等方面</li></ul><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink">https://github.com/chongzicbo/ReadWriteThink</a></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>开发</category>
      
      <category>音视频</category>
      
      <category>基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>音视频开发</tag>
      
      <tag>音视频基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>YOLO11 详解</title>
    <link href="/2024/12/07/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/computer-vision/CV007-YOLO11%E8%AF%A6%E8%A7%A3/"/>
    <url>/2024/12/07/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/computer-vision/CV007-YOLO11%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>2024 年是 YOLO 模型的一年。在 2023 年发布 Ultralytics YOLOv8 之后， YOLOv9 和 YOLOv10也在2024年发布了。但等等，这还不是结束！Ultralytics YOLO11 终于来了，在激动人心的 YOLO Vision 2024 （YV24） 活动中亮相。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/feature.gif"></p><p>YOLO11 系列是 YOLO 系列中最先进的 （SOTA）、最轻、最高效的型号，性能优于其前代产品。它由 Ultralytics 创建，该组织发布了 YOLOv8，这是迄今为止最稳定和使用最广泛的 YOLO 变体。现在，YOLO11 将继续 YOLO 系列的传统。在本文中，我们将探讨：</p><ul><li><strong>什么是 YOLO11？</strong></li><li><strong>YOLO11 能做什么？</strong></li><li><strong>YOLO11 比其他 YOLO 变体更高效吗？</strong></li><li><strong>YOLO11 架构有哪些改进？</strong></li><li><strong>YOLO11 的代码pipeline是如何工作的？</strong></li><li><strong>YOLO11 的基准测试</strong></li><li><strong>YOLO11 快速回顾</strong></li></ul><h1 id="什么是YOLO11"><a href="#什么是YOLO11" class="headerlink" title="什么是YOLO11"></a>什么是YOLO11</h1><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-1.png"></p><p>YOLO11 是 Ultralytics 的 YOLO 系列的最新版本。YOLO11 配备了超轻量级型号，比以前的 YOLO 更快、更高效。YOLO11 能够执行更广泛的计算机视觉任务。Ultralytics 根据大小发布了 5 个 YOLO11 模型，并在<strong>所有任务中发布了 25 个模型</strong>：</p><ul><li><strong>YOLO11n</strong> – Nano 适用于小型和轻量级任务。</li><li><strong>YOLO11s</strong> – Nano 的小升级，具有一些额外的准确性。</li><li><strong>YOLO11m</strong> – 通用型。</li><li><strong>YOLO11l</strong> – 大，精度更高，计算量更高。</li><li><strong>YOLO11x</strong> – 超大尺寸，可实现最大精度和性能。</li></ul><p><img src="https://learnopencv.com/wp-content/uploads/2024/10/yolo11-model-table.png"></p><p>YOLO11 构建在 Ultralytics YOLOv8 代码库之上，并进行了一些架构修改。它还集成了以前 YOLO（如 YOLOv9 和 YOLOv10）的新功能（改进这些功能）以提高性能。我们将在博客文章的后面部分探讨架构和代码库中的新变化。</p><h1 id="YOLO11的应用"><a href="#YOLO11的应用" class="headerlink" title="YOLO11的应用"></a>YOLO11的应用</h1><p>YOLO 以其对象检测模型而闻名。但是，YOLO11 可以执行多个计算机视觉任务，例如 YOLOv8。它包括：</p><ul><li><strong>对象检测</strong></li><li><strong>实例分段</strong></li><li><strong>图像分类</strong></li><li><strong>姿势估计</strong></li><li><strong>定向目标检测 （OBB）</strong></li></ul><p>让我们来探索所有这些。</p><h3 id="对象检测"><a href="#对象检测" class="headerlink" title="对象检测"></a>对象检测</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-object-detection.gif" alt="yolo11-对象检测"></p><p>YOLO11 通过将输入图像传递到 CNN 以提取特征来执行对象检测。然后，网络预测这些网格中对象的边界框和类概率。为了处理多尺度检测，使用图层来确保检测到各种大小的物体。然后使用非极大值抑制 （NMS） 来优化这些预测，以过滤掉重复或低置信度的框，从而获得更准确的对象检测。YOLO11 在 MS-COCO 数据集上进行对象检测训练，其中包括 80 个预训练类。</p><h3 id="实例分割"><a href="#实例分割" class="headerlink" title="实例分割"></a>实例分割</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-instance-segmentation-1-1733102326734-1.png" alt=" "></p><p>除了检测对象之外，YOLO11 还通过添加掩码预测分支扩展到实例分割。这些模型在 MS-COCO 数据集上进行训练，其中包括 80 个预训练类。此分支为每个检测到的对象生成像素级分割掩码，使模型能够区分重叠的对象并提供其形状的精确轮廓。head 中的蒙版分支处理特征映射并输出对象蒙版，从而在识别和区分图像中的对象时实现像素级精度。</p><h3 id="姿势估计"><a href="#姿势估计" class="headerlink" title="姿势估计"></a>姿势估计</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-pose-estimation.gif" alt="YOLO11 姿势"></p><p>YOLO11 通过检测和预测物体上的关键点（例如人体的关节）来执行姿态估计。关键点连接起来形成骨架结构，该结构表示姿势。这些模型在 COCO 上进行训练，其中包括一个预先训练的类“person”。</p><p>在头部添加姿态估计层，并训练网络预测关键点的坐标。后处理步骤将点连接起来以形成骨架结构，从而实现实时姿势识别。</p><h3 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-image-classification.gif" alt="YOLO11 图像分类"></p><p>对于图像分类，YOLO11 使用其深度神经网络从输入图像中提取高级特征，并将其分配给多个预定义类别之一。这些模型在 ImageNet 上进行训练，其中包括 1000 个预训练类。该网络通过多层卷积和池化处理图像，在增强基本特征的同时减少空间维度。网络顶部的分类头输出预测的类，使其适用于需要识别图像整体类别的任务。</p><h3 id="定向目标检测-（OBB）"><a href="#定向目标检测-（OBB）" class="headerlink" title="定向目标检测 （OBB）"></a>定向目标检测 （OBB）</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-obb-detection-1.gif" alt="YOLO11-OBB"></p><p>YOLO11 通过整合 OBB 扩展了常规对象检测，使模型能够检测和分类旋转或不规则方向的物体。这对于航空影像分析等应用程序特别有用。这些模型在 DOTAv1 上进行训练，其中包括 15 个预训练类。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/yolo11-obb-logic-1-1024x615.png" alt="YOLO11-OBB"></p><p>OBB 模型不仅输出边界框坐标，还输出旋转角度 （θ） 或四个角点。这些坐标用于创建与对象方向对齐的边界框，从而提高旋转对象的检测准确性。</p><h1 id="YOLO11-架构和-YOLO11-中的新增功能"><a href="#YOLO11-架构和-YOLO11-中的新增功能" class="headerlink" title="YOLO11 架构和 YOLO11 中的新增功能"></a>YOLO11 架构和 YOLO11 中的新增功能</h1><p>YOLO11 架构是对 YOLOv8 架构的升级，具有一些新的集成和参数调整。在我们继续主要部分之前，您可以查看我们关于 <a href="https://learnopencv.com/ultralytics-yolov8/"><strong>YOLOv8</strong></a> 的详细文章以大致了解架构。现在，如果你看一下 YOLO11 的配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Parameters</span><br><span class="hljs-attr">nc:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># number of classes</span><br><span class="hljs-attr">scales:</span> <span class="hljs-comment"># model compound scaling constants, i.e. &#x27;model=yolo11n.yaml&#x27; will call yolo11.yaml with scale &#x27;n&#x27;</span><br>  <span class="hljs-comment"># [depth, width, max_channels]</span><br>  <span class="hljs-attr">n:</span> [<span class="hljs-number">0.50</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">1024</span>] <span class="hljs-comment"># summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs</span><br>  <span class="hljs-attr">s:</span> [<span class="hljs-number">0.50</span>, <span class="hljs-number">0.50</span>, <span class="hljs-number">1024</span>] <span class="hljs-comment"># summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs</span><br>  <span class="hljs-attr">m:</span> [<span class="hljs-number">0.50</span>, <span class="hljs-number">1.00</span>, <span class="hljs-number">512</span>] <span class="hljs-comment"># summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs</span><br>  <span class="hljs-attr">l:</span> [<span class="hljs-number">1.00</span>, <span class="hljs-number">1.00</span>, <span class="hljs-number">512</span>] <span class="hljs-comment"># summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs</span><br>  <span class="hljs-attr">x:</span> [<span class="hljs-number">1.00</span>, <span class="hljs-number">1.50</span>, <span class="hljs-number">512</span>] <span class="hljs-comment"># summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs</span><br> <br><span class="hljs-comment"># YOLO11n backbone</span><br><span class="hljs-attr">backbone:</span><br>  <span class="hljs-comment"># [from, repeats, module, args]</span><br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">Conv</span>, [<span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>]] <span class="hljs-comment"># 0-P1/2</span><br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">Conv</span>, [<span class="hljs-number">128</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>]] <span class="hljs-comment"># 1-P2/4</span><br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>, <span class="hljs-string">C3k2</span>, [<span class="hljs-number">256</span>, <span class="hljs-literal">False</span>, <span class="hljs-number">0.25</span>]]<br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">Conv</span>, [<span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>]] <span class="hljs-comment"># 3-P3/8</span><br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>, <span class="hljs-string">C3k2</span>, [<span class="hljs-number">512</span>, <span class="hljs-literal">False</span>, <span class="hljs-number">0.25</span>]]<br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">Conv</span>, [<span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>]] <span class="hljs-comment"># 5-P4/16</span><br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>, <span class="hljs-string">C3k2</span>, [<span class="hljs-number">512</span>, <span class="hljs-literal">True</span>]]<br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">Conv</span>, [<span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>]] <span class="hljs-comment"># 7-P5/32</span><br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>, <span class="hljs-string">C3k2</span>, [<span class="hljs-number">1024</span>, <span class="hljs-literal">True</span>]]<br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">SPPF</span>, [<span class="hljs-number">1024</span>, <span class="hljs-number">5</span>]] <span class="hljs-comment"># 9</span><br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>, <span class="hljs-string">C2PSA</span>, [<span class="hljs-number">1024</span>]] <span class="hljs-comment"># 10</span><br> <br><span class="hljs-comment"># YOLO11n head</span><br><span class="hljs-attr">head:</span><br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">nn.Upsample</span>, [<span class="hljs-string">None</span>, <span class="hljs-number">2</span>, <span class="hljs-string">&quot;nearest&quot;</span>]]<br>  <span class="hljs-bullet">-</span> [[<span class="hljs-number">-1</span>, <span class="hljs-number">6</span>], <span class="hljs-number">1</span>, <span class="hljs-string">Concat</span>, [<span class="hljs-number">1</span>]] <span class="hljs-comment"># cat backbone P4</span><br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>, <span class="hljs-string">C3k2</span>, [<span class="hljs-number">512</span>, <span class="hljs-literal">False</span>]] <span class="hljs-comment"># 13</span><br> <br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">nn.Upsample</span>, [<span class="hljs-string">None</span>, <span class="hljs-number">2</span>, <span class="hljs-string">&quot;nearest&quot;</span>]]<br>  <span class="hljs-bullet">-</span> [[<span class="hljs-number">-1</span>, <span class="hljs-number">4</span>], <span class="hljs-number">1</span>, <span class="hljs-string">Concat</span>, [<span class="hljs-number">1</span>]] <span class="hljs-comment"># cat backbone P3</span><br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>, <span class="hljs-string">C3k2</span>, [<span class="hljs-number">256</span>, <span class="hljs-literal">False</span>]] <span class="hljs-comment"># 16 (P3/8-small)</span><br> <br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">Conv</span>, [<span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>]]<br>  <span class="hljs-bullet">-</span> [[<span class="hljs-number">-1</span>, <span class="hljs-number">13</span>], <span class="hljs-number">1</span>, <span class="hljs-string">Concat</span>, [<span class="hljs-number">1</span>]] <span class="hljs-comment"># cat head P4</span><br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>, <span class="hljs-string">C3k2</span>, [<span class="hljs-number">512</span>, <span class="hljs-literal">False</span>]] <span class="hljs-comment"># 19 (P4/16-medium)</span><br> <br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">Conv</span>, [<span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>]]<br>  <span class="hljs-bullet">-</span> [[<span class="hljs-number">-1</span>, <span class="hljs-number">10</span>], <span class="hljs-number">1</span>, <span class="hljs-string">Concat</span>, [<span class="hljs-number">1</span>]] <span class="hljs-comment"># cat head P5</span><br>  <span class="hljs-bullet">-</span> [<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>, <span class="hljs-string">C3k2</span>, [<span class="hljs-number">1024</span>, <span class="hljs-literal">True</span>]] <span class="hljs-comment"># 22 (P5/32-large)</span><br> <br>  <span class="hljs-bullet">-</span> [[<span class="hljs-number">16</span>, <span class="hljs-number">19</span>, <span class="hljs-number">22</span>], <span class="hljs-number">1</span>, <span class="hljs-string">Detect</span>, [<span class="hljs-string">nc</span>]] <span class="hljs-comment"># Detect(P3, P4, P5)</span><br></code></pre></td></tr></table></figure><p>架构级别的变化：</p><h3 id="1-骨干"><a href="#1-骨干" class="headerlink" title="1. 骨干"></a><strong>1. 骨干</strong></h3><p>主干是模型的一部分，用于从多个比例的输入图像中提取特征。它通常涉及堆叠卷积层和块以创建不同分辨率的特征图。</p><p><strong>卷积层：</strong>YOLO11 具有类似的结构，带有初始卷积层来对图像进行下采样：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gauss">- [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-built_in">Conv</span>, [<span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>]] <span class="hljs-meta"># 0-P1/2</span><br>- [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-built_in">Conv</span>, [<span class="hljs-number">128</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>]] <span class="hljs-meta"># 1-P2/4</span><br></code></pre></td></tr></table></figure><ul><li><p><strong>C3k2 区块：</strong>YOLO11 引入了 <strong>C3k2 块，而不是 C2f</strong>，它在计算方面效率更高。此块是 <strong>CSP 瓶颈</strong>的自定义实现，它使用两个卷积，而不是一个大型卷积（如 YOLOv8 中所示）。</p><ul><li><strong>CSP （Cross Stage Partial）：</strong>CSP 网络拆分特征图并通过瓶颈层处理一部分，同时将另一部分与瓶颈的输出合并。这减少了计算负载并改善了特征表示。</li></ul><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs inform7">- <span class="hljs-comment">[-1, 2, C3k2, <span class="hljs-comment">[256, False, 0.25]</span>]</span><br></code></pre></td></tr></table></figure><ul><li>C3k2 块还使用较小的内核大小（由 k2 表示），使其更快，同时保持性能。</li></ul><p><strong>SPPF 和 C2PSA：</strong>YOLO11 保留了 SPPF 块，但在 SPPF 之后添加了一个新的 <strong>C2PSA</strong> 块：</p></li></ul><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs inform7">- <span class="hljs-comment">[-1, 1, SPPF, <span class="hljs-comment">[1024, 5]</span>]</span><br>- <span class="hljs-comment">[-1, 2, C2PSA, <span class="hljs-comment">[1024]</span></span><br></code></pre></td></tr></table></figure><ul><li><strong>C2PSA （Cross Stage Partial with Spatial Attention）</strong> 模块增强了特征图中的空间注意力，从而提高了模型对图像重要部分的关注。这使模型能够通过在空间上池化特征来更有效地关注特定的感兴趣区域。</li></ul><h3 id="2-neck"><a href="#2-neck" class="headerlink" title="2. neck"></a><strong>2. neck</strong></h3><p>neck 负责聚合来自不同分辨率的特征，并将它们传递给头部进行预测。它通常涉及来自不同级别的特征图的上采样和连接。</p><p><strong>C3k2 区块：</strong>YOLO11 用 <strong>C3k2</strong> 块替换了颈部的 C2f 块。如前所述，C3k2 是一个更快、更高效的区块。例如，在上采样和串联后，YOLO11 中的 neck 如下所示：</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs autoit"><br>- [<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>, C3k2, [<span class="hljs-number">512</span>, <span class="hljs-literal">False</span>]] <span class="hljs-meta"># P4/16-medium</span><br></code></pre></td></tr></table></figure><ul><li>此更改提高了要素聚合过程的速度和性能。</li><li><strong>注意力机制：</strong>YOLO11 通过 <strong>C2PSA</strong> 更侧重于空间注意力，这有助于模型专注于图像中的关键区域，以便更好地检测。这在 YOLOv8 中是缺失的，这使得 YOLO11 在检测较小或被遮挡的对象时可能更准确。</li></ul><hr><h3 id="3-head"><a href="#3-head" class="headerlink" title="3. head"></a><strong>3. head</strong></h3><p>head 是模型中负责生成最终预测的部分。在对象检测中，这通常意味着生成边界框并对这些框内的对象进行分类。</p><p><strong>C3k2 区块：</strong>与颈部类似，YOLO11 取代了头部的 <strong>C2f</strong> 块。</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs autoit"><br>- [<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>, C3k2, [<span class="hljs-number">512</span>, <span class="hljs-literal">False</span>]] <span class="hljs-meta"># P4/16-medium</span><br><br></code></pre></td></tr></table></figure><p><strong>检测层：</strong>最终的 Detect 层与 YOLOv8 中的层相同：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs lua">- <span class="hljs-string">[[16, 19, 22], 1, Detect, [nc]]</span> # Detect(P3, P4, P5)<br><br></code></pre></td></tr></table></figure><p>使用 C3k2 块使模型在推理方面更快，在参数方面更高效。<br>那么，让我们看看新块（层）在代码中的样子：</p><hr><p>那么，让我们看看新块（层）在代码中的样子：</p><ol><li><strong>C3k2 区块（从</strong> <strong>blocks.py</strong> 开始<strong>）：</strong><ul><li><strong>C3k2</strong> 是 <strong>CSP 瓶颈</strong>的更快、更高效的变体。它使用两个卷积而不是一个大型卷积，从而加快了特征提取速度。</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">C3k2</span>(<span class="hljs-title class_ inherited__">C2f</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, c1, c2, n=<span class="hljs-number">1</span>, c3k=<span class="hljs-literal">False</span>, e=<span class="hljs-number">0.5</span>, g=<span class="hljs-number">1</span>, shortcut=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__(c1, c2, n, shortcut, g, e)<br>        <span class="hljs-variable language_">self</span>.m = nn.ModuleList(<br>            C3k(<span class="hljs-variable language_">self</span>.c, <span class="hljs-variable language_">self</span>.c, <span class="hljs-number">2</span>, shortcut, g) <span class="hljs-keyword">if</span> c3k <span class="hljs-keyword">else</span> Bottleneck(<span class="hljs-variable language_">self</span>.c, <span class="hljs-variable language_">self</span>.c, shortcut, g) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)<br>        )<br></code></pre></td></tr></table></figure><ol start="2"><li><strong>C3k 块（从</strong> <strong>blocks.py</strong> 开始<strong>）</strong>：</li></ol><ul><li><strong>C3k</strong> 是一个更灵活的瓶颈模块，允许自定义内核大小。这对于提取图像中更详细的特征非常有用。</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros">class C3k(C3):<br>    def __init__(self, c1, c2, <span class="hljs-attribute">n</span>=1, <span class="hljs-attribute">shortcut</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">g</span>=1, <span class="hljs-attribute">e</span>=0.5, <span class="hljs-attribute">k</span>=3):<br>        super().__init__(c1, c2, n, shortcut, g, e)<br>        c_ = int(c2 * e)  # hidden channels<br>        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=(k, k), <span class="hljs-attribute">e</span>=1.0) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(n)))<br></code></pre></td></tr></table></figure><ol start="3"><li><strong>C2PSA 块（从</strong> <strong>blocks.py</strong> 年起<strong>）：</strong></li></ol><ul><li><strong>C2PSA</strong> （Cross Stage Partial with Spatial Attention） 增强了模型的空间注意力能力。此模块增加了对特征图的关注，帮助模型专注于图像的重要区域。</li></ul><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">C2</span>PSA(nn.<span class="hljs-title class_">Module</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, c1, c2, e=<span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-variable language_">super</span>().__init__()<br>        c_ = int(c2 * e)<br>        <span class="hljs-variable language_">self</span>.cv1 = <span class="hljs-title class_">Conv</span>(c1, c_, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.cv2 = <span class="hljs-title class_">Conv</span>(c1, c_, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.cv3 = <span class="hljs-title class_">Conv</span>(<span class="hljs-number">2</span> * c_, c2, <span class="hljs-number">1</span>)<br>     <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.cv3(torch.cat((<span class="hljs-variable language_">self</span>.cv1(x), <span class="hljs-variable language_">self</span>.cv2(x)), <span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><h1 id="YOLO11-pipeline"><a href="#YOLO11-pipeline" class="headerlink" title="YOLO11 pipeline"></a>YOLO11 pipeline</h1><p>在 <a href="https://github.com/ultralytics/ultralytics"><strong>ultralytics</strong></a> GitHub 仓库中，我们将主要关注：</p><ol><li><strong>nn&#x2F;modules&#x2F;</strong> 中的模块<ul><li><strong>block.py</strong></li><li><strong>conv.py</strong></li><li><strong>head.py</strong></li><li><strong>transformer.py</strong></li><li><strong>utils.py</strong></li></ul></li><li><strong>nn&#x2F;tasks.py</strong> 文件</li></ol><h3 id="1-代码库概述"><a href="#1-代码库概述" class="headerlink" title="1. 代码库概述"></a>1. 代码库概述</h3><p>代码库被构建为多个模块，这些模块定义了 YOLO11 模型中使用的各种神经网络组件。这些组件在 nn&#x2F;modules&#x2F; 目录中被组织到不同的文件中：</p><ul><li><strong>block.py</strong>：定义模型中使用的各种构建块（模块），例如瓶颈、CSP 模块和注意力机制。</li><li><strong>conv.py</strong>：包含卷积模块，包括标准卷积、深度卷积和其他变体。</li><li><strong>head.py</strong>：实现负责生成最终预测（例如，边界框、类概率）的模型头。</li><li><strong>transformer.py</strong>：包括基于 transformer 的模块，用于注意力机制和高级特征提取。</li><li><strong>utils.py</strong>：提供跨模块使用的实用程序函数和帮助程序类。</li></ul><p>nn&#x2F;tasks.py 文件定义了不同的特定于任务的模型（例如，检测、分割、分类），这些模型将这些模块组合成完整的架构。</p><h3 id="2-nn-modules-中的模块"><a href="#2-nn-modules-中的模块" class="headerlink" title="2. nn&#x2F;modules&#x2F; 中的模块"></a>2. nn&#x2F;modules&#x2F; 中的模块</h3><p>如前所述，YOLO11 构建在 YOLOv8 代码库之上。因此，我们将主要关注更新的脚本：<strong>block.py</strong>、<strong>conv.py</strong> 和 <strong>head.py</strong> 在这里。</p><h4 id="block-py"><a href="#block-py" class="headerlink" title="block.py"></a><strong>block.py</strong></h4><p>此文件定义 YOLO11 模型中使用的各种构建块。这些块是构成神经网络层的基本组件。</p><h5 id="关键组件："><a href="#关键组件：" class="headerlink" title="关键组件："></a><strong>关键组件：</strong></h5><ol><li>瓶颈模块：<ul><li><strong>Bottleneck</strong>：具有可选快捷方式连接的标准瓶颈模块。</li><li><strong>Res</strong>：使用一系列卷积和身份快捷方式的残差块。</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Bottleneck</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, c1, c2, shortcut=<span class="hljs-literal">True</span>, g=<span class="hljs-number">1</span>, e=<span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        c_ = <span class="hljs-built_in">int</span>(c2 * e)<br>        <span class="hljs-variable language_">self</span>.cv1 = Conv(c1, c_, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.cv2 = Conv(c_, c2, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, g=g)<br>        <span class="hljs-variable language_">self</span>.add = shortcut <span class="hljs-keyword">and</span> c1 == c2<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> x + <span class="hljs-variable language_">self</span>.cv2(<span class="hljs-variable language_">self</span>.cv1(x)) <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.add <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.cv2(<span class="hljs-variable language_">self</span>.cv1(x))<br><br></code></pre></td></tr></table></figure><ul><li>Bottleneck 类实现了一个 bottleneck 模块，该模块减少了通道的数量（降维），然后再次扩展它们。</li><li><strong>组件</strong>：<ul><li>self.cv1：一个 1×1 卷积，用于减少通道数。</li><li>self.cv2：一个 3×3 卷积，用于将通道数增加回原始通道数。</li><li>self.add：一个布尔值，指示是否添加快捷方式连接。</li></ul></li><li><strong>Forward Pass</strong>：输入 x 通过 cv1 和 cv2 传递。如果 self.add 为 True，则原始输入 x 将添加到输出（残差连接）。</li></ul><ol start="2"><li>CSP （Cross Stage Partial） 模块：</li></ol><ul><li><strong>BottleneckCSP：</strong>瓶颈模块的 CSP 版本。</li><li><strong>CSPBlock</strong>：具有多个瓶颈层的更复杂的 CSP 模块。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BottleneckCSP</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, c1, c2, n=<span class="hljs-number">1</span>, shortcut=<span class="hljs-literal">True</span>, g=<span class="hljs-number">1</span>, e=<span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        c_ = <span class="hljs-built_in">int</span>(c2 * e)<br>        <span class="hljs-variable language_">self</span>.cv1 = Conv(c1, c_, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.cv2 = nn.Sequential(<br>            *[Bottleneck(c_, c_, shortcut, g, e=<span class="hljs-number">1.0</span>) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]<br>        )<br>        <span class="hljs-variable language_">self</span>.cv3 = Conv(<span class="hljs-number">2</span> * c_, c2, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.add = c1 == c2<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        y1 = <span class="hljs-variable language_">self</span>.cv2(<span class="hljs-variable language_">self</span>.cv1(x))<br>        y2 = x <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.add <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.cv3(torch.cat((y1, y2), <span class="hljs-number">1</span>)) <span class="hljs-keyword">if</span> y2 <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.cv3(y1)<br><br></code></pre></td></tr></table></figure><ul><li>CSPBottleneck 模块将特征图分为两部分。一部分通过一系列瓶颈层，另一部分直接连接到输出，从而降低了计算成本并增强了梯度流。</li><li><strong>组件</strong>：<ul><li>self.cv1：减少通道数。</li><li>self.cv2：瓶颈层序列。</li><li>self.cv3：合并功能并调整通道数。</li><li>self.add：确定是否添加快捷方式连接。</li></ul></li></ul><ol start="3"><li>其他模块：</li></ol><ul><li><strong>SPPF：</strong>Spatial Pyramid Pooling Fast 模块，可在多个比例下执行池化。</li><li><strong>Concat</strong>：沿指定维度连接多个 Tensor。</li></ul><figure class="highlight gml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs gml">class SPPF(nn.Module):<br>    def __init__(<span class="hljs-symbol">self</span>, c1, c2, k=<span class="hljs-number">5</span>):<br>        super().__init__()<br>        c_ = c1 <span class="hljs-comment">// 2</span><br>        <span class="hljs-symbol">self</span>.cv1 = Conv(c1, c_, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-symbol">self</span>.cv2 = Conv(c_ * <span class="hljs-number">4</span>, c2, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-symbol">self</span>.m = nn.MaxPool2d(kernel_size=k, stride=<span class="hljs-number">1</span>, padding=k <span class="hljs-comment">// 2)</span><br> <br>    def forward(<span class="hljs-symbol">self</span>, <span class="hljs-variable language_">x</span>):<br>        <span class="hljs-variable language_">x</span> = <span class="hljs-symbol">self</span>.cv1(<span class="hljs-variable language_">x</span>)<br>        y1 = <span class="hljs-symbol">self</span>.m(<span class="hljs-variable language_">x</span>)<br>        y2 = <span class="hljs-symbol">self</span>.m(y1)<br>        y3 = <span class="hljs-symbol">self</span>.m(y2)<br>        <span class="hljs-keyword">return</span> <span class="hljs-symbol">self</span>.cv2(torch.cat([<span class="hljs-variable language_">x</span>, y1, y2, y3], <span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><ul><li>SPPF 模块在不同比例下执行最大池化，并将结果连接起来以捕获多个空间比例的要素。</li><li><strong>组件</strong>：<ul><li>self.cv1：减少通道数。</li><li>self.cv2：调整拼接后的 Channel 数。</li><li>self.m：最大池化层数。</li></ul></li><li><strong>Forward Pass</strong>：输入 x 通过 cv1，然后通过三个连续的最大池化层（y1、y2、y3）。结果被连接并通过 cv2 传递。</li></ul><h5 id="了解概念："><a href="#了解概念：" class="headerlink" title="了解概念："></a><strong>了解概念：</strong></h5><ul><li><strong>瓶颈层</strong>：用于通过在昂贵的操作之前减少通道数并在之后增加通道数来降低计算复杂性。</li><li><strong>残差连接</strong>：通过缓解梯度消失问题来帮助训练更深的网络。</li><li><strong>CSP 架构</strong>：将特征图分为两部分;一部分发生转换，而另一部分保持不变，从而提高学习能力并减少计算。</li></ul><h4 id="conv-py"><a href="#conv-py" class="headerlink" title="conv.py"></a><strong>conv.py</strong></h4><p>此文件包含各种卷积模块，包括标准卷积和专用卷积。</p><h5 id="关键组件：-1"><a href="#关键组件：-1" class="headerlink" title="关键组件："></a><strong>关键组件：</strong></h5><p><strong>标准卷积模块 （Conv）：</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs routeros">class Conv(nn.Module):<br>    default_act = nn.SiLU()  #<span class="hljs-built_in"> default </span>activation<br> <br>    def __init__(self, c1, c2, <span class="hljs-attribute">k</span>=1, <span class="hljs-attribute">s</span>=1, <span class="hljs-attribute">p</span>=None, <span class="hljs-attribute">g</span>=1, <span class="hljs-attribute">d</span>=1, <span class="hljs-attribute">act</span>=<span class="hljs-literal">True</span>):<br>        super().__init__()<br>        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), <span class="hljs-attribute">groups</span>=g, <span class="hljs-attribute">dilation</span>=d, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>)<br>        self.bn = nn.BatchNorm2d(c2)<br>        self.act = self.default_act <span class="hljs-keyword">if</span> act is <span class="hljs-literal">True</span> <span class="hljs-keyword">else</span> act <span class="hljs-keyword">if</span> isinstance(act, nn.Module) <span class="hljs-keyword">else</span> nn.Identity()<br> <br>    def forward(self, x):<br>        return self.act(self.bn(self.conv(x)))<br></code></pre></td></tr></table></figure><ul><li>实现具有批量规范化和激活的标准卷积层。</li><li><strong>组件</strong>：<ul><li>self.conv：卷积层。</li><li>self.bn：批量规范化。</li><li>self.act：激活函数（默认为 nn.SiLU（））的</li></ul></li><li><strong>Forward Pass</strong>：应用卷积，然后进行批量规范化和激活。</li></ul><p><strong>深度卷积 （DWConv）：</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">class DWConv(Conv):<br>    def __init__(self, c1, c2, <span class="hljs-attribute">k</span>=1, <span class="hljs-attribute">s</span>=1, <span class="hljs-attribute">d</span>=1, <span class="hljs-attribute">act</span>=<span class="hljs-literal">True</span>):<br>        super().__init__(c1, c2, k, s, <span class="hljs-attribute">g</span>=math.gcd(c1, c2), <span class="hljs-attribute">d</span>=d, <span class="hljs-attribute">act</span>=act)<br></code></pre></td></tr></table></figure><ul><li>执行深度卷积，其中每个输入通道单独卷积。</li><li><strong>组件</strong>：<ul><li>继承自 Conv。</li><li>将 groups 参数设置为 c1 和 c2 的最大公约数，从而有效地对每个通道的卷积进行分组。</li></ul></li></ul><ol><li>其他卷积模块：<ul><li><strong>Conv2</strong>：RepConv 的简化版本，用于模型压缩和加速。</li><li><strong>GhostConv</strong>：实现 GhostNet 的 ghost 模块，减少特性图中的冗余。</li><li><strong>RepConv</strong>：可重新参数化的卷积层，可以从训练模式转换为推理模式。</li></ul></li></ol><h5 id="了解概念：-1"><a href="#了解概念：-1" class="headerlink" title="了解概念："></a><strong>了解概念：</strong></h5><ul><li><strong>自动填充 （<strong><strong>autopad</strong></strong>）：</strong>自动计算保持输出尺寸一致所需的填充。</li><li><strong>深度卷积和点卷积</strong>：用于 MobileNet 架构，以减少计算，同时保持准确性。</li><li><strong>重新参数化</strong>：RepConv 等技术通过合并层来实现高效的训练和更快的推理。</li></ul><h4 id="head-py"><a href="#head-py" class="headerlink" title="head.py"></a><strong>head.py</strong></h4><p>此文件实现了负责生成模型最终预测的 head 模块。</p><h5 id="关键组件：-2"><a href="#关键组件：-2" class="headerlink" title="关键组件："></a><strong>关键组件：</strong></h5><p><strong>检测头 （Detect）：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Detect</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, nc=<span class="hljs-number">80</span>, ch=(<span class="hljs-params"></span>)</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.nc = nc  <span class="hljs-comment"># number of classes</span><br>        <span class="hljs-variable language_">self</span>.nl = <span class="hljs-built_in">len</span>(ch)  <span class="hljs-comment"># number of detection layers</span><br>        <span class="hljs-variable language_">self</span>.reg_max = <span class="hljs-number">16</span>  <span class="hljs-comment"># DFL channels</span><br>        <span class="hljs-variable language_">self</span>.no = nc + <span class="hljs-variable language_">self</span>.reg_max * <span class="hljs-number">4</span>  <span class="hljs-comment"># number of outputs per anchor</span><br>        <span class="hljs-variable language_">self</span>.stride = torch.zeros(<span class="hljs-variable language_">self</span>.nl)  <span class="hljs-comment"># strides computed during build</span><br> <br>        <span class="hljs-comment"># Define layers</span><br>        <span class="hljs-variable language_">self</span>.cv2 = nn.ModuleList(<br>            nn.Sequential(Conv(x, c2, <span class="hljs-number">3</span>), Conv(c2, c2, <span class="hljs-number">3</span>), nn.Conv2d(c2, <span class="hljs-number">4</span> * <span class="hljs-variable language_">self</span>.reg_max, <span class="hljs-number">1</span>)) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> ch<br>        )<br>        <span class="hljs-variable language_">self</span>.cv3 = nn.ModuleList(<br>            nn.Sequential(<br>                nn.Sequential(DWConv(x, x, <span class="hljs-number">3</span>), Conv(x, c3, <span class="hljs-number">1</span>)),<br>                nn.Sequential(DWConv(c3, c3, <span class="hljs-number">3</span>), Conv(c3, c3, <span class="hljs-number">1</span>)),<br>                nn.Conv2d(c3, <span class="hljs-variable language_">self</span>.nc, <span class="hljs-number">1</span>),<br>            )<br>            <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> ch<br>        )<br>        <span class="hljs-variable language_">self</span>.dfl = DFL(<span class="hljs-variable language_">self</span>.reg_max) <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.reg_max &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> nn.Identity()<br></code></pre></td></tr></table></figure><ul><li>Detect 类定义输出边界框坐标和类概率的检测头。</li><li><strong>组件</strong>：<ul><li>self.cv2：用于边界框回归的卷积层。</li><li>self.cv3：用于分类的卷积层。</li><li>self.dfl：用于边界框细化的 Distribution Focal Loss 模块。</li></ul></li><li><strong>Forward Pass</strong>：处理输入特征映射并输出边界框和类的预测。</li></ul><p><strong>分割 （<strong><strong>Segment</strong></strong>）：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Segment</span>(<span class="hljs-title class_ inherited__">Detect</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, nc=<span class="hljs-number">80</span>, nm=<span class="hljs-number">32</span>, npr=<span class="hljs-number">256</span>, ch=(<span class="hljs-params"></span>)</span>):<br>        <span class="hljs-built_in">super</span>().__init__(nc, ch)<br>        <span class="hljs-variable language_">self</span>.nm = nm  <span class="hljs-comment"># number of masks</span><br>        <span class="hljs-variable language_">self</span>.npr = npr  <span class="hljs-comment"># number of prototypes</span><br>        <span class="hljs-variable language_">self</span>.proto = Proto(ch[<span class="hljs-number">0</span>], <span class="hljs-variable language_">self</span>.npr, <span class="hljs-variable language_">self</span>.nm)  <span class="hljs-comment"># protos</span><br> <br>        c4 = <span class="hljs-built_in">max</span>(ch[<span class="hljs-number">0</span>] // <span class="hljs-number">4</span>, <span class="hljs-variable language_">self</span>.nm)<br>        <span class="hljs-variable language_">self</span>.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, <span class="hljs-number">3</span>), Conv(c4, c4, <span class="hljs-number">3</span>), nn.Conv2d(c4, <span class="hljs-variable language_">self</span>.nm, <span class="hljs-number">1</span>)) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> ch)<br><br></code></pre></td></tr></table></figure><ul><li>扩展 Detect 类以包含分段功能。</li><li><strong>组件</strong>：<ul><li>self.proto：生成掩码原型。</li><li>self.cv4：掩码系数的卷积层。</li></ul></li><li><strong>Forward Pass</strong>：输出边界框、类概率和掩码系数。</li></ul><p><strong>姿势估计头部 （Pose）：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Pose</span>(<span class="hljs-title class_ inherited__">Detect</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, nc=<span class="hljs-number">80</span>, kpt_shape=(<span class="hljs-params"><span class="hljs-number">17</span>, <span class="hljs-number">3</span></span>), ch=(<span class="hljs-params"></span>)</span>):<br>        <span class="hljs-built_in">super</span>().__init__(nc, ch)<br>        <span class="hljs-variable language_">self</span>.kpt_shape = kpt_shape  <span class="hljs-comment"># number of keypoints, number of dimensions</span><br>        <span class="hljs-variable language_">self</span>.nk = kpt_shape[<span class="hljs-number">0</span>] * kpt_shape[<span class="hljs-number">1</span>]  <span class="hljs-comment"># total number of keypoint outputs</span><br> <br>        c4 = <span class="hljs-built_in">max</span>(ch[<span class="hljs-number">0</span>] // <span class="hljs-number">4</span>, <span class="hljs-variable language_">self</span>.nk)<br>        <span class="hljs-variable language_">self</span>.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, <span class="hljs-number">3</span>), Conv(c4, c4, <span class="hljs-number">3</span>), nn.Conv2d(c4, <span class="hljs-variable language_">self</span>.nk, <span class="hljs-number">1</span>)) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> ch)<br><br></code></pre></td></tr></table></figure><ul><li>扩展了 Detect 类，用于人体姿势估计任务。</li><li><strong>组件</strong>：<ul><li>self.kpt_shape：关键点的形状（关键点的数量、每个关键点的维度）。</li><li>self.cv4：用于关键点回归的卷积层。</li></ul></li><li><strong>Forward Pass</strong>：输出边界框、类概率和关键点坐标。</li></ul><h5 id="了解概念：-2"><a href="#了解概念：-2" class="headerlink" title="了解概念："></a><strong>了解概念：</strong></h5><ul><li><strong>模块化</strong>：通过扩展 Detect 类，我们可以为不同的任务创建专门的 head，同时重用通用功能。</li><li><strong>无锚点检测</strong>：现代对象检测器通常使用无锚点方法，直接预测边界框。</li><li><strong>关键点估计</strong>：在姿势估计中，模型预测表示关节或地标的关键点。</li></ul><h3 id="3-nn-tasks-py-文件"><a href="#3-nn-tasks-py-文件" class="headerlink" title="3. nn&#x2F;tasks.py 文件"></a>3. <strong>nn&#x2F;tasks.py</strong> 文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Ultralytics YOLO &lt;img draggable=&quot;false&quot; role=&quot;img&quot; class=&quot;emoji&quot; alt=&quot;🚀&quot; src=&quot;https://s.w.org/images/core/emoji/15.0.3/svg/1f680.svg&quot;&gt;, AGPL-3.0 license</span><br> <br><span class="hljs-keyword">import</span> contextlib<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> types<br><span class="hljs-keyword">from</span> copy <span class="hljs-keyword">import</span> deepcopy<br><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path<br> <br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br> <br><span class="hljs-comment"># Other imports...</span><br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BaseModel</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;The BaseModel class serves as a base class for all the models in the Ultralytics YOLO family.&quot;&quot;&quot;</span><br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, *args, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Handles both training and inference, returns predictions or loss.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(x, <span class="hljs-built_in">dict</span>):<br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.loss(x, *args, **kwargs)  <span class="hljs-comment"># Training: return loss</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.predict(x, *args, **kwargs)  <span class="hljs-comment"># Inference: return predictions</span><br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, x, profile=<span class="hljs-literal">False</span>, visualize=<span class="hljs-literal">False</span>, augment=<span class="hljs-literal">False</span>, embed=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Run a forward pass through the network for inference.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> augment:<br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>._predict_augment(x)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>._predict_once(x, profile, visualize, embed)<br>     <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fuse</span>(<span class="hljs-params">self, verbose=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Fuses Conv and BatchNorm layers for efficiency during inference.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, (Conv, Conv2, DWConv)) <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(m, <span class="hljs-string">&quot;bn&quot;</span>):<br>                m.conv = fuse_conv_and_bn(m.conv, m.bn)<br>                <span class="hljs-built_in">delattr</span>(m, <span class="hljs-string">&quot;bn&quot;</span>)<br>                m.forward = m.forward_fuse  <span class="hljs-comment"># Use the fused forward</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span><br>     <br>    <span class="hljs-comment"># More BaseModel methods...</span><br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DetectionModel</span>(<span class="hljs-title class_ inherited__">BaseModel</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;YOLOv8 detection model.&quot;&quot;&quot;</span><br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg=<span class="hljs-string">&quot;yolov8n.yaml&quot;</span>, ch=<span class="hljs-number">3</span>, nc=<span class="hljs-literal">None</span>, verbose=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize the YOLOv8 detection model with config and parameters.&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.yaml = cfg <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(cfg, <span class="hljs-built_in">dict</span>) <span class="hljs-keyword">else</span> yaml_model_load(cfg)<br>        <span class="hljs-variable language_">self</span>.model, <span class="hljs-variable language_">self</span>.save = parse_model(deepcopy(<span class="hljs-variable language_">self</span>.yaml), ch=ch, verbose=verbose)<br>        <span class="hljs-variable language_">self</span>.names = &#123;i: <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;i&#125;</span>&quot;</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.yaml[<span class="hljs-string">&quot;nc&quot;</span>])&#125;  <span class="hljs-comment"># Class names</span><br>        <span class="hljs-variable language_">self</span>.inplace = <span class="hljs-variable language_">self</span>.yaml.get(<span class="hljs-string">&quot;inplace&quot;</span>, <span class="hljs-literal">True</span>)<br> <br>        <span class="hljs-comment"># Initialize strides</span><br>        m = <span class="hljs-variable language_">self</span>.model[-<span class="hljs-number">1</span>]  <span class="hljs-comment"># Detect() layer</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, Detect):<br>            s = <span class="hljs-number">256</span>  <span class="hljs-comment"># Max stride</span><br>            m.stride = torch.tensor([s / x.shape[-<span class="hljs-number">2</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>._predict_once(torch.zeros(<span class="hljs-number">1</span>, ch, s, s))])<br>            <span class="hljs-variable language_">self</span>.stride = m.stride<br>            m.bias_init()  <span class="hljs-comment"># Initialize biases</span><br> <br>    <span class="hljs-comment"># More DetectionModel methods...</span><br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SegmentationModel</span>(<span class="hljs-title class_ inherited__">DetectionModel</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;YOLOv8 segmentation model.&quot;&quot;&quot;</span><br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg=<span class="hljs-string">&quot;yolov8n-seg.yaml&quot;</span>, ch=<span class="hljs-number">3</span>, nc=<span class="hljs-literal">None</span>, verbose=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize YOLOv8 segmentation model with given config and parameters.&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_criterion</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize the loss criterion for the SegmentationModel.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> v8SegmentationLoss(<span class="hljs-variable language_">self</span>)<br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PoseModel</span>(<span class="hljs-title class_ inherited__">DetectionModel</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;YOLOv8 pose model.&quot;&quot;&quot;</span><br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg=<span class="hljs-string">&quot;yolov8n-pose.yaml&quot;</span>, ch=<span class="hljs-number">3</span>, nc=<span class="hljs-literal">None</span>, data_kpt_shape=(<span class="hljs-params"><span class="hljs-literal">None</span>, <span class="hljs-literal">None</span></span>), verbose=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize YOLOv8 Pose model.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(cfg, <span class="hljs-built_in">dict</span>):<br>            cfg = yaml_model_load(cfg)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">list</span>(data_kpt_shape) != <span class="hljs-built_in">list</span>(cfg[<span class="hljs-string">&quot;kpt_shape&quot;</span>]):<br>            cfg[<span class="hljs-string">&quot;kpt_shape&quot;</span>] = data_kpt_shape<br>        <span class="hljs-built_in">super</span>().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_criterion</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize the loss criterion for the PoseModel.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> v8PoseLoss(<span class="hljs-variable language_">self</span>)<br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ClassificationModel</span>(<span class="hljs-title class_ inherited__">BaseModel</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;YOLOv8 classification model.&quot;&quot;&quot;</span><br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg=<span class="hljs-string">&quot;yolov8n-cls.yaml&quot;</span>, ch=<span class="hljs-number">3</span>, nc=<span class="hljs-literal">None</span>, verbose=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize the YOLOv8 classification model.&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>._from_yaml(cfg, ch, nc, verbose)<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_from_yaml</span>(<span class="hljs-params">self, cfg, ch, nc, verbose</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Set YOLOv8 model configurations and define the model architecture.&quot;&quot;&quot;</span><br>        <span class="hljs-variable language_">self</span>.yaml = cfg <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(cfg, <span class="hljs-built_in">dict</span>) <span class="hljs-keyword">else</span> yaml_model_load(cfg)<br>        <span class="hljs-variable language_">self</span>.model, <span class="hljs-variable language_">self</span>.save = parse_model(deepcopy(<span class="hljs-variable language_">self</span>.yaml), ch=ch, verbose=verbose)<br>        <span class="hljs-variable language_">self</span>.names = &#123;i: <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;i&#125;</span>&quot;</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.yaml[<span class="hljs-string">&quot;nc&quot;</span>])&#125;<br>        <span class="hljs-variable language_">self</span>.info()<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reshape_outputs</span>(<span class="hljs-params">model, nc</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Update a classification model to match the class count (nc).&quot;&quot;&quot;</span><br>        name, m = <span class="hljs-built_in">list</span>((model.model <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(model, <span class="hljs-string">&quot;model&quot;</span>) <span class="hljs-keyword">else</span> model).named_children())[-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>            <span class="hljs-keyword">if</span> m.out_features != nc:<br>                <span class="hljs-built_in">setattr</span>(model, name, nn.Linear(m.in_features, nc))<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_criterion</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize the loss criterion for the ClassificationModel.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> v8ClassificationLoss()<br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Ensemble</span>(nn.ModuleList):<br>    <span class="hljs-string">&quot;&quot;&quot;Ensemble of models.&quot;&quot;&quot;</span><br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize an ensemble of models.&quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>().__init__()<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, augment=<span class="hljs-literal">False</span>, profile=<span class="hljs-literal">False</span>, visualize=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Generate the ensemble’s final layer by combining outputs from each model.&quot;&quot;&quot;</span><br>        y = [module(x, augment, profile, visualize)[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>]<br>        <span class="hljs-keyword">return</span> torch.cat(y, <span class="hljs-number">2</span>), <span class="hljs-literal">None</span>  <span class="hljs-comment"># Concatenate outputs along the third dimension</span><br> <br><span class="hljs-comment"># Functions ------------------------------------------------------------------------------------------------------------</span><br> <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_model</span>(<span class="hljs-params">d, ch, verbose=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Parse a YOLO model.yaml dictionary into a PyTorch model.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">import</span> ast<br> <br>    max_channels = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>)<br>    nc, act, scales = (d.get(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> (<span class="hljs-string">&quot;nc&quot;</span>, <span class="hljs-string">&quot;activation&quot;</span>, <span class="hljs-string">&quot;scales&quot;</span>))<br>    depth, width, kpt_shape = (d.get(x, <span class="hljs-number">1.0</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> (<span class="hljs-string">&quot;depth_multiple&quot;</span>, <span class="hljs-string">&quot;width_multiple&quot;</span>, <span class="hljs-string">&quot;kpt_shape&quot;</span>))<br> <br>    <span class="hljs-comment"># Model scaling</span><br>    <span class="hljs-keyword">if</span> scales:<br>        scale = d.get(<span class="hljs-string">&quot;scale&quot;</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> scale:networ<br>            scale = <span class="hljs-built_in">tuple</span>(scales.keys())[<span class="hljs-number">0</span>]<br>            LOGGER.warning(<span class="hljs-string">f&quot;WARNING &lt;img draggable=&quot;</span>false<span class="hljs-string">&quot; role=&quot;</span>img<span class="hljs-string">&quot; class=&quot;</span>emoji<span class="hljs-string">&quot; alt=&quot;</span>⚠️<span class="hljs-string">&quot; src=&quot;</span>https://s.w.org/images/core/emoji/<span class="hljs-number">15.0</span><span class="hljs-number">.3</span>/svg/26a0.svg<span class="hljs-string">&quot;&gt; no model scale passed. Assuming scale=&#x27;&#123;scale&#125;&#x27;.&quot;</span>)<br>        depth, width, max_channels = scales[scale]<br> <br>    <span class="hljs-keyword">if</span> act:<br>        Conv.default_act = <span class="hljs-built_in">eval</span>(act)  <span class="hljs-comment"># redefine default activation</span><br>        <span class="hljs-keyword">if</span> verbose:<br>            LOGGER.info(<span class="hljs-string">f&quot;Activation: <span class="hljs-subst">&#123;act&#125;</span>&quot;</span>)<br> <br>    <span class="hljs-comment"># Logging and parsing layers</span><br>    <span class="hljs-keyword">if</span> verbose:<br>        LOGGER.info(<span class="hljs-string">f&quot;\n<span class="hljs-subst">&#123;<span class="hljs-string">&#x27;&#x27;</span>:&gt;<span class="hljs-number">3</span>&#125;</span><span class="hljs-subst">&#123;<span class="hljs-string">&#x27;from&#x27;</span>:&gt;<span class="hljs-number">20</span>&#125;</span><span class="hljs-subst">&#123;<span class="hljs-string">&#x27;n&#x27;</span>:&gt;<span class="hljs-number">3</span>&#125;</span><span class="hljs-subst">&#123;<span class="hljs-string">&#x27;params&#x27;</span>:&gt;<span class="hljs-number">10</span>&#125;</span>  <span class="hljs-subst">&#123;<span class="hljs-string">&#x27;module&#x27;</span>:&lt;<span class="hljs-number">45</span>&#125;</span><span class="hljs-subst">&#123;<span class="hljs-string">&#x27;arguments&#x27;</span>:&lt;<span class="hljs-number">30</span>&#125;</span>&quot;</span>)<br>    ch = [ch]<br>    layers, save, c2 = [], [], ch[-<span class="hljs-number">1</span>]<br> <br>    <span class="hljs-keyword">for</span> i, (f, n, m, args) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(d[<span class="hljs-string">&quot;backbone&quot;</span>] + d[<span class="hljs-string">&quot;head&quot;</span>]):  <span class="hljs-comment"># from, number, module, args</span><br>        m = <span class="hljs-built_in">globals</span>()[m] <span class="hljs-keyword">if</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">globals</span>() <span class="hljs-keyword">else</span> <span class="hljs-built_in">getattr</span>(nn, m[<span class="hljs-number">3</span>:], m)  <span class="hljs-comment"># get module</span><br>        <span class="hljs-keyword">for</span> j, a <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(args):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(a, <span class="hljs-built_in">str</span>):<br>                <span class="hljs-keyword">with</span> contextlib.suppress(ValueError):<br>                    args[j] = ast.literal_eval(a) <span class="hljs-keyword">if</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">locals</span>() <span class="hljs-keyword">else</span> a<br> <br>        n = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">round</span>(n * depth), <span class="hljs-number">1</span>) <span class="hljs-keyword">if</span> n &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> n  <span class="hljs-comment"># depth gain</span><br>        <span class="hljs-keyword">if</span> m <span class="hljs-keyword">in</span> &#123;Conv, Bottleneck, C2f, C3k2, ...&#125;:  <span class="hljs-comment"># Module list</span><br>            c1, c2 = ch[f], args[<span class="hljs-number">0</span>]<br>            c2 = make_divisible(<span class="hljs-built_in">min</span>(c2, max_channels) * width, <span class="hljs-number">8</span>)<br>            args = [c1, c2, *args[<span class="hljs-number">1</span>:]]<br>            <span class="hljs-keyword">if</span> m <span class="hljs-keyword">in</span> &#123;C2f, C3k2, ...&#125;:  <span class="hljs-comment"># Repeated layers</span><br>                args.insert(<span class="hljs-number">2</span>, n)<br>                n = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> m <span class="hljs-keyword">in</span> &#123;Concat, Detect, ...&#125;:  <span class="hljs-comment"># Head layers</span><br>            args.append([ch[x] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> f])<br>        <span class="hljs-comment"># Append layers</span><br>        m_ = nn.Sequential(*(m(*args) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n))) <span class="hljs-keyword">if</span> n &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> m(*args)<br>        layers.append(m_)<br> <br>        ch.append(c2)<br>        save.extend([x % i <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> ([f] <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(f, <span class="hljs-built_in">int</span>) <span class="hljs-keyword">else</span> f) <span class="hljs-keyword">if</span> x != -<span class="hljs-number">1</span>])<br> <br>        <span class="hljs-keyword">if</span> verbose:<br>            LOGGER.info(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;i:&gt;<span class="hljs-number">3</span>&#125;</span><span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(f):&gt;<span class="hljs-number">20</span>&#125;</span><span class="hljs-subst">&#123;n:&gt;<span class="hljs-number">3</span>&#125;</span><span class="hljs-subst">&#123;<span class="hljs-built_in">sum</span>(x.numel() <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> m_.parameters()):<span class="hljs-number">10.0</span>f&#125;</span>  <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(m):&lt;<span class="hljs-number">45</span>&#125;</span><span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(args):&lt;<span class="hljs-number">30</span>&#125;</span>&quot;</span>)<br> <br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers), <span class="hljs-built_in">sorted</span>(save)<br> <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">yaml_model_load</span>(<span class="hljs-params">path</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Load a YOLO model from a YAML file.&quot;&quot;&quot;</span><br>    path = Path(path)<br>    unified_path = path.with_name(path.stem.replace(<span class="hljs-string">&quot;yolov8&quot;</span>, <span class="hljs-string">&quot;yolov&quot;</span>))<br>    yaml_file = check_yaml(<span class="hljs-built_in">str</span>(unified_path), hard=<span class="hljs-literal">False</span>) <span class="hljs-keyword">or</span> check_yaml(path)<br>    d = yaml_load(yaml_file)<br>    d[<span class="hljs-string">&quot;scale&quot;</span>] = guess_model_scale(path)<br>    d[<span class="hljs-string">&quot;yaml_file&quot;</span>] = <span class="hljs-built_in">str</span>(path)<br>    <span class="hljs-keyword">return</span> d<br> <br><span class="hljs-comment"># More utility functions...</span><br> <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">guess_model_scale</span>(<span class="hljs-params">model_path</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Extract the scale from the YAML file.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">import</span> re<br>    <span class="hljs-keyword">return</span> re.search(<span class="hljs-string">r&quot;yolov\d+([nslmx])&quot;</span>, Path(model_path).stem).group(<span class="hljs-number">1</span>)<br> <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">attempt_load_weights</span>(<span class="hljs-params">weights, device=<span class="hljs-literal">None</span>, inplace=<span class="hljs-literal">True</span>, fuse=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Loads weights for a model or an ensemble of models.&quot;&quot;&quot;</span><br>    ensemble = Ensemble()<br>    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> weights <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(weights, <span class="hljs-built_in">list</span>) <span class="hljs-keyword">else</span> [weights]:<br>        ckpt, _ = torch_safe_load(w)<br>        model = (ckpt.get(<span class="hljs-string">&quot;ema&quot;</span>) <span class="hljs-keyword">or</span> ckpt[<span class="hljs-string">&quot;model&quot;</span>]).to(device).<span class="hljs-built_in">float</span>()<br>        model = model.fuse().<span class="hljs-built_in">eval</span>() <span class="hljs-keyword">if</span> fuse <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(model, <span class="hljs-string">&quot;fuse&quot;</span>) <span class="hljs-keyword">else</span> model.<span class="hljs-built_in">eval</span>()<br>        ensemble.append(model)<br> <br>    <span class="hljs-keyword">return</span> ensemble <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(ensemble) &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> ensemble[-<span class="hljs-number">1</span>]<br> <br></code></pre></td></tr></table></figure><p>此 tasks.py 脚本是代码管道的核心部分;它仍然使用 YOLOv8 方法和逻辑;我们只需要将 YOLO11 模型解析到其中。此脚本专为各种计算机视觉任务而设计，例如对象检测、分割、分类、姿势估计、OBB 等。它定义了用于训练、推理和模型管理的基础模型、特定于任务的模型和效用函数。</p><h4 id="关键组件：-3"><a href="#关键组件：-3" class="headerlink" title="关键组件："></a><strong>关键组件：</strong></h4><ul><li><strong>Imports：</strong>该脚本从 Ultralytics 导入 PyTorch （torch）、神经网络层 （torch.nn） 和实用函数等基本模块。一些关键导入包括：<ul><li>对 <strong>C3k2</strong>、<strong>C2PSA</strong>、<strong>C3</strong>、<strong>SPPF、****Concat</strong> 等架构模块进行建模。</li><li>损失函数，如 <strong>v8DetectionLoss</strong>、<strong>v8SegmentationLoss</strong>、<strong>v8ClassificationLoss</strong>、<strong>v8OBBLoss</strong>。</li><li>各种实用程序函数，如 model_info、<strong>fuse_conv_and_bn</strong>、<strong>scale_img</strong> <strong>time_sync</strong>，以帮助进行模型处理、分析和评估。</li></ul></li></ul><h4 id="模型基类："><a href="#模型基类：" class="headerlink" title="模型基类："></a><strong>模型基类：</strong></h4><ol><li>BaseModel 类：<ul><li>BaseModel 用作 Ultralytics YOLO 系列中所有模型的基类。</li><li>实现如下基本方法：<ul><li><strong>forward（）：</strong>根据输入数据处理训练和推理。</li><li><strong>predict（）：</strong>处理前向传递以进行推理。</li><li><strong>fuse（）：</strong>融合 Conv2d 和 BatchNorm2d 层以提高效率。</li><li><strong>info（）：</strong>提供详细的模型信息。</li></ul></li><li>此类旨在通过特定于任务的模型（例如检测、分割和分类）进行扩展。</li></ul></li><li><strong>DetectionModel</strong> <strong>类：</strong><ul><li>扩展 BaseModel，专门用于对象检测任务。</li><li>加载模型配置，初始化检测头（如 Detect 模块）并设置模型步幅。</li><li>它支持使用 YOLOv8 等架构的检测任务，并可以通过 <strong>_predict_augment（）</strong> 执行增强推理。</li></ul></li></ol><h4 id="特定于任务的模型："><a href="#特定于任务的模型：" class="headerlink" title="特定于任务的模型："></a><strong>特定于任务的模型：</strong></h4><ol><li><strong>SegmentationModel 的 SegmentationModel</strong> <strong>中：</strong><ul><li>专门用于分割任务（如 YOLOv8 分割）的 DetectionModel 的子类。</li><li>初始化特定于分割的损失函数 （v8SegmentationLoss）。</li></ul></li><li><strong>PoseModel 的 PoseModel</strong> <strong>中：</strong><ul><li>通过初始化具有关键点检测 （<strong>kpt_shape</strong>） 特定配置的模型来处理姿态估计任务。</li><li>使用 v8PoseLoss 进行特定于姿势的损失计算。</li></ul></li><li><strong>分类型号****：</strong><ul><li>专为使用 YOLOv8 分类架构的图像分类任务而设计。</li><li>初始化和管理特定于分类的损失 （<strong>v8ClassificationLoss</strong>）。</li><li>它还支持重塑用于分类任务的预训练 TorchVision 模型。</li></ul></li><li><strong>OBB型号****：</strong><ul><li>用于定向边界框 （OBB） 检测任务。</li><li>实现特定的损失函数 （<strong>v8OBBLoss</strong>） 来处理旋转的边界框。</li></ul></li><li><strong>世界模型****：</strong><ul><li>此模型处理图像字幕和基于文本的识别等任务。</li><li>利用 CLIP 模型中的文本特征执行基于文本的视觉识别任务。</li><li>包括对文本嵌入 （<strong>txt_feats</strong>） 的特殊处理，用于字幕和世界相关任务。</li></ul></li></ol><h4 id="集成模型："><a href="#集成模型：" class="headerlink" title="集成模型："></a><strong>集成模型：</strong></h4><ol><li><strong>集成****：</strong><ul><li>一个简单的 ensemble 类，它将多个模型合并为一个模型。</li><li>允许对不同模型的输出进行平均或串联，以提高整体性能。</li><li>对于组合多个模型的输出提供更好的预测的任务非常有用。</li></ul></li></ol><h4 id="实用功能："><a href="#实用功能：" class="headerlink" title="实用功能："></a><strong>实用功能：</strong></h4><ol><li>模型加载和管理：<ul><li><strong>attempt_load_weights（）、****attempt_load_one_weight（）</strong>：用于加载模型、管理集成模型以及处理加载预训练权重时的兼容性问题的函数。</li><li>这些功能可确保以适当的步幅、层和配置正确加载模型。</li></ul></li><li>临时模块重定向：<ul><li><strong>temporary_modules（）</strong>：一个上下文管理器，用于临时重定向模块路径，确保在模块位置更改时向后兼容。</li><li>有助于保持与旧型号版本的兼容性。</li></ul></li><li><strong>Pickle</strong>安全处理：<ul><li>SafeUnpickler：一个自定义的解封器，可以安全地加载模型检查点，确保未知类被安全的占位符（SafeClass）替换，以避免在加载过程中发生崩溃。</li></ul></li></ol><h4 id="模型解析："><a href="#模型解析：" class="headerlink" title="模型解析："></a><strong>模型解析：</strong></h4><ol><li><strong>parse_model（）</strong> <strong>中：</strong><ul><li>此函数将 YAML 文件中的模型配置解析为 PyTorch 模型。</li><li>它处理主干和头架构，解释每个层类型（如 Conv、SPPF、Detect），并构建最终模型。</li><li>支持各种架构，包括 C3k2、C2PSA 等 YOLO11 组件。</li></ul></li><li>YAML 模型加载：<ul><li><strong>yaml_model_load（）</strong>）：从 YAML 文件加载模型配置，检测模型比例（例如 n、s、m、l、x）并相应地调整参数。</li><li><strong>guess_model_scale（）、****guess_model_task（）</strong>：用于根据 YAML 文件结构推断模型规模和任务的辅助函数。</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>computer-vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>yolo</tag>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>YOLO V10 详解</title>
    <link href="/2024/12/07/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/computer-vision/CV010-YOLO%20V10%E8%AF%A6%E8%A7%A3/"/>
    <url>/2024/12/07/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/computer-vision/CV010-YOLO%20V10%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>实时物体检测一直是计算机视觉领域的研究热点，旨在低延迟下准确预测图像中物体的类别和位置。该技术广泛应用于自动驾驶、机器人导航和物体跟踪等实际应用中。近年来，基于卷积神经网络（CNN）的物体检测器因其高效的性能而受到广泛关注，其中YOLO系列因其出色的性能和效率平衡而脱颖而出。</p><h3 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h3><p>本文旨在解决YOLO系列在实际部署中依赖非极大值抑制（NMS）导致的推理延迟问题，并通过优化模型架构进一步提升其性能和效率。具体来说，本文提出了以下两个主要目标：</p><ol><li>提出一种无需NMS训练的一致双分配策略，以实现高效的端到端检测。</li><li>提出一种全面的效率-准确性驱动的模型设计策略，从后处理和模型架构两方面提升YOLO的性能和效率。</li></ol><h3 id="研究难点"><a href="#研究难点" class="headerlink" title="研究难点"></a>研究难点</h3><p>YOLO系列在实际应用中面临的主要挑战包括：</p><ol><li><strong>NMS依赖性</strong>：传统的YOLO训练过程中采用一对多标签分配策略，导致推理过程中需要依赖NMS进行后处理，这不仅增加了推理延迟，还使得模型对NMS超参数敏感，难以实现最优的端到端部署。</li><li><strong>模型架构设计</strong>：尽管已有大量研究探索了不同的模型架构设计策略，但YOLO系列在各个组件的设计上仍存在计算冗余，限制了模型的性能和效率。</li></ol><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>本文回顾了现有的实时物体检测器和端到端物体检测器的相关研究：</p><ol><li><strong>传统YOLO系列</strong>：YOLOv1、YOLOv2和YOLOv3是典型的三部分检测架构，包括主干、颈部和头部。后续的YOLOv4和YOLOv5引入了CSPNet设计，并结合数据增强策略和多种模型尺度。YOLOv6、YOLOv7和YOLOv8分别提出了BiC、E-ELAN和C2f等新的组件设计。</li><li><strong>端到端物体检测器</strong>：DETR系列通过引入Transformer架构和匈牙利损失实现了一对一匹配预测，消除了手工设计的组件和后处理。其他研究如Learnable NMS和关系网络也尝试通过不同的方法实现端到端检测。</li></ol><h3 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h3><p>本文提出了一种无需NMS训练的一致双分配策略和全面的效率-准确性驱动的模型设计策略：</p><ol><li><strong>一致双分配策略</strong>：通过引入双标签分配和一致的匹配度量，结合一对多和一对一标签分配的优势，实现高效的端到端检测。</li><li><strong>效率-准确性驱动的模型设计策略</strong>：从模型架构的各个组件入手，提出了轻量级分类头、空间-通道解耦下采样和排名引导的块设计等优化方法，减少计算冗余并提升模型性能。</li></ol><h3 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h3><p>本文在COCO数据集上对提出的YOLOv10模型进行了广泛的实验验证，具体包括：</p><ol><li><strong>数据集</strong>：使用COCO数据集进行训练和评估，采用标准的训练-验证-测试划分。</li><li><strong>实验设置</strong>：所有模型在8块NVIDIA 3090 GPU上进行训练，采用SGD优化器，并结合Mosaic、Mixup和复制粘贴等数据增强策略。</li><li><strong>评估指标</strong>：使用标准平均精度（AP）和不同IoU阈值下的AP值评估模型性能，并测量推理延迟以评估效率。</li></ol><h3 id="结果与分析"><a href="#结果与分析" class="headerlink" title="结果与分析"></a>结果与分析</h3><p>实验结果表明，YOLOv10在多个模型尺度上均取得了显著的性能和效率提升：</p><ol><li><strong>性能提升</strong>：YOLOv10-S在相似的AP下比RT-DETR-R18快1.8倍，YOLOv10-B在相同性能下比YOLOv9-C减少了46%的延迟。</li><li><strong>参数和计算量减少</strong>：YOLOv10-S和YOLOv10-B分别比RT-DETR-R18和YOLOv9-C减少了2.8倍和25%的参数数量和FLOPs。</li><li><strong>全面优势</strong>：YOLOv10在多个模型尺度上均优于现有先进模型，展示了其在计算-准确性权衡上的优越性。</li></ol><h3 id="总体结论"><a href="#总体结论" class="headerlink" title="总体结论"></a>总体结论</h3><p>本文通过提出一致双分配策略和全面的效率-准确性驱动的模型设计策略，成功提升了YOLO系列的性能和效率。实验结果验证了YOLOv10在多个模型尺度上的优越性，展示了其在实时物体检测领域的潜力。未来的工作将进一步探索减少小模型中一对多训练和无需NMS训练之间的性能差距的方法。</p><h1 id="研究方法-1"><a href="#研究方法-1" class="headerlink" title="研究方法"></a>研究方法</h1><h3 id="无NMS训练的一致双重分配"><a href="#无NMS训练的一致双重分配" class="headerlink" title="无NMS训练的一致双重分配"></a>无NMS训练的一致双重分配</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/5f60bbd9bccdfacc4ce2a53bce9011dc-image.png" alt="img"></p><ul><li><p>双重标签分配。</p><p>为了实现无需NMS的训练，论文提出了一致的双重分配策略。该策略结合一对多和多对一的标签分配策略的优点。具体来说，双重分配策略包括一个多对一的头和一个一对一的头。在训练过程中，两个头共同优化，使得主干和颈部能够享受多对一分配提供的丰富监督信号。在推理过程中，丢弃多对一头，只使用一对一头来做出预测，从而实现无需NMS的高效端到端部署。</p></li><li><p>一致的匹配度量。</p></li></ul><p>为了确保两个头之间的和谐监督，论文提出了一致的匹配度量。该度量公式如下：<br>$$<br>m(\alpha,\beta)&#x3D;s\cdot p^{\alpha}\cdot IoU(\hat{b},b)^{\beta}<br>$$<br>其中，<em>p</em> 是分类分数，$\hat{b}$ 和 <em>b</em> 分别表示预测和实例的边界框，<em>s</em> 表示空间先验，指示预测的锚点是否在实例内，<em>α</em> 和 <em>β</em> 是两个重要的超参数，平衡语义预测任务和位置回归任务的影响。</p><h3 id="全局效率-准确性驱动的模型设计"><a href="#全局效率-准确性驱动的模型设计" class="headerlink" title="全局效率-准确性驱动的模型设计"></a>全局效率-准确性驱动的模型设计</h3><p>除了后处理，YOLOs的模型架构也对效率-准确性权衡提出了巨大挑战[50, 8, 29]。尽管以前的工作探索了各种设计策略，但仍然缺乏对YOLOs中各个组件的全面检查。因此，模型架构展现出不可忽视的计算冗余和受限能力，这阻碍了其实现高效率和性能的潜力。在这里，我们旨在从效率和准确性角度全面执行YOLOs的模型设计。</p><p>效率驱动的模型设计。YOLO中的组件包括茎(stem)、下采样层、带有基本构建块的阶段以及头部。茎的计算成本较低，因此我们对其他三个部分采用效率驱动的模型设计。</p><p>（1）轻量级分类头部。在YOLO中，分类和回归头部通常共享相同的架构。然而，它们在计算开销上表现出显著的差异。例如，在YOLOv8-S中，分类头部的FLOPs和参数数量分别为回归头部的2.5倍和2.4倍。然而，在分析分类错误和回归错误的影响后（见表6），我们发现回归头部对YOLOs的性能承担了更大的重要性。因此，我们可以减少分类头部的开销，而不用担心会大幅损害性能。因此，我们简单地采用轻量级的架构用于分类头部，它由两个深度可分离的卷积[25, 9]组成，核大小为3x3，然后是一个1x1卷积。</p><p>（2）空间通道解耦的下采样。YOLO通常利用常规的3x3标准卷积，步长为2，同时实现空间下采样（从H x W到$\frac{H}{2} \times \frac{W}{2}$）和通道变换（从C到2C）。这引入了不可忽视的计算成本，即O(29<em>H<strong>W</strong>C</em>2)，以及参数数量，即O(18$C^2$)。相反，我们提出将空间缩减和通道增加操作解耦，以实现更高效的缩减。具体来说，我们首先利用逐点卷积来调节通道维度，然后利用深度卷积来进行空间缩减。这将计算成本降低到$O(2HWC^2+ \frac{9}{2}HWC)$，参数数量减少到$O(2C^2+18C)$。同时，在缩减过程中最大化信息保留，从而在延迟减少的同时具有竞争力。</p><p>（3）基于rank引导的模块设计：YOLOs通常在所有阶段使用相同的基本构建块，例如YOLOv8中的瓶颈块。为了彻底检查YOLOs的这种同质设计，我们利用内在秩来分析每个阶段的冗余。具体来说，我们计算每个阶段中最后一个基本块的最后一个卷积的数值秩，这计算了大于阈值的奇异值的数量。图3.(a)展示了YOLOv8的结果，表明深层阶段和大型模型更容易表现出更多的冗余。这一观察表明，简单地为所有阶段应用相同的块设计对于最佳的容量-效率权衡是次优的。为了解决这个问题，我们提出了一种基于秩的块设计方案，旨在通过紧凑的架构设计降低被证明是冗余的阶段复杂度。我们首先提出了一个紧凑的倒置块（CIB）结构，它采用廉价的深度可分离卷积进行空间混合，以及成本效益高的点对点卷积进行通道混合，如图3.(b)所示。它可以作为高效的基本构建块，例如嵌入在ELAN结构中（图3.(b)）。然后，我们提倡一种基于秩的块分配策略，以实现最佳效率，同时保持有竞争力的容量。具体来说，给定一个模型，我们根据其内在秩按升序对所有阶段进行排序。我们进一步检查用CIB替换领先阶段的基本块的性能变化。如果与给定模型相比没有性能下降，我们就继续替换下一个阶段，否则就停止该过程。因此，我们可以在不同阶段和模型规模上实现自适应的紧凑块设计，实现更高的效率而不损害性能。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/0af9b20597598897c15c90a02d5c1093-1733232140019-3.png" alt="img"></p><h3 id="准确性驱动的模型设计。"><a href="#准确性驱动的模型设计。" class="headerlink" title="准确性驱动的模型设计。"></a>准确性驱动的模型设计。</h3><p>我们进一步探索大核卷积和自注意力在准确性驱动设计中的应用，旨在以最小的成本提升性能。</p><p>（1）大核卷积。采用大核深度卷积是一种有效的方法来扩大感受野并增强模型的能力[10, 40, 39]。然而，简单地在所有阶段都利用它们可能会引入浅层特征中的污染，同时也会在高分辨率阶段引入显著的I&#x2F;O开销和延迟[8]。因此，我们建议在深度阶段利用CIB中的大核深度卷积。具体来说，我们增加了CIB中第二个3x3深度卷积的核大小为7x7，随后[39]。此外，我们采用结构重参数化技术[11,10,59]来引入另一个3x3深度卷积分支，以缓解优化问题而不增加推理开销。此外，随着模型规模的增加，其感受野自然扩大，使用大核卷积的好处逐渐减弱。因此，我们只在小型模型规模上采用大核卷积。</p><p>(2) 部分自注意力(PSA)。自注意力[58]由于其显著的全球建模能力[38, 14, 76]而被广泛应用于各种视觉任务。然而，它表现出高计算复杂性和内存占用。为了解决这个问题，鉴于普遍存在的注意力头重用[69]，我们提出了一个高效的局部自注意力(PSA)模块设计，如图3.(c)所示。具体来说，在1x1卷积之后，我们将通道中的特征均匀划分为两部分。我们只将一部分输入到由多头自注意力模块(MHSA)和前馈网络(FFN)组成的NPSA块中。然后将两部分通过1x1卷积连接并融合。此外，我们遵循[22]的方法，将查询和键的维度分配给MHSA中的值的一半，并用BatchNorm[27]替换LayerNorm[1]以进行快速推理。此外，PSA仅在分辨率最低的第4阶段之后放置，避免了过多的开销。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><p>我们选择YOLOv8[21]作为我们的基线模型，因为它在延迟准确性和各种模型规模的可用性方面表现良好。我们采用了一致的NMS-free训练双重分配，并基于此进行了全体的效率-准确性驱动的模型设计，从而带来了我们的YOLOv10模型。YOLOv10具有与YOLOv8相同的变体，即N&#x2F;S&#x2F;M&#x2F;L&#x2F;X。此外，我们通过简单增加YOLOv10-M的宽度尺度因子，推导出了一个新的变体YOLOv10-B。我们在相同的全局从零开始设置[21, 65, 62]下，在COCO[35]上验证了所提出的检测器。此外，所有模型的延迟都在T4 GPU和TensorRT FP16上进行测试，遵循[78]的方法。</p><h3 id="与最先进技术的比较"><a href="#与最先进技术的比较" class="headerlink" title="与最先进技术的比较"></a>与最先进技术的比较</h3><p>如表1所示，我们的YOLOv10在各种模型规模上实现了最先进的性能和端到端的延迟。我们首先将YOLOv10与我们基线模型，即YOLOv8进行比较。在N&#x2F;S&#x2F;M&#x2F;L&#x2F;X五种变体中，我们的YOLOv10实现了1.2%&#x2F;1.4%&#x2F;0.5%&#x2F;0.3%&#x2F;0.5%的AP改进，参数减少了28%&#x2F; 36%&#x2F; 41%&#x2F; 44%&#x2F; 57%，计算减少了23%&#x2F; 24%&#x2F; 25%&#x2F; 27%&#x2F; 38%，延迟减少了70%&#x2F;65%&#x2F;50%&#x2F;41%&#x2F;37%。与其他YOLO相比，</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203212603338.png" alt="image-20241203212603338"></p><p>YOLOv10在准确性和计算成本之间也展现了卓越的权衡。具体来说，对于轻量级和小型的模型，YOLOv10-N&#x2F;S的性能超过了YOLOv6-3.0-N&#x2F;S，分别提高了1.5 AP和2.0 AP，参数减少了51%，计算量减少了41%。对于中等规模的模型，与YOLOv9-C&#x2F; YOLO-MS相比，YOLOv10-B&#x2F;M在相同或更好的性能下，延迟降低了46%&#x2F;62%。对于大型模型，与Gold-YOLO-L相比，我们的YOLOv10-L在参数减少了68%，延迟降低了32%，并且AP显著提高了1.4%。此外，与RT-DETR相比，YOLOv10获得了显著的性能和延迟提升。值得注意的是，在相似的性能下，YOLOv10-S&#x2F;X的推理速度比RT-DETR-R18&#x2F;R101快了1.8倍和1.3倍。这些结果充分展示了YOLOv10作为实时端到端检测器的优越性。</p><p>我们还使用原始的一对多训练方法将YOLOv10与其他YOLO进行了比较。在这种情况下，我们考虑了模型前向过程（Latencyf）的性能和延迟[62, 21, 60]。如表1所示，YOLOv10在不同模型规模上也展现了最先进的表现和效率，这表明了我们架构设计的有效性。</p><h3 id="模型分析"><a href="#模型分析" class="headerlink" title="模型分析"></a>模型分析</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203212826090.png" alt="image-20241203212826090"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213021438.png" alt="image-20241203213021438"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213123173.png" alt="image-20241203213123173"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241203213200887.png" alt="image-20241203213200887"></p><ol><li><p><strong>消融研究</strong>：基于YOLOv10-S和YOLOv10-M的消融研究表明，无NMS训练结合一致的双标签分配显著降低了YOLOv10-S的端到端延迟，同时保持了44.3%的AP竞争力。此外，效率驱动的设计减少了参数数量和计算量，并显著降低了延迟。</p></li><li><p><strong>双标签分配</strong>：双标签分配为无NMS的YOLO提供了丰富的监督信息，并在推理时实现了高效性。一致匹配度量的引入进一步缩小了两个分支之间的监督差距，提高了性能。</p></li><li><p><strong>效率驱动模型设计</strong>：效率驱动模型设计通过轻量级分类头、空间-通道解耦下采样和紧凑倒置块（CIB）等组件，有效减少了参数数量、FLOPs和延迟，同时保持了竞争性的性能。</p></li><li><p><strong>准确性驱动模型设计</strong>：准确性驱动模型设计通过大核卷积和部分自注意力（PSA）模块，在不显著增加延迟的情况下提高了性能。</p></li><li><p><strong>大核卷积</strong>：大核卷积的使用扩大了感受野并增强了模型能力，但在小模型中效果更佳。</p></li><li><p><strong>部分自注意力模块</strong>：PSA模块通过减少自注意力头中的冗余来缓解优化问题，从而在不牺牲高效率的情况下提升了模型性能。</p></li></ol><h1 id="YOLOv10代码"><a href="#YOLOv10代码" class="headerlink" title="YOLOv10代码"></a>YOLOv10代码</h1><h3 id="C2fUIB介绍"><a href="#C2fUIB介绍" class="headerlink" title="C2fUIB介绍"></a>C2fUIB介绍</h3><p><strong>C2fUIB只是用CIB结构替换了YOLOv8中 C2f的Bottleneck结构</strong></p><p><strong>实现代码ultralytics&#x2F;nn&#x2F;modules&#x2F;block.py</strong></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/8ed70c479c7530fe3d36f4f44fbbf2d8.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/a3198a08d3b755ec2e1e85cb8979c2ee.png" alt="img"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CIB</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Standard bottleneck.&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, c1, c2, shortcut=<span class="hljs-literal">True</span>, e=<span class="hljs-number">0.5</span>, lk=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initializes a bottleneck module with given input/output channels, shortcut option, group, kernels, and</span><br><span class="hljs-string">        expansion.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        c_ = <span class="hljs-built_in">int</span>(c2 * e)  <span class="hljs-comment"># hidden channels</span><br>        <span class="hljs-variable language_">self</span>.cv1 = nn.Sequential(<br>            Conv(c1, c1, <span class="hljs-number">3</span>, g=c1),<br>            Conv(c1, <span class="hljs-number">2</span> * c_, <span class="hljs-number">1</span>),<br>            Conv(<span class="hljs-number">2</span> * c_, <span class="hljs-number">2</span> * c_, <span class="hljs-number">3</span>, g=<span class="hljs-number">2</span> * c_) <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> lk <span class="hljs-keyword">else</span> RepVGGDW(<span class="hljs-number">2</span> * c_),<br>            Conv(<span class="hljs-number">2</span> * c_, c2, <span class="hljs-number">1</span>),<br>            Conv(c2, c2, <span class="hljs-number">3</span>, g=c2),<br>        )<br><br>        <span class="hljs-variable language_">self</span>.add = shortcut <span class="hljs-keyword">and</span> c1 == c2<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;&#x27;forward()&#x27; applies the YOLO FPN to input data.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> x + <span class="hljs-variable language_">self</span>.cv1(x) <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.add <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.cv1(x)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">C2fCIB</span>(<span class="hljs-title class_ inherited__">C2f</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Faster Implementation of CSP Bottleneck with 2 convolutions.&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, c1, c2, n=<span class="hljs-number">1</span>, shortcut=<span class="hljs-literal">False</span>, lk=<span class="hljs-literal">False</span>, g=<span class="hljs-number">1</span>, e=<span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,</span><br><span class="hljs-string">        expansion.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>().__init__(c1, c2, n, shortcut, g, e)<br>        <span class="hljs-variable language_">self</span>.m = nn.ModuleList(CIB(<span class="hljs-variable language_">self</span>.c, <span class="hljs-variable language_">self</span>.c, shortcut, e=<span class="hljs-number">1.0</span>, lk=lk) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n))<br></code></pre></td></tr></table></figure><h3 id="PSA介绍"><a href="#PSA介绍" class="headerlink" title="PSA介绍"></a>PSA介绍</h3><p>具体来说，我们在1×1卷积后将特征均匀地分为两部分。我们只将一部分输入到由多头自注意力模块（MHSA）和前馈网络（FFN）组成的NPSA块中。然后，两部分通过1×1卷积连接并融合。此外，遵循将查询和键的维度分配为值的一半，并用BatchNorm替换LayerNorm以实现快速推理。</p><p><strong>实现代码ultralytics&#x2F;nn&#x2F;modules&#x2F;block.py</strong></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/82d0ed0834ac712354683efcb0108bc6.png" alt="img"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Attention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, num_heads=<span class="hljs-number">8</span>,</span><br><span class="hljs-params">                 attn_ratio=<span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.num_heads = num_heads<br>        <span class="hljs-variable language_">self</span>.head_dim = dim // num_heads<br>        <span class="hljs-variable language_">self</span>.key_dim = <span class="hljs-built_in">int</span>(<span class="hljs-variable language_">self</span>.head_dim * attn_ratio)<br>        <span class="hljs-variable language_">self</span>.scale = <span class="hljs-variable language_">self</span>.key_dim ** -<span class="hljs-number">0.5</span><br>        nh_kd = nh_kd = <span class="hljs-variable language_">self</span>.key_dim * num_heads<br>        h = dim + nh_kd * <span class="hljs-number">2</span><br>        <span class="hljs-variable language_">self</span>.qkv = Conv(dim, h, <span class="hljs-number">1</span>, act=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.proj = Conv(dim, dim, <span class="hljs-number">1</span>, act=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.pe = Conv(dim, dim, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, g=dim, act=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        B, _, H, W = x.shape<br>        N = H * W<br>        qkv = <span class="hljs-variable language_">self</span>.qkv(x)<br>        q, k, v = qkv.view(B, <span class="hljs-variable language_">self</span>.num_heads, -<span class="hljs-number">1</span>, N).split([<span class="hljs-variable language_">self</span>.key_dim, <span class="hljs-variable language_">self</span>.key_dim, <span class="hljs-variable language_">self</span>.head_dim], dim=<span class="hljs-number">2</span>)<br><br>        attn = (<br>            (q.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>) @ k) * <span class="hljs-variable language_">self</span>.scale<br>        )<br>        attn = attn.softmax(dim=-<span class="hljs-number">1</span>)<br>        x = (v @ attn.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)).view(B, -<span class="hljs-number">1</span>, H, W) + <span class="hljs-variable language_">self</span>.pe(v.reshape(B, -<span class="hljs-number">1</span>, H, W))<br>        x = <span class="hljs-variable language_">self</span>.proj(x)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PSA</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, c1, c2, e=<span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-keyword">assert</span>(c1 == c2)<br>        <span class="hljs-variable language_">self</span>.c = <span class="hljs-built_in">int</span>(c1 * e)<br>        <span class="hljs-variable language_">self</span>.cv1 = Conv(c1, <span class="hljs-number">2</span> * <span class="hljs-variable language_">self</span>.c, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.cv2 = Conv(<span class="hljs-number">2</span> * <span class="hljs-variable language_">self</span>.c, c1, <span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-variable language_">self</span>.attn = Attention(<span class="hljs-variable language_">self</span>.c, attn_ratio=<span class="hljs-number">0.5</span>, num_heads=<span class="hljs-variable language_">self</span>.c // <span class="hljs-number">64</span>)<br>        <span class="hljs-variable language_">self</span>.ffn = nn.Sequential(<br>            Conv(<span class="hljs-variable language_">self</span>.c, <span class="hljs-variable language_">self</span>.c*<span class="hljs-number">2</span>, <span class="hljs-number">1</span>),<br>            Conv(<span class="hljs-variable language_">self</span>.c*<span class="hljs-number">2</span>, <span class="hljs-variable language_">self</span>.c, <span class="hljs-number">1</span>, act=<span class="hljs-literal">False</span>)<br>        )<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        a, b = <span class="hljs-variable language_">self</span>.cv1(x).split((<span class="hljs-variable language_">self</span>.c, <span class="hljs-variable language_">self</span>.c), dim=<span class="hljs-number">1</span>)<br>        b = b + <span class="hljs-variable language_">self</span>.attn(b)<br>        b = b + <span class="hljs-variable language_">self</span>.ffn(b)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.cv2(torch.cat((a, b), <span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><h3 id="SCDown"><a href="#SCDown" class="headerlink" title="SCDown"></a>SCDown</h3><p>OLOs通常利用常规的3×3标准卷积，步长为2，同时实现空间下采样（从H×W到H&#x2F;2×W&#x2F;2）和通道变换（从C到2C）。这引入了不可忽视的计算成本$O(9HWC^2)$和参数数量O$(18C^2)$。相反，我们提议将空间缩减和通道增加操作解耦，以实现更高效的下采样。具体来说，我们首先利用点对点卷积来调整通道维度，然后利用深度可分离卷积进行空间下采样。这将计算成本降低到O(2HWC^2 + 9HWC)，并将参数数量减少到O(2C^2 + 18C)。同时，它最大限度地保留了下采样过程中的信息，从而在减少延迟的同时保持了有竞争力的性能。</p><p><strong>实现代码ultralytics&#x2F;nn&#x2F;modules&#x2F;block.py</strong></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SCDown</span>(nn.<span class="hljs-title class_">Module</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, c1, c2, k, s</span>):<br>        <span class="hljs-variable language_">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.cv1 = <span class="hljs-title class_">Conv</span>(c1, c2, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.cv2 = <span class="hljs-title class_">Conv</span>(c2, c2, k=k, s=s, g=c2, act=<span class="hljs-title class_">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.cv2(<span class="hljs-variable language_">self</span>.cv1(x))<br></code></pre></td></tr></table></figure><p>参考：<a href="https://cloud.tencent.com/developer/article/2426044">YOLOv10真正实时端到端目标检测（原理介绍+代码详见+结构框图）-腾讯云开发者社区-腾讯云</a></p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink">https://github.com/chongzicbo/ReadWriteThink</a></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>computer-vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>yolo</tag>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>YOLO模型的全面综述</title>
    <link href="/2024/12/07/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/computer-vision/CV008-%E8%AF%84%E4%BC%B0%20YOLO%20%EF%BC%88You%20Only%20Look%20Once%EF%BC%89%20%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%BC%94%E5%8F%98%EF%BC%9AYOLO11%20%E5%8F%8A%E5%85%B6%E5%89%8D%E8%BA%AB%E7%9A%84%E5%85%A8%E9%9D%A2%E5%9F%BA%E5%87%86%E7%A0%94%E7%A9%B6/"/>
    <url>/2024/12/07/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/computer-vision/CV008-%E8%AF%84%E4%BC%B0%20YOLO%20%EF%BC%88You%20Only%20Look%20Once%EF%BC%89%20%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%BC%94%E5%8F%98%EF%BC%9AYOLO11%20%E5%8F%8A%E5%85%B6%E5%89%8D%E8%BA%AB%E7%9A%84%E5%85%A8%E9%9D%A2%E5%9F%BA%E5%87%86%E7%A0%94%E7%A9%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本研究对YOLO （You Only Look Once） 的各个版本进行了全面的基准测试分析，从 YOLOv3 到最新的算法。它代表了首次全面评估 YOLO11 性能的研究，YOLO11 是 YOLO 系列的最新成员。它评估了它们在三个不同数据集上的性能：交通标志（具有不同的对象大小）、非洲野生动物（具有不同的纵横比，每个图像至少有一个对象实例）以及船舶和船只（具有单个类别的小型对象），确保在具有不同挑战的数据集之间进行全面评估。为了确保稳健的评估，我们采用了一套全面的指标，包括精度、召回率、平均精度均值 （mAP）、处理时间、GFLOP 计数和模型大小。我们的分析强调了每个 YOLO 版本的独特优势和局限性。例如：YOLOv9 表现出很高的准确性，但在检测小物体和效率方面表现不佳，而 YOLOv10 表现出相对较低的准确性，因为架构选择会影响其在重叠物体检测方面的性能，但在速度和效率方面表现出色。此外，YOLO11 系列在准确性、速度、计算效率和模型大小方面始终表现出卓越的性能。YOLO11m 在准确性和效率之间取得了显著的平衡，在交通标志、非洲野生动物和船舶数据集上的mAP50-95得分分别为0.795、0.81和0.325，同时保持了2.4毫秒的平均推理时间，模型大小为38.8Mb，平均约为67.6 GFLOPs。这些结果为工业界和学术界提供了重要的见解，有助于为各种应用选择最合适的 YOLO 算法，并指导未来的增强功能。</p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/x1.png" alt="Refer to caption"></p><p>​                             Figure 1:Evolution of YOLO Algorithms throughout the years.</p><p>主要介绍了物体检测在计算机视觉系统中的重要性及其应用，并概述了YOLO（You Only Look Once）算法的发展历程和优势。</p><ul><li><strong>物体检测的重要性</strong>：物体检测是计算机视觉系统的关键组成部分，广泛应用于自动驾驶、机器人技术、库存管理、视频监控和体育分析等领域。</li><li><strong>传统方法的局限性</strong>：传统的物体检测方法如Viola-Jones算法和DPM模型在鲁棒性和泛化能力上存在局限，而深度学习方法已成为主流。</li><li><strong>一阶段与两阶段方法</strong>：一阶段方法如RetinaNet和SSD在速度和准确性之间取得平衡，而两阶段方法如R-CNN提供高精度但计算密集。</li><li><strong>YOLO算法的崛起</strong>：YOLO算法以其鲁棒性和效率脱颖而出，自2015年首次提出以来，通过不断改进框架和设计，成为实时物体检测的领先算法。</li><li><strong>YOLO算法的演进</strong>：YOLO算法的演进包括从YOLOv1到YOLOv11的多个版本，每个版本都引入了新的架构和技术来提高性能。</li><li><strong>Ultralytics的角色</strong>：Ultralytics在YOLO算法的发展中扮演了重要角色，通过维护和改进模型，使其更易于访问和定制。</li><li><strong>研究目的</strong>：本研究旨在对YOLO算法的演变进行全面比较分析，特别是对最新成员YOLO11进行首次全面评估，并探讨其在不同应用场景中的优势和局限性。</li><li><strong>研究方法</strong>：研究使用了三个多样化的数据集，并采用了一致的超参数设置，以确保公平和无偏见的比较。</li><li><strong>研究贡献</strong>：研究的贡献在于提供了对YOLO11及其前身的全面比较，深入分析了这些算法的结构演变，并扩展了性能评估指标，为选择最适合特定用例的YOLO算法提供了宝贵的见解。</li></ul><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>主要回顾了YOLO算法的演变、不同版本的架构、以及与其他计算机视觉算法的基准测试。以下是对该章节的详细总结分析：</p><h3 id="YOLO算法的演变："><a href="#YOLO算法的演变：" class="headerlink" title="YOLO算法的演变："></a>YOLO算法的演变：</h3><ul><li>论文[14]分析了包括YOLOv8在内的七种语义分割和检测算法，用于云层分割的遥感图像。</li><li>论文[22]回顾了YOLO从版本1到版本8的演变，但没有考虑YOLOv9、YOLOv10和YOLO11。</li><li>论文[12]详细分析了从YOLOv1到YOLOv4的单阶段物体检测器，并比较了两阶段和单阶段物体检测器。</li><li>论文[53]探讨了YOLO从版本1到10的演变，强调了其在汽车安全、医疗保健等领域的应用。</li><li>论文[61]讨论了YOLO算法的发展直到第四版，并提出了新的方法和挑战。</li><li>论文[27]分析了YOLO算法的发展和性能，比较了从第8版到第8版的YOLO版本。</li></ul><h3 id="YOLO算法的应用："><a href="#YOLO算法的应用：" class="headerlink" title="YOLO算法的应用："></a>YOLO算法的应用：</h3><ul><li>YOLO算法在自动驾驶、医疗保健、工业制造、监控和农业等领域有广泛应用。</li><li>YOLOv8提供了多种应用，包括实例分割、姿态估计和定向物体检测（OOB）。</li></ul><h3 id="YOLO算法的基准测试："><a href="#YOLO算法的基准测试：" class="headerlink" title="YOLO算法的基准测试："></a>YOLO算法的基准测试：</h3><ul><li>论文[14]进行了云层分割的基准测试，评估了不同算法的架构方法和性能。</li><li>论文[22]提出了结合联邦学习以提高隐私、适应性和协作训练的通用性。</li><li>论文[12]提供了单阶段和两阶段物体检测器的比较。</li><li>论文[53]探讨了YOLO算法对未来AI驱动应用的潜在整合。</li><li>论文[61]强调了YOLO算法在物体检测方面的挑战和需要进一步研究的地方。</li></ul><h3 id="YOLO算法的挑战："><a href="#YOLO算法的挑战：" class="headerlink" title="YOLO算法的挑战："></a>YOLO算法的挑战：</h3><ul><li>YOLO算法在处理小物体和不同旋转角度的物体时面临挑战。</li><li>YOLOv9、YOLOv10和YOLO11的最新模型在准确性和效率方面表现出色，但在某些情况下仍需改进。</li></ul><h3 id="YOLO算法的改进："><a href="#YOLO算法的改进：" class="headerlink" title="YOLO算法的改进："></a>YOLO算法的改进：</h3><ul><li>YOLOv9引入了信息瓶颈原理和可逆函数来保留数据，提高了模型的收敛性和性能。</li><li>YOLOv10通过增强的CSP-Net主干和PAN层提高了梯度流动和减少了计算冗余。</li><li>YOLO11引入了C2PSA模块，结合了跨阶段部分网络和自注意力机制，提高了检测精度。</li></ul><h3 id="YOLO算法的未来方向："><a href="#YOLO算法的未来方向：" class="headerlink" title="YOLO算法的未来方向："></a>YOLO算法的未来方向：</h3><ul><li>未来的研究可以专注于优化YOLOv10以提高其准确性，同时保持其速度和效率优势。</li><li>继续改进架构设计可能会带来更先进的YOLO算法。</li></ul><h3 id="研究贡献："><a href="#研究贡献：" class="headerlink" title="研究贡献："></a>研究贡献：</h3><ul><li>本研究首次全面比较了YOLO11及其前身，并在三个多样化的数据集上评估了它们的性能。</li><li>研究结果为工业界和学术界提供了选择最适合特定应用场景的YOLO算法的宝贵见解。</li></ul><p>通过这些分析，可以看出YOLO算法在不断演进和改进，以适应不同的应用需求和挑战。</p><h1 id="Benchmark-设置"><a href="#Benchmark-设置" class="headerlink" title="Benchmark 设置"></a>Benchmark 设置</h1><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>介绍了三种数据集，分别是Traffic Signs Dataset、Africa Wildlife Dataset和Ships&#x2F;Vessels Dataset。以下是对这三种数据集的详细介绍：</p><h4 id="1-Traffic-Signs-Dataset（交通标志数据集）"><a href="#1-Traffic-Signs-Dataset（交通标志数据集）" class="headerlink" title="1. Traffic Signs Dataset（交通标志数据集）"></a>1. Traffic Signs Dataset（交通标志数据集）</h4><ul><li><strong>来源</strong>：由Radu Oprea在Kaggle上提供的开源数据集。</li><li>特点：<ul><li>包含约55个类别的交通标志图像。</li><li>训练集包含3253张图像，验证集包含1128张图像。</li><li>图像大小不一，初始尺寸为640x640像素。</li><li>为了平衡不同类别的数量，采用了欠采样技术。</li></ul></li><li><strong>应用领域</strong>：自动驾驶、交通管理、道路安全和智能交通系统。</li><li>挑战：<ul><li>目标物体大小变化较大。</li><li>不同类别之间的模式相似，增加了检测难度。</li></ul></li></ul><h4 id="2-Africa-Wildlife-Dataset（非洲野生动物数据集）"><a href="#2-Africa-Wildlife-Dataset（非洲野生动物数据集）" class="headerlink" title="2. Africa Wildlife Dataset（非洲野生动物数据集）"></a>2. Africa Wildlife Dataset（非洲野生动物数据集）</h4><ul><li><strong>来源</strong>：由Bianca Ferreira在Kaggle上设计的开源数据集。</li><li>特点：<ul><li>包含四种常见的非洲动物类别：水牛、大象、犀牛和斑马。</li><li>每个类别至少有376张图像，通过Google图像搜索收集并手动标注为YOLO格式。</li><li>数据集分为训练集、验证集和测试集，比例为70%、20%和10%。</li></ul></li><li><strong>应用领域</strong>：野生动物保护、反偷猎、生物多样性监测和生态研究。</li><li>挑战：<ul><li>目标物体的宽高比变化较大。</li><li>每张图像至少包含一种指定的动物类别，可能还包含其他类别的多个实例或发生情况。</li><li>目标物体重叠，增加了检测难度。</li></ul></li></ul><h4 id="3-Ships-Vessels-Dataset（船舶数据集）"><a href="#3-Ships-Vessels-Dataset（船舶数据集）" class="headerlink" title="3. Ships&#x2F;Vessels Dataset（船舶数据集）"></a>3. Ships&#x2F;Vessels Dataset（船舶数据集）</h4><ul><li><strong>来源</strong>：由Siddharth Sah从多个Roboflow数据集中收集并整理的开源数据集。</li><li>特点：<ul><li>包含约13.5k张图像，专门用于船舶检测。</li><li>每张图像都使用YOLO格式手动标注了边界框。</li><li>数据集分为训练集、验证集和测试集，比例为70%、20%和10%。</li></ul></li><li><strong>应用领域</strong>：海事安全、渔业管理、海洋污染监测、国防、海事安全和更多实际应用。</li><li>挑战：<ul><li>目标物体（船舶）相对较小。</li><li>目标物体具有不同的旋转角度，增加了检测难度。</li></ul></li></ul><p>这些数据集在对象检测研究中具有重要意义，因为它们涵盖了不同大小、形状和密度的对象，能够全面评估YOLO算法在不同场景下的性能。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><h4 id="比较分析：Ultralytics-vs-原始YOLO模型"><a href="#比较分析：Ultralytics-vs-原始YOLO模型" class="headerlink" title="比较分析：Ultralytics vs 原始YOLO模型"></a>比较分析：Ultralytics vs 原始YOLO模型</h4><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202134407205.png"></p><p>在Traffic Signs数据集上，对Ultralytics提供的版本和原始模型进行比较分析，使用相同的超参数设置如表V所示。目标是为了强调突出Ultralytics提供的版本和原始模型之间的差异。由于Ultraytics缺乏对YOLO v4、YOLO v6、YOLO v7的支持，因此本文将这几个YOLO版本排除在外了。</p><h5 id="Ultralytics支持库中的模型和任务"><a href="#Ultralytics支持库中的模型和任务" class="headerlink" title="Ultralytics支持库中的模型和任务"></a>Ultralytics支持库中的模型和任务</h5><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202134825220.png" alt="image-20241202134825220"></p><p>根据表I，Ultralytics库为研究人员和程序员提供了各种YOLO模型，用于推理、验证、训练和导出。我们注意到Ultralytics不支持YOLOv1、YOLOv2、YOLOv4和YOLOv7。对于YOLOv6，库只支持配置文件.yaml，而不支持预训练的.pt模型。</p><h5 id="Ultralytics和原始模型的性能比较"><a href="#Ultralytics和原始模型的性能比较" class="headerlink" title="Ultralytics和原始模型的性能比较"></a>Ultralytics和原始模型的性能比较</h5><p>通过对Ultralytics模型及其原始版本在交通标志数据集上的比较分析，我们观察到Ultralytics版本和原始版本之间存在显著差异。例如，Ultralytics版本的YOLOv5n（nano）和YOLOv3表现优越，突显了Ultralytics所做的增强和优化。相反，原始版本的YOLOv9c（compact）略微优于其Ultralytics版本，可能是由于Ultralytics对该较新模型的优化不足。这些观察结果表明，Ultralytics模型经过了大量修改，直接比较原始版本和Ultralytics版本是不公平和不准确的。因此，本文将专注于Ultralytics支持的版本，以确保基准测试的一致性和公平性。</p><h6 id="YOLOv3u"><a href="#YOLOv3u" class="headerlink" title="YOLOv3u"></a>YOLOv3u</h6><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202135426435.png" alt="image-20241202135426435"></p><p>YOLOv3基于其前身，旨在提高定位错误和检测效率，特别是对于较小的物体。它使用Darknet-53框架，该框架有53个卷积层，速度是ResNet-152的两倍。YOLOv3还结合了特征金字塔网络（FPN）的元素，如残差块、跳跃连接和上采样，以增强跨不同尺度的物体检测能力。该算法生成三个不同尺度的特征图，以32、16和8的因子对输入进行下采样，并使用三尺度检测机制来检测大、中、小尺寸物体，分别使用不同的特征图。尽管有所改进，YOLOv3在检测中等和大型物体时仍面临挑战，因此Ultralytics发布了YOLOv3u。YOLOv3u是YOLOv3的改进版本，使用无锚点检测方法，并提高了YOLOv3的准确性和速度，特别是对于中等和大型物体。</p><h6 id="YOLOv5u"><a href="#YOLOv5u" class="headerlink" title="YOLOv5u"></a>YOLOv5u</h6><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202135925381.png" alt="image-20241202135925381"></p><p>YOLOv5由Glenn Jocher提出，从Darknet框架过渡到PyTorch，保留了YOLOv4的许多改进，并使用CSPDarknet作为其骨干。CSPDarknet是原始Darknet架构的修改版本，通过将特征图分成单独的路径来实现更高效的特征提取和减少计算成本。YOLOv5采用步幅卷积层，旨在减少内存和计算成本。此外，该版本采用空间金字塔池化快速（SPPF）模块，通过在不同尺度上池化特征并提供多尺度表示来工作。YOLOv5实现了多种增强，如马赛克、复制粘贴、随机仿射、MixUp、HSV增强和随机水平翻转。Ultralytics通过YOLOv5u积极改进该模型，采用无锚点检测方法，并在复杂物体的不同尺寸上实现了更好的整体性能。</p><h6 id="YOLOv8"><a href="#YOLOv8" class="headerlink" title="YOLOv8"></a>YOLOv8</h6><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140006851.png" alt="image-20241202140006851"></p><p>Ultralytics引入了YOLOv8，这是YOLO系列的重大进化，包括五个缩放版本。除了物体检测外，YOLOv8还提供了图像分类、姿态估计、实例分割和定向物体检测（OOB）等多种应用。关键特性包括类似于YOLOv5的主干，调整后的CSPLayer（现称为C2f模块），结合了高级特征和上下文信息以提高检测精度。YOLOv8还引入了一个语义分割模型YOLOv8-Seg，结合了CSPDarknet53特征提取器和C2F模块，在物体检测和语义分割基准测试中取得了最先进的结果，同时保持了高效率。</p><h6 id="YOLOv9"><a href="#YOLOv9" class="headerlink" title="YOLOv9"></a>YOLOv9</h6><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140048800.png" alt="image-20241202140048800"></p><p>YOLOv9由Chien-Yao Wang、I-Hau Yeh和Hong-Yuan Mark Liao开发，使用信息瓶颈原理和可逆函数来在网络深度中保留关键数据，确保可靠的梯度生成并提高模型收敛性和性能。可逆函数可以在不丢失信息的情况下反转，这是YOLOv9架构的另一个基石。这种属性允许网络保持完整的信息流，使模型参数的更新更加准确。此外，YOLOv9提供了五个缩放版本，重点是轻量级模型，这些模型通常欠参数化，并且在前向过程中容易丢失重要信息。可编程梯度信息（PGI）是YOLOv9引入的一项重大进步。PGI是一种在训练期间动态调整梯度信息的方法，通过选择性关注最具信息量的梯度来优化学习效率。通过这种方式，PGI有助于保留可能在轻量级模型中丢失的关键信息。此外，YOLOv9还包括GELAN（梯度增强轻量级架构网络），这是一种新的架构改进，旨在通过优化网络内的计算路径来提高参数利用和计算效率。</p><h6 id="YOLOv10"><a href="#YOLOv10" class="headerlink" title="YOLOv10"></a>YOLOv10</h6><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140137372.png" alt="image-20241202140137372"></p><p>YOLOv10由清华大学的研究人员开发，基于先前模型的优势进行了关键创新。该架构具有增强的CSP-Net（跨阶段部分网络）主干，以提高梯度流动和减少计算冗余。网络结构分为三部分：主干、颈部和检测头。颈部包括PAN（路径聚合网络）层，用于有效的多尺度特征融合。PAN旨在通过聚合不同层的特征来增强信息流，使网络能够更好地捕捉和结合不同尺度的细节，这对于检测不同大小的物体至关重要。此外，该版本还提供五个缩放版本，从纳米到超大。对于推理，One-to-One Head为每个物体生成单个最佳预测，消除了对非极大值抑制（NMS）的需求。通过移除对NMS的需求，YOLOv10减少了延迟并提高了后处理速度。此外，YOLOv10还包括NMS-Free Training，使用一致的双重分配来减少推理延迟，并优化了从效率和准确性角度的各种组件，包括轻量级分类头、空间-通道解耦下采样和排名引导块设计。此外，该模型还包括大核卷积和部分自注意力模块，以在不显著增加计算成本的情况下提高性能。</p><h6 id="YOLO11"><a href="#YOLO11" class="headerlink" title="YOLO11"></a>YOLO11</h6><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140334746.png" alt="image-20241202140334746"></p><p>YOLO11是Ultralytics推出的最新创新，基于其前身的发展，特别是YOLOv8。这一迭代提供了从纳米到超大的五种缩放模型，适用于各种应用。与YOLOv8一样，YOLO11包括物体检测、实例分割、图像分类、姿态估计和定向物体检测（OBB）等多种应用。关键改进包括引入C2PSA（跨阶段部分自注意力）模块，结合了跨阶段部分网络和自注意力机制的优势。这使得模型能够在多个层次上更有效地捕获上下文信息，提高物体检测精度，特别是对于小型和重叠物体。此外，在YOLO11中，C2f块被C3k2块取代，C3k2是CSP Bottleneck的自定义实现，使用两个卷积而不是YOLOv8中使用的一个大卷积。这个块使用较小的内核，在保持精度的同时提高了效率和速度。</p><h3 id="硬件和软件设置"><a href="#硬件和软件设置" class="headerlink" title="硬件和软件设置"></a>硬件和软件设置</h3><ul><li>表III：实验的软件设置</li><li>表IV：6个YOLO版本的不同尺寸的模型</li></ul><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140352257.png" alt="image-20241202140352257"></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202140542480.png" alt="image-20241202140542480"></p><p>总结了用于评估YOLO模型的硬件和软件环境设置。</p><ol><li><strong>软件环境</strong>：实验使用了Python 3.12、Ubuntu 22.04、CUDA 12.5、cuDNN 8.9.7、Ultralytics 8.2.55和WandB 0.17.4等软件包。</li><li><strong>硬件环境</strong>：实验在两块NVIDIA RTX 4090 GPU上进行，每块GPU拥有16,384个CUDA核心。</li><li><strong>数据集处理</strong>：针对交通标志数据集，应用了欠采样技术以确保数据集平衡，并将图像数量从4381减少到3233张。</li><li><strong>训练验证测试分割</strong>：非洲野生动物数据集和船只数据集分别按照70%训练、20%验证和10%测试的比例进行分割。</li><li><strong>模型训练</strong>：实验中训练了23个模型，涵盖了5种不同的YOLO版本，并使用了相似的超参数以确保公平比较。</li><li><strong>模型规模</strong>：交通标志数据集包含24个类别，平均每个类别约100张图像；非洲野生动物数据集包含4个类别，每个类别至少有376张图像；船只数据集专注于单一类别的小型物体检测。</li></ol><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>评估指标包括准确性、计算效率和模型大小三个方面：</p><h4 id="准确性指标"><a href="#准确性指标" class="headerlink" title="准确性指标"></a>准确性指标</h4><ol><li><p><strong>Precision（精确率）</strong>：</p><ul><li>定义：正确预测的观察值与总预测观察值的比率。</li><li>计算公式：$$ \text{Precision} &#x3D; \frac{\text{TP}}{\text{TP} + \text{FP}} $$</li><li>其中，TP（True Positives）为真正例，FP（False Positives）为假正例。</li></ul></li><li><p><strong>Recall（召回率）</strong>：</p><ul><li>定义：正确预测的观察值与所有实际观察值的比率。</li><li>计算公式：$$ \text{Recall} &#x3D; \frac{\text{TP}}{\text{TP} + \text{FN}} $$</li><li>其中，FN（False Negatives）为假反例。</li></ul></li><li><p><strong>mAP50（Mean Average Precision at an IoU threshold of 0.50）</strong>：</p><ul><li>定义：在IoU（Intersection over Union）阈值为0.50时的平均精度均值。</li><li>计算公式：$$ \text{mAP50} &#x3D; \frac{1}{|C|} \sum_{c \in C} \text{AP}_c $$</li><li>其中，$C$ 是类别集合，$\text{AP}_c$ 是类别 $c$ 的平均精度。</li></ul></li><li><p><strong>mAP50-95（Mean Average Precision across IoU thresholds from 0.50 to 0.95）</strong>：</p><ul><li>定义：在IoU阈值从0.50到0.95范围内的平均精度均值。</li><li>计算公式：$$ \text{mAP50-95} &#x3D; \frac{1}{15} \sum_{r&#x3D;1}^{15} \text{AP}_{0.50 + \frac{r-1}{14} \times 0.05} $$</li><li>其中，$r$ 表示IoU阈值的范围。</li></ul></li></ol><h4 id="计算效率指标"><a href="#计算效率指标" class="headerlink" title="计算效率指标"></a>计算效率指标</h4><ol><li><p><strong>Preprocessing Time（预处理时间）</strong>：</p><ul><li>定义：准备原始数据以输入模型所需的持续时间。</li></ul></li><li><p><strong>Inference Time（推理时间）</strong>：</p><ul><li>定义：模型处理输入数据并生成预测所需的持续时间。</li></ul></li><li><p><strong>Postprocessing Time（后处理时间）</strong>：</p><ul><li>定义：将模型的原始预测转换为最终可用格式所需的时间。</li></ul></li><li><p><strong>Total Time（总时间）</strong>：</p><ul><li>定义：预处理时间、推理时间和后处理时间的总和。</li></ul></li><li><p><strong>GFLOPs（Giga Floating-Point Operations Per Second）</strong>：</p><ul><li>定义：模型训练的计算能力，反映其效率。</li></ul></li></ol><h4 id="模型大小指标"><a href="#模型大小指标" class="headerlink" title="模型大小指标"></a>模型大小指标</h4><ol><li><strong>Size（大小）</strong>：<ul><li>定义：模型的实际磁盘大小及其参数数量。</li></ul></li></ol><p>这些指标提供了对YOLO模型性能的全面概述，有助于在不同真实世界场景中选择最优的YOLO算法。</p><h1 id="实验结果和讨论"><a href="#实验结果和讨论" class="headerlink" title="实验结果和讨论"></a>实验结果和讨论</h1><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142033886.png" alt="image-20241202142033886"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><h4 id="交通信号数据集"><a href="#交通信号数据集" class="headerlink" title="交通信号数据集"></a>交通信号数据集</h4><p>YOLO模型在检测交通标志方面的有效性，展示了各种精度范围。最高的mAP50-95为0.799，而最低的精度为0.64。另一方面，最高的mAP50为0.893，而最低的为0.722。mAP50和mAP50-95之间的显著差距表明，模型在处理不同大小的交通标志时，在较高阈值下遇到了困难，这反映了其检测算法中潜在的改进领域。</p><p>a) 准确性：如图8所示，YOLOv5ul展示了最高的准确性，实现了mAP50为0.866和mAP50-95为0.799。紧随其后的是YOLO11m，其mAP50-95为0.795，YOLO11l的mAP50-95为0.794。相比之下，YOLOv10n展示了最低的精度，其mAP50为0.722，mAP50-95为0.64，紧随其后的是YOLOv5un，其mAP50-95为0.665，如数据点在图8中所证明的。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142326776.png" alt="image-20241202142326776"></p><p>b) 精度和召回率：图9阐明了考虑模型大小的情况下精度和召回率之间的权衡。像YOLO11m、YOLO10l、YOLOv9m、YOLOv5ux和YOLO111这样的模型展示了高精度和召回率，特别是YOLO11m实现了0.898的精度和0.826的召回率，同时模型大小为67.9Mb，而YOLOv10l实现了0.873的精度和0.807的召回率，但模型大小显著更大（126.8 Mb）。相比之下，较小的模型如YOLOv10n（精度0.722，召回率0.602）、YOLOv8n（精度0.749，召回率0.688）和YOLO11n（精度0.768，召回率0.695）在两个指标上都表现不佳。这突显了较大模型在交通标志数据集上的优越性能。此外，YOLOv5um的高精度（0.849）和低召回率（0.701）表明了对假阴性的倾向，而YOLOv3u的高召回率（0.849）和低精度（0.75）则表明了对假阳性的倾向。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142423060.png" alt="image-20241202142423060"></p><p>c) 计算效率：在计算效率方面，YOLOv10n是最有效的，每张图片的处理时间为2ms，GFLOPs计数为8.3，如图10和11所示。YOLO11n紧随其后，处理时间为2.2ms，GFLOPs计数为6.4，而YOLOv3u-tiny的处理时间为2.4ms，GFLOPs计数为19，与其他快速模型相比，这使得它在计算上相对低效。然而，数据显示YOLOv9e、YOLOv9m、YOLOv9c和YOLOv9s是效率最低的，推理时间分别为16.1ms、12.1ms、11.6ms和11.1ms，GFLOPs计数分别为189.4、76.7、102.6和26.8。这些发现描绘了一个明显的权衡，即在精度和计算效率之间。</p><p>d) 整体性能：在评估整体性能时，包括准确性、大小和模型效率，YOLO11m作为一个一致的表现最佳的模型脱颖而出。它实现了mAP50-95为0.795，推理时间为2.4ms，模型大小为38.8Mb，GFLOPs计数为67.9，如图8、10、11和表VI中详细说明的。紧随其后的是YOLO111（mAP50-95为0.794，推理时间为4.6ms，大小为49Mb，GFLOPs计数为86.8）和YOLOv10m（mAP50-95为0.781，推理时间为2.4ms，大小为32.1Mb，63.8 GFLOPs计数）。这些结果突显了这些模型在检测各种大小的交通标志方面的稳健性，同时保持了较短的推理时间和较小的模型大小。值得注意的是，YOLO11和YOLOv10家族在准确性和计算效率方面显著优于其他YOLO家族，因为它们的模型在这些数据集上一致超越了其他家族的对应物。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142515870.png" alt="image-20241202142515870"></p><h4 id="非洲野生动物数据集"><a href="#非洲野生动物数据集" class="headerlink" title="非洲野生动物数据集"></a>非洲野生动物数据集</h4><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142815914.png" alt="image-20241202142815914"></p><p>表 VII 展示了 YOLO 模型在非洲野生动物数据集上的性能。该数据集包含大型物体尺寸，重点关注 YOLO 模型预测大型物体的能力以及由于数据集大小而导致过拟合的风险。模型在各个方面的准确性都表现出色，最高性能的模型 mAP50-95 范围从 0.832 到 0.725。这个相对较短的范围反映了模型在检测和分类大型野生动物物体时保持高准确性的有效性。</p><p>a) 准确性：如图 12 所示，YOLOv9s 展现了出色的性能，具有高达 0.832 的 mAP50-95 和 0.956 的 mAP50，展示了其在各种 IoU 阈值下的稳健准确性。YOLOv9c 和 YOLOv9t 紧随其后，mAP50 分数分别为 0.96 和 0.948，召回率分别为 0.896。值得注意的是，YOLOv8n 实现了 mAP50-95 得分分别为 0.83 和 0.825。这些结果突出了 YOLOv9 系列从少量图像样本中有效学习模式的能力，使其特别适合于较小型的数据集。相比之下，YOLOv5un、YOLOv10n 和 YOLOv3u-tiny 显示出较低的 mAP50-95 得分，分别为 0.791、0.786 和 0.725，表明它们在准确性方面的局限性。较大的模型如 YOLO11x、YOLOv5ux、YOLOv5ul 和 YOLOv10l 的表现不佳，可以归因于过拟合，特别是考虑到数据集规模较小。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142853898.png" alt="image-20241202142853898"></p><p>b) 精度和召回率：图 13 表明 YOLO8l 和 YOLO111 实现了最高的精度和召回率，精度值分别为 0.942 和 0.937，召回率分别为 0.898 和 0.896。值得注意的是，YOLOv8n 实现了 0.932 的精度和 0.908 的召回率。总体而言，YOLOv8l 和 YOLO111 在精度和召回率方面表现最佳，YOLOv8n 的表现也相当出色。然而，YOLOv11 模型倾向于产生误报，这反映在其较低的精度和较高的召回率上。与此同时，YOLOv10 在精度和召回率方面的表现均不佳，尽管它是 YOLO 系列中最新的模型之一。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202142924537.png" alt="image-20241202142924537"></p><p>c) 计算效率：如图 14 和 15 所示，YOLOv10n、YOLOv8n 和 YOLOv3u-tiny 是最快的模型，处理时间分别为 2ms 和 1.8ms，GFLOPs 计数分别为 8.2 和 19.1。前两个模型具有相同的处理速度和 GFLOPs 计数，如表 VII 中所示。相比之下，YOLOv9e 展现了最慢的处理时间，为 11.2ms，GFLOPs 计数为 189.3，其次是 YOLOv5ux，处理时间为 7.5ms，GFLOPs 计数为 246.2 GFLOPs 计数。这些结果表明，较大的模型通常需要更多的处理时间和硬件资源，强调了模型大小和处理效率之间的权衡。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143004149.png" alt="image-20241202143004149"></p><p>d) 整体性能：表 VII 和图 13、14 和 15 中的结果表明，YOLOv9t 和 YOLOv9s 在各个指标上持续表现出色，提供高准确性，同时保持较小的模型大小、低 GFLOPs 和短的处理时间，展示了 YOLOv9 较小型模型的稳健性及其在小数据集上的有效性。相比之下，YOLO5ux 和 YOLO11x 尽管具有较大的尺寸和较长的推理时间，但准确性表现不佳，可能是由于过拟合所致。大多数大型模型在这个数据集上的表现都不尽如人意，YOLOv10x 是一个例外，得益于现代架构防止过拟合，表现优异。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143030167.png" alt="image-20241202143030167"></p><h4 id="船只和船舶数据集："><a href="#船只和船舶数据集：" class="headerlink" title="船只和船舶数据集："></a>船只和船舶数据集：</h4><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143313627.png" alt="image-20241202143313627"></p><p>表 VIII 展示了 YOLO 模型在船只和船舶数据集上的性能，这是一个包含微小物体且旋转变化多样的大型数据集。总体而言，模型在检测船只和船舶方面表现出中等效果，mAP50-95 的范围从 0.273 到 0.327。这一表现表明 YOLO 算法在准确检测较小物体方面可能面临挑战，数据集中物体尺寸和旋转的多样性为测试模型能力提供了全面的测试。</p><p>a) 准确性：图 16 中 mAP50-95 和 mAP50 之间的差异凸显了 YOLO 模型在检测小物体时面临的挑战，尤其是在更高的 IoU 阈值下。此外，YOLO 模型在检测不同旋转的物体时也遇到困难。在各个模型中，YOLO11x 实现了最高的准确性，mAP50 为 0.529，mAP50-95 为 0.327，紧随其后的是 YOLO111、YOLO11m 和 YOLO11s，它们记录的 mAP50 值分别为 0.529、0.528 和 0.53，mAP50-95 值分别为 0.327、0.325 和 0.325。这些结果突出了 YOLO11 系列在检测小型和微小物体方面的稳健性。相比之下，YOLOv3u-tiny、YOLOv8n、YOLOv3u 和 YOLOv5n 展示了最低的准确性，mAP50 分数分别为 0.489、0.515、0.519 和 0.514，mAP50-95 分数分别为 0.273、0.297、0.298 和 0.298。这表明 YOLOv3u 的过时架构以及由于数据集规模较大而导致的小型模型的潜在欠拟合。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143334789.png" alt="image-20241202143334789"></p><p>b) 精度和召回率：图 17 表明 YOLOv5ux 的表现优于其他模型，实现了 0.668 的精度和 0.555 的召回率。它紧随其后的是 YOLOv9m（精度为 0.668，召回率为 0.551）和 YOLOv8m（精度为 0.669，召回率为 0.525），两者在尺寸上显著较小（YOLOv9m 为 40.98 Mb，YOLOv8m 为 52.12 Mb）。相比之下，YOLO11n 和 YOLOv10s 表现较差，精度分别为 0.574 和 0.586，召回率分别为 0.51 和 0.511，这可能是由于欠拟合问题。总体而言，YOLO11 模型倾向于产生误报，这反映在其较低的精度和较高的召回率上。与此同时，YOLOv10 在精度和召回率方面的表现均不佳，尽管它是 YOLO 系列中最新的模型之一。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143403226.png" alt="image-20241202143403226"></p><p>c) 计算效率：如图 18 和 19 所示，YOLOv3u-tiny 实现了最快的处理时间，为 2 毫秒，紧随其后的是 YOLOv8n 和 YOLOv5un，两者均记录了 2.3 毫秒。YOLOv10 和 YOLO11 模型也在速度上表现出色，YOLOv10n 和 YOLO11n 分别实现了 2.4 毫秒和 2.5 毫秒的快速推理时间，以及 8.2 和 6.3 的 GFLOPs 计数。相比之下，YOLOv9e 展现了最慢的速度，推理时间为 7.6 毫秒，GFLOPs 计数为 189.3，突显了 YOLOv9 系列在准确性和效率之间的权衡。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143422503.png" alt="image-20241202143422503"></p><p>d) 整体性能：表 VIII 和图 16、17 和 18 中的结果表明，YOLO11s 和 YOLOv10s 在准确性方面表现优异，同时保持了紧凑的尺寸、低 GFLOPs 和快速的处理时间。相比之下，YOLOv3u、YOLOv8x 和 YOLOv8l 未能达到预期，尽管它们的尺寸较大且处理时间较长。这些发现突出了 YOLO11 系列的稳健性和可靠性，特别是在提高 YOLO 系列检测小型和微小物体的性能方面，同时确保高效处理。此外，结果还揭示了 YOLOv9 模型在面对大型数据集和小物体时的表现不佳，尽管它们具有现代架构。</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143442543.png" alt="image-20241202143442543"></p><h3 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h3><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20241202143815028.png" alt="image-20241202143815028"></p><p>基于三个数据集上模型的性能，我们按准确性、速度、GFLOps 计数和大小对它们进行了排名，如表 IX 所示，以便进行全面评估。对于准确性，由于 mAP50-95 指标能够评估模型在一系列 IoU 阈值下的表现，因此我们采用了该指标。对于速度，模型根据总处理时间进行排序，总处理时间包括预处理、推理和后处理持续时间。排名范围从第 1 名（表示最高性能）到第 28 名（表示最低性能），表中的相应排名已加粗显示。</p><p>表 IX 的分析得出了几个关键观察结果：</p><ol><li>准确性：YOLO11m 一致地成为顶级表现者，经常位居前列，紧随其后的是 YOLOv10x、YOLO111、YOLOv9m 和 YOLO11x。这突显了 YOLO11 系列在各种 IoU 阈值和物体大小下的稳健性能，这可以归因于它们使用 C2PSA 来保留上下文信息，从而提高了收敛性和整体性能。此外，大核卷积和部分自注意力模块的实施有助于提高算法的性能。</li></ol><p>相比之下，YOLOv3u-tiny 展现了最低的准确性，特别是在非洲野生动物和船只及船舶数据集上，YOLOv5un 和 YOLOv8n 的表现稍好但仍不理想。这表明 YOLO11 模型目前是要求高准确性的应用中最可靠的。</p><p>紧随 YOLO11 系列之后，YOLOv9 模型在检测各种大小和不同 IoU 阈值的物体方面表现出色。然而，它们可能在检测小物体时遇到困难，这在船只和船舶数据集上可见。相比之下，YOLOv10 系列尽管推出较晚，但在交通标志和非洲动物数据集上的准确性相对较低，导致平均准确性下降了 2.075%，这可以归因于它们采用一对一头部方法而不是非极大值抑制（NMS）来定义边界框。这种策略在捕捉物体时可能会遇到困难，特别是在处理重叠物品时，因为它依赖于每个物体的单个预测。这一限制有助于解释第二个数据集中观察到的相对较差的结果。</p><p>YOLOv3u 的过时架构也导致了其性能不佳，平均准确性比 YOLO11 模型低 6.5%。这种下降可以追溯到其对 2018 年首次引入的较旧 Darknet-53 框架的依赖，该框架可能无法充分应对当代检测挑战。</p><ol start="2"><li>计算效率：YOLOv10n 在速度和 GFLOPs 计数方面始终表现优异，在所有三个数据集上均名列前茅，在速度方面排名第 1，在 GFLOPs 计数方面排名第 5。YOLOv3u-tiny、YOLOv10s 和 YOLO11n 也展示了显著的计算效率。</li></ol><p>YOLOv9e 展现了最慢的推理时间和非常高的 GFLOPs 计数，突显了准确性与效率之间的权衡。YOLO11 的速度提升可归因于它们使用的 C3k2 块，使其适用于需要快速处理的场景，超过了 YOLOv10 和 YOLOv9 模型，分别在速度上平均快了 %1.41 和 %31。</p><p>虽然 YOLOv9 模型在准确性方面表现出色，但它们的推理时间却是最慢的，使它们不太适合对时间敏感的应用。相比之下，YOLOv10 模型虽然略慢于 YOLO11 变体，但仍提供了效率与速度之间的值得称赞的平衡。它们的表现非常适合时间敏感的场景，提供快速处理而不显著牺牲准确性，使它们成为实时应用的可行选择。</p><ol start="3"><li>模型大小：YOLOv9t 是最小的模型，在所有三个数据集上均排名第一，其次是 YOLO11n 和 YOLOv10n。这种模型大小的效率突显了较新 YOLO 版本，特别是 YOLOv10，在高效参数利用方面的进步，实施了空间-通道解耦下采样。</li></ol><p>YOLOv3u 是最大的模型，突显了与其更现代的对应物相比，它的效率低下。</p><ol start="4"><li>整体性能：考虑到准确性、速度、大小和 GFLOPs，YOLO11m、YOLOv11n、YOLO11s 和 YOLOv10s 成为最一致的表现者。它们实现了高准确性、低处理时间和功率以及高效的磁盘使用，使其适用于广泛的应用，其中速度和准确性都至关重要。</li></ol><p>相反，YOLOv9e、YOLOv5ux 和 YOLOv3u 在所有指标上的表现都较差，计算效率低下且相对于其大小表现不佳。YOLO11 模型显示出最佳的整体性能，可能是由于最近的增强功能，如 C3k2 块和 C2PSA 模块。紧随其后的是 YOLOv10 模型，尽管在准确性方面略有逊色，但由于其一对一头部用于预测的实施，在效率方面表现出色。虽然 YOLOv9 在计算效率方面表现不佳，但它在准确性方面仍然具有竞争力，这要归功于其 PGI 集成。这使 YOLOv9 成为优先考虑精度而非速度的应用的可行选择。</p><p>此外，YOLOv8 和 YOLOv5u 展示了竞争性结果，超过了 YOLOv3u 的准确性，这可能是由于 YOLOv3u 的较旧架构。然而，它们的准确性仍然显著低于较新的模型，如 YOLOv9、YOLOv10 和 YOLO11。虽然 YOLOv8 和 YOLOv5u 的处理时间比 YOLOv9 快，但它们的整体表现仍然不如较新的模型。</p><ol start="5"><li>物体大小和旋转检测：YOLO 算法在检测大中型物体方面效果很好，如非洲野生动物和交通标志数据集所证明的那样，准确性很高。然而，它在检测小物体方面存在困难，可能是由于将图像划分为网格，使得识别小而分辨率低的物体变得具有挑战性。此外，YOLO 在处理不同旋转的物体时也面临挑战，因为无法包围旋转物体，导致整体结果不佳。</li></ol><p>为了处理旋转物体，可以实现像 YOLO11 OBB[26] 和 YOLOv8 OBB[25]（定向边界框）这样的模型。保持与标准 YOLOv8 和 YOLO11 相同的基础架构，YOLOv8 OBB 和 YOLO11 OBB 用预测旋转矩形四个角点的头部替换了标准边界框预测头部，允许更准确的定位和表示任意方向的物体。</p><ol start="6"><li><p>YOLO11 对 YOLOv8 的崛起：尽管 YOLOv8[25] 因其在姿态估计、实例分割和定向物体检测（OBB）任务中的多功能性而成为算法的首选，但 YOLO11[26] 已经成为一个更高效和准确的替代品。通过处理相同任务的同时提供改进的上下文理解和更好的架构模块，YOLO11 设定了新的性能标准，在各种应用中的速度和准确性方面都超过了 YOLOv8。</p></li><li><p>数据集大小：数据集的大小显著影响 YOLO 模型的性能。例如，大型模型在小型非洲野生动物数据集上的表现不如在交通标志和船只及船舶数据集上的表现，因为它们更容易过拟合。相反，像 YOLOv9t 和 YOLOv9s 这样的小模型在非洲野生动物数据集上的表现显著更好，展示了小规模模型在处理有限数据集时的有效性。</p></li><li><p>训练数据集的影响：如表 VI、VII 和 VIII 所示，YOLO 模型的性能受到所使用的训练数据集的影响。不同的数据集产生不同的结果和顶尖表现者，表明数据集复杂性影响算法性能。这突显了在基准测试期间使用多样化数据集以获得每个模型优缺点全面结果的重要性。</p></li></ol><p>这次讨论强调了在选择 YOLO 模型进行特定应用时，需要平衡考虑准确性、速度和模型大小。YOLO11 模型在各个指标上的一致表现使它们非常适合于需要准确性和速度的多功能场景。同时，YOLOv10 模型可以在保持更快处理时间和更小模型大小的同时，类似地执行。此外，YOLOv9 可以在准确性方面提供可比的结果，但牺牲了速度，使其适用于优先考虑精度而非快速处理的应用。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>这项基准研究全面评估了各种 YOLO 算法的性能。它是首个对 YOLO11 及其前辈进行全面比较的研究，评估了它们在三个多样化数据集上的表现：交通标志、非洲野生动物和船只及船舶。这些数据集经过精心挑选，包含了广泛的物体属性，包括不同的物体大小、宽高比和物体密度。我们通过检查精度、召回率、平均精度均值（mAP）、处理时间、GFLOPs 计数和模型大小等一系列指标，展示了每个 YOLO 版本和家族的优势和劣势。我们的研究解决了以下关键研究问题：</p><p>● 哪个 YOLO 算法在一系列综合指标上展示了卓越的性能？</p><p>● 不同的 YOLO 版本在具有不同物体特征（如大小、宽高比和密度）的数据集上的表现如何？</p><p>● 每个 YOLO 版本的具体优势和局限性是什么，这些见解如何指导选择最适合各种应用的算法？</p><p>特别是，YOLO11 系列作为最一致的表现在各个指标上脱颖而出，YOLO11m 在准确性、效率、模型大小之间取得了最佳平衡。虽然 YOLOv10 的准确性略低于 YOLO11，但它在速度和效率方面表现出色，使其成为需要效率和快速处理的应用的强有力选择。此外，YOLOv9 总体上也表现良好，特别是在较小的数据集上表现尤为突出。这些发现为工业界和学术界提供了宝贵的见解，指导选择最适合的 YOLO 算法，并为未来的发展和改进提供信息。虽然评估的算法展示了有希望的性能，但仍有一些改进的空间。未来的研究可以专注于优化 YOLOv10，以提高其准确性，同时保持其速度和效率优势。此外，架构设计的持续进步可能为更突破性的 YOLO 算法铺平道路。我们未来的工作包括深入研究这些算法中确定的差距，并提出改进措施，以展示它们对整体效率的潜在影响。</p><h1 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h1><ul><li><strong>在交通标志数据集上，YOLOv5ul和YOLOv10n的性能差异是什么？原因是什么？</strong></li></ul><p>在交通标志数据集上，YOLOv5ul的mAP50-95达到了0.799，而YOLOv10n的mAP50-95仅为0.64，相差显著。YOLOv5ul在精度和召回率上都表现更好，具体来说，YOLOv5ul的Precision为0.866，Recall为0.849，而YOLOv10n的Precision为0.722，Recall为0.602。这种差异的原因可能包括：</p><ol><li><strong>模型架构改进</strong>：YOLOv5ul采用了更先进的CSPDarknet53作为主干网络，并引入了Spatial Pyramid Pooling Fast（SPPF）模块，这些改进提高了模型的特征提取能力和多尺度适应性。</li><li><strong>数据增强</strong>：YOLOv5ul使用了多种数据增强技术，如Mosaic、Copy-Paste、Random Affine等，这些技术有助于提高模型的泛化能力和鲁棒性。</li><li><strong>优化策略</strong>：YOLOv5ul在训练过程中使用了更有效的优化策略，如AdamW优化器和学习率调度，这些策略有助于模型更快地收敛和提高性能。</li></ol><ul><li><strong>在船舶与船只数据集上，YOLOv11x为什么表现最好？与其他模型相比有哪些优势？</strong></li></ul><p>​在船舶与船只数据集上，YOLOv11x的mAP50-95达到了0.327，表现最好。与其他模型相比，YOLOv11x有以下优势：</p><ol><li><strong>小对象检测</strong>：YOLOv11x特别适用于检测小对象，能够在复杂的图像环境中准确识别和定位小尺寸的船只。</li><li><strong>架构改进</strong>：YOLOv11x引入了C3k2块和C2PSA（Cross-Stage Partial with Self-Attention）模块，这些改进提高了模型的空间注意力和特征提取能力，特别是在处理重叠和小的对象时表现优异。</li><li><strong>计算效率</strong>：尽管YOLOv11x在精度上有所提升，但其推理时间仍然保持在2.4ms左右，保持了较高的计算效率，适合实时应用。</li></ol><ul><li><p><strong>在非洲野生动物数据集上，YOLOv9s的表现优于其他模型的原因是什么？</strong></p><p>在非洲野生动物数据集上，YOLOv9s的mAP50-95达到了0.832，表现优于其他模型。YOLOv9s之所以表现优异，主要原因包括：</p><ol><li><strong>小型数据集适应性</strong>：YOLOv9s在小型数据集上表现出色，能够有效学习对象的模式。其小型模型（如YOLOv9t）在非洲野生动物数据集上表现尤为突出，mAP50-95为0.832，mAP50为0.956。</li><li><strong>特征提取能力</strong>：YOLOv9s采用了CSPNet（Cross-Stage Partial Network），这种结构通过分割特征图来提高特征提取效率，减少了计算复杂度。</li><li><strong>正则化技术</strong>：YOLOv9s使用了GelAN（Gradient Enhanced Lightweight Architecture Network），这种技术通过优化网络内的计算路径，提高了参数利用率和计算效率。</li></ol></li></ul><p><a href="https://arxiv.org/html/2411.00201v1">Evaluating the Evolution of YOLO (You Only Look Once) Models: A Comprehensive Benchmark Study of YOLO11 and Its Predecessors</a></p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="二维码"></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>computer-vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>yolo</tag>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>transformers自定义数据集</title>
    <link href="/2024/12/07/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/nlp/NLP025-huggingface%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD%E5%AD%98%E5%82%A8%E7%9B%AE%E5%BD%95/"/>
    <url>/2024/12/07/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/nlp/NLP025-huggingface%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD%E5%AD%98%E5%82%A8%E7%9B%AE%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>huggingface的transformers和dataset等库都默认把模型或者数据集下载保存到~&#x2F;.cache目录下，但是模型或者数据集通常都比较大，占用home目录空间，这里就讲下如何设置数据集或者模型的下载保存的目录。</p><p>以datasets库数据集下载为例。datasets数据集下载的相关配置在datasets.config.py模块中，如下图：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20231031165446512.png" alt="image-20231031165446512"></p><p>变量DEFAULT_XDG_CACHE_HOME&#x3D;”<del>&#x2F;.cache”,如果没有设置环境变量”XDG_CACHE_HOME”，则数据集默认保存在”</del>&#x2F;.cache”目录下，因此，可以通过设置环境变量的方式修改数据集下载目录，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Descripttion: chengbo&#x27;s code</span><br><span class="hljs-string">version: 1.0.0</span><br><span class="hljs-string">Author: chengbo</span><br><span class="hljs-string">Date: 2023-10-30 12:06:23</span><br><span class="hljs-string">LastEditors: chengbo</span><br><span class="hljs-string">LastEditTime: 2023-10-30 12:06:30</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment">#修改环境变量要在导入datasets或者transformers模块之前</span><br>os.environ[<span class="hljs-string">&quot;XDG_CACHE_HOME&quot;</span>] = <span class="hljs-string">&quot;/data/.cache&quot;</span>  <br><span class="hljs-comment"># os.environ[&quot;HF_CACHE_HOME&quot;] = &quot;/data/huggingface&quot;</span><br><span class="hljs-comment"># os.environ[&quot;HF_DATASETS_CACHE&quot;] = &quot;/data/huggingface/datasets&quot;</span><br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, DownloadConfig<br><span class="hljs-keyword">import</span> datasets<br><br><br>dataset = load_dataset(<br>    <span class="hljs-string">&quot;Graphcore/vqa&quot;</span>,<br>    download_config=DownloadConfig(resume_download=<span class="hljs-literal">True</span>),<br>    split=<span class="hljs-string">&quot;validation[:200]&quot;</span>,<br>)<br><span class="hljs-built_in">print</span>(dataset[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(os.getenv(<span class="hljs-string">&quot;XDG_CACHE_HOME&quot;</span>))<br><span class="hljs-built_in">print</span>(os.getenv(<span class="hljs-string">&quot;HF_HOME&quot;</span>))<br><span class="hljs-built_in">print</span>(datasets.config.HF_CACHE_HOME)<br><span class="hljs-built_in">print</span>(datasets.config.HF_DATASETS_CACHE)<br><span class="hljs-built_in">print</span>(datasets.config.EXTRACTED_DATASETS_PATH)<br><br></code></pre></td></tr></table></figure><p>通过os.environ[“XDG_CACHE_HOME”] &#x3D; “&#x2F;data&#x2F;.cache” 代码，后续的模型和数据集都会下载到该目录下。</p><p>另外一种修改模型或者数据集下载目录的方式是通过参数配置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">load_dataset(<span class="hljs-string">&quot;Graphcore/vqa&quot;</span>,cache_dir=<span class="hljs-string">&quot;/data/.cache&quot;</span>)<br></code></pre></td></tr></table></figure><p>但是这种方式有个缺点，有些数据集下载的是个压缩文件，如.zip,下载后会进行自动解压，解压的默认目录还是在”~&#x2F;.cache”目录下。从上面的图片中可以看到有个EXTRACTED_DATASETS_PATH 参数，这个参数就是设置解压的目录，可以通过修改该参数来配置解压后保存的目录。</p><p>其它参数也可以参考上图config.py模块中的代码进行设置。</p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>huggingface</tag>
      
      <tag>transformers</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>音视频开发01：RGB和YUV颜色模型详解</title>
    <link href="/2024/12/05/%E5%BC%80%E5%8F%91/%E9%9F%B3%E8%A7%86%E9%A2%91/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%9101%EF%BC%9ARGB%E5%92%8CYUV%E9%A2%9C%E8%89%B2%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/"/>
    <url>/2024/12/05/%E5%BC%80%E5%8F%91/%E9%9F%B3%E8%A7%86%E9%A2%91/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%9101%EF%BC%9ARGB%E5%92%8CYUV%E9%A2%9C%E8%89%B2%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="1-RGB颜色模型"><a href="#1-RGB颜色模型" class="headerlink" title="1. RGB颜色模型"></a>1. RGB颜色模型</h1><p>RGB（Red, Green, Blue）颜色模型是一种加色模型，通过混合不同强度的红、绿、蓝三种基本颜色来生成各种颜色。RGB模型广泛应用于显示器、电视、摄像机等设备中。</p><h3 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1 基本概念"></a>1.1 基本概念</h3><ul><li><strong>红色（Red）</strong>：表示红色光的强度。</li><li><strong>绿色（Green）</strong>：表示绿色光的强度。</li><li><strong>蓝色（Blue）</strong>：表示蓝色光的强度。</li></ul><h3 id="1-2-颜色表示"><a href="#1-2-颜色表示" class="headerlink" title="1.2 颜色表示"></a>1.2 颜色表示</h3><ul><li>每个颜色通道（R、G、B）通常使用8位（0-255）或16位（0-65535）来表示强度。</li><li>例如，纯红色表示为 <code>(255, 0, 0)</code>，纯绿色为 <code>(0, 255, 0)</code>，纯蓝色为 <code>(0, 0, 255)</code>。</li><li>白色为 <code>(255, 255, 255)</code>，黑色为 <code>(0, 0, 0)</code>。</li></ul><h3 id="1-3-优点"><a href="#1-3-优点" class="headerlink" title="1.3 优点"></a>1.3 优点</h3><ul><li><strong>直观</strong>：RGB模型直接对应显示器的发光原理，易于理解和实现。</li><li><strong>广泛应用</strong>：适用于大多数显示设备和图像处理软件。</li></ul><h3 id="1-4-缺点"><a href="#1-4-缺点" class="headerlink" title="1.4 缺点"></a>1.4 缺点</h3><ul><li><strong>空间不均匀</strong>：人眼对不同颜色的敏感度不同，RGB模型在颜色空间上不够均匀。</li><li><strong>不适合传输</strong>：RGB数据量大，不适合直接用于视频传输和存储。</li></ul><h1 id="2-YUV颜色模型"><a href="#2-YUV颜色模型" class="headerlink" title="2.YUV颜色模型"></a>2.YUV颜色模型</h1><p>YUV颜色模型是一种用于视频压缩和传输的颜色模型，它将亮度（Y）和色度（U、V）分开表示。YUV模型广泛应用于电视广播、视频编码等领域。</p><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><ul><li><strong>Y（Luminance）</strong>：表示亮度信息，即图像的灰度值。</li><li><strong>U（Chrominance）</strong>：表示蓝色和亮度之间的差异。</li><li><strong>V（Chrominance）</strong>：表示红色和亮度之间的差异。</li></ul><h3 id="2-2-颜色表示"><a href="#2-2-颜色表示" class="headerlink" title="2.2  颜色表示"></a>2.2  颜色表示</h3><ul><li>YUV模型中，Y通常使用8位（0-255）表示亮度。</li><li>U和V通常使用8位（-128到127）表示色度。</li><li>例如，YUV值 <code>(128, 0, 0)</code> 表示中等亮度的灰色，没有颜色偏差。</li></ul><h3 id="2-3-优点"><a href="#2-3-优点" class="headerlink" title="2.3 优点"></a>2.3 优点</h3><ul><li><strong>压缩效率高</strong>：YUV模型将亮度和色度分开，可以进行色度子采样（如4:2:0、4:2:2），减少数据量。</li><li><strong>适合传输</strong>：YUV模型适合用于视频压缩和传输，如MPEG、H.264等标准。</li></ul><h3 id="2-4-缺点"><a href="#2-4-缺点" class="headerlink" title="2.4 缺点"></a>2.4 缺点</h3><ul><li><strong>不直观</strong>：YUV模型不如RGB模型直观，需要转换才能在显示器上显示。</li><li><strong>转换复杂</strong>：从RGB到YUV的转换涉及复杂的数学运算。</li></ul><h1 id="3-YUV相比RGB的优势"><a href="#3-YUV相比RGB的优势" class="headerlink" title="3. YUV相比RGB的优势"></a>3. YUV相比RGB的优势</h1><p>YUV颜色模型相对于RGB颜色模型具有以下几个显著的优势，特别是在视频处理和传输领域：</p><h3 id="3-1-压缩效率高"><a href="#3-1-压缩效率高" class="headerlink" title="3.1 压缩效率高"></a>3.1 压缩效率高</h3><ul><li><strong>色度子采样</strong>：YUV模型将亮度和色度信息分开表示，可以对色度信息进行子采样（如4:2:0、4:2:2），从而减少数据量。人眼对亮度的变化更为敏感，对色度的变化相对不敏感，因此可以减少色度信息的采样率而不显著影响视觉质量。</li><li><strong>数据量减少</strong>：通过色度子采样，YUV模型可以显著减少视频数据的存储和传输需求，提高压缩效率。</li></ul><h3 id="3-2-适合视频传输"><a href="#3-2-适合视频传输" class="headerlink" title="3.2 适合视频传输"></a>3.2 适合视频传输</h3><ul><li><strong>带宽优化</strong>：在视频传输中，YUV模型通过减少色度信息的带宽占用，可以更有效地利用有限的带宽资源。</li><li><strong>兼容性</strong>：YUV模型广泛应用于电视广播和视频编码标准（如MPEG、H.264等），具有良好的兼容性和广泛的应用基础。</li></ul><h3 id="3-3-视觉感知优化"><a href="#3-3-视觉感知优化" class="headerlink" title="3.3 视觉感知优化"></a>3.3 视觉感知优化</h3><ul><li><strong>亮度优先</strong>：YUV模型将亮度信息（Y）单独表示，更符合人眼对亮度的敏感性。在视频处理中，可以优先保证亮度信息的准确性，从而提高视觉质量。</li><li><strong>色度调整</strong>：通过调整色度信息（U、V），可以在不影响亮度的情况下进行颜色校正和调整，更适合视频后期处理。</li></ul><h3 id="3-4-历史和标准支持"><a href="#3-4-历史和标准支持" class="headerlink" title="3.4 历史和标准支持"></a>3.4 历史和标准支持</h3><ul><li><strong>历史悠久</strong>：YUV模型起源于模拟电视系统（如PAL和NTSC），经过多年的发展和优化，已经成为视频处理和传输的标准模型。</li><li><strong>标准支持</strong>：YUV模型被广泛应用于各种视频编码标准和传输协议中，具有强大的标准支持和生态系统。</li></ul><h3 id="3-5-计算复杂度"><a href="#3-5-计算复杂度" class="headerlink" title="3.5 计算复杂度"></a>3.5 计算复杂度</h3><ul><li><strong>转换简单</strong>：虽然从RGB到YUV的转换涉及一定的数学运算，但在视频处理中，这种转换通常是批量进行的，计算复杂度相对可控。</li><li><strong>硬件支持</strong>：现代视频处理硬件（如GPU、专用视频处理器）通常支持YUV模型的处理和转换，具有较高的计算效率。</li></ul><h3 id="3-6-总结"><a href="#3-6-总结" class="headerlink" title="3.6 总结"></a>3.6 总结</h3><p>YUV模型在视频处理和传输中具有显著的优势，特别是在压缩效率、带宽优化、视觉感知优化和标准支持方面。尽管RGB模型在显示器和图像处理中更为直观和直接，但在视频领域，YUV模型因其高效的数据压缩和传输特性而成为主流选择。</p><h1 id="4-RGB与YUV的转换"><a href="#4-RGB与YUV的转换" class="headerlink" title="4. RGB与YUV的转换"></a>4. RGB与YUV的转换</h1><h3 id="4-1-RGB到YUV的转换公式"><a href="#4-1-RGB到YUV的转换公式" class="headerlink" title="4.1 RGB到YUV的转换公式"></a>4.1 RGB到YUV的转换公式</h3><ul><li><strong>Y</strong> &#x3D; 0.299R + 0.587G + 0.114B</li><li><strong>U</strong> &#x3D; 0.492(B - Y)</li><li><strong>V</strong> &#x3D; 0.877(R - Y)</li></ul><h3 id="4-2-YUV到RGB的转换公式"><a href="#4-2-YUV到RGB的转换公式" class="headerlink" title="4.2 YUV到RGB的转换公式"></a>4.2 YUV到RGB的转换公式</h3><ul><li><strong>R</strong> &#x3D; Y + 1.140V</li><li><strong>G</strong> &#x3D; Y - 0.395U - 0.581V</li><li><strong>B</strong> &#x3D; Y + 2.032U</li></ul><h3 id="4-3-总结"><a href="#4-3-总结" class="headerlink" title="4.3 总结"></a>4.3 总结</h3><ul><li><strong>RGB模型</strong>：适用于显示器和图像处理，直观但不适合传输。</li><li><strong>YUV模型</strong>：适用于视频压缩和传输，压缩效率高但不够直观。</li></ul><p>在实际应用中，通常根据需求选择合适的颜色模型，并在不同模型之间进行转换。</p><h1 id="5-YUV的采样方式"><a href="#5-YUV的采样方式" class="headerlink" title="5. YUV的采样方式"></a>5. YUV的采样方式</h1><p>下面我们通过具体的例子来解释YUV的各种采样方式，以便更好地理解它们的工作原理和效果。</p><h3 id="5-1-4-4-4-采样"><a href="#5-1-4-4-4-采样" class="headerlink" title="5.1  4:4:4 采样"></a>5.1  4:4:4 采样</h3><p><strong>例子</strong>：假设有一个4x2像素的图像，每个像素的Y、U、V分量都进行采样。</p><table><thead><tr><th>像素</th><th>Y</th><th>U</th><th>V</th></tr></thead><tbody><tr><td>1</td><td>Y1</td><td>U1</td><td>V1</td></tr><tr><td>2</td><td>Y2</td><td>U2</td><td>V2</td></tr><tr><td>3</td><td>Y3</td><td>U3</td><td>V3</td></tr><tr><td>4</td><td>Y4</td><td>U4</td><td>V4</td></tr><tr><td>5</td><td>Y5</td><td>U5</td><td>V5</td></tr><tr><td>6</td><td>Y6</td><td>U6</td><td>V6</td></tr><tr><td>7</td><td>Y7</td><td>U7</td><td>V7</td></tr><tr><td>8</td><td>Y8</td><td>U8</td><td>V8</td></tr></tbody></table><p><strong>解释</strong>：每个像素都有完整的Y、U、V信息，没有进行任何压缩。这种采样方式提供了最高质量的图像，但数据量最大。</p><h3 id="5-2-4-2-2-采样"><a href="#5-2-4-2-2-采样" class="headerlink" title="5.2  4:2:2 采样"></a>5.2  4:2:2 采样</h3><p><strong>例子</strong>：假设有一个4x2像素的图像，每两个像素共享一个U和V分量。</p><table><thead><tr><th>像素</th><th>Y</th><th>U</th><th>V</th></tr></thead><tbody><tr><td>1</td><td>Y1</td><td>U1</td><td></td></tr><tr><td>2</td><td>Y2</td><td></td><td>V2</td></tr><tr><td>3</td><td>Y3</td><td>U3</td><td></td></tr><tr><td>4</td><td>Y4</td><td></td><td>V4</td></tr><tr><td>5</td><td>Y5</td><td>U5</td><td></td></tr><tr><td>6</td><td>Y6</td><td></td><td>V6</td></tr><tr><td>7</td><td>Y7</td><td>U7</td><td></td></tr><tr><td>8</td><td>Y8</td><td></td><td>V8</td></tr></tbody></table><p><strong>解释</strong>：每两个像素中，第一个像素采样Y、U，第二个像素采样Y、V。这种采样方式减少了U和V分量的数据量，但仍然保留了较高的图像质量。</p><h3 id="5-3-4-2-0-采样"><a href="#5-3-4-2-0-采样" class="headerlink" title="5.3  4:2:0 采样"></a>5.3  4:2:0 采样</h3><p><strong>例子</strong>：假设有一个4x2像素的图像，每四个像素共享一个U和V分量。</p><table><thead><tr><th>像素</th><th>Y</th><th>U</th><th>V</th></tr></thead><tbody><tr><td>1</td><td>Y1</td><td>U1</td><td></td></tr><tr><td>2</td><td>Y2</td><td></td><td></td></tr><tr><td>3</td><td>Y3</td><td></td><td>V3</td></tr><tr><td>4</td><td>Y4</td><td></td><td></td></tr><tr><td>5</td><td>Y5</td><td>U5</td><td></td></tr><tr><td>6</td><td>Y6</td><td></td><td></td></tr><tr><td>7</td><td>Y7</td><td></td><td>V7</td></tr><tr><td>8</td><td>Y8</td><td></td><td></td></tr></tbody></table><p><strong>解释</strong>：每四个像素中，第一个像素采样Y、U，第三个像素采样Y、V，其他像素只采样Y。这种采样方式显著减少了U和V分量的数据量，适用于对数据量要求较高的场景，如数字电视、DVD和流媒体。</p><h3 id="5-4-4-1-1-采样"><a href="#5-4-4-1-1-采样" class="headerlink" title="5.4  4:1:1 采样"></a>5.4  4:1:1 采样</h3><p><strong>例子</strong>：假设有一个4x2像素的图像，每四个像素共享一个U和V分量。</p><table><thead><tr><th>像素</th><th>Y</th><th>U</th><th>V</th></tr></thead><tbody><tr><td>1</td><td>Y1</td><td>U1</td><td></td></tr><tr><td>2</td><td>Y2</td><td></td><td></td></tr><tr><td>3</td><td>Y3</td><td></td><td>V3</td></tr><tr><td>4</td><td>Y4</td><td></td><td></td></tr><tr><td>5</td><td>Y5</td><td>U5</td><td></td></tr><tr><td>6</td><td>Y6</td><td></td><td></td></tr><tr><td>7</td><td>Y7</td><td></td><td>V7</td></tr><tr><td>8</td><td>Y8</td><td></td><td></td></tr></tbody></table><p><strong>解释</strong>：每四个像素中，第一个像素采样Y、U，第三个像素采样Y、V，其他像素只采样Y。这种采样方式与4:2:0类似，但较少使用，主要在一些旧的视频编码标准中。</p><h3 id="5-5-总结"><a href="#5-5-总结" class="headerlink" title="5.5 总结"></a>5.5 总结</h3><p>通过这些例子，我们可以看到不同YUV采样方式对数据量的影响和图像质量的权衡。4:4:4采样提供最高质量，但数据量最大；4:2:2采样在质量和数据量之间取得平衡；4:2:0采样在保证视觉质量的前提下，最大限度地减少数据量，广泛应用于数字电视、DVD和流媒体等领域。</p><h1 id="6-举例说明YUV和RGB的存储差异"><a href="#6-举例说明YUV和RGB的存储差异" class="headerlink" title="6.举例说明YUV和RGB的存储差异"></a>6.举例说明YUV和RGB的存储差异</h1><p>我们来比较一下RGB和YUV各个采样方式在存储方面的差异。假设我们有一个4x2像素的图像，每个像素的RGB值用8位表示（0-255）。</p><h3 id="6-1-RGB存储"><a href="#6-1-RGB存储" class="headerlink" title="6.1 RGB存储"></a>6.1 RGB存储</h3><p>每个像素有3个分量（R、G、B），每个分量占用8位。</p><p><strong>总存储量</strong>：</p><ul><li>每个像素：3字节（24位）</li><li>4x2像素图像：4 * 2 * 3 &#x3D; 24字节</li></ul><h3 id="6-2-YUV存储"><a href="#6-2-YUV存储" class="headerlink" title="6.2 YUV存储"></a>6.2 YUV存储</h3><h4 id="6-2-1-YUV-4-4-4-采样"><a href="#6-2-1-YUV-4-4-4-采样" class="headerlink" title="6.2.1 YUV 4:4:4 采样"></a>6.2.1 YUV 4:4:4 采样</h4><p>每个像素有3个分量（Y、U、V），每个分量占用8位。</p><p><strong>总存储量</strong>：</p><ul><li>每个像素：3字节（24位）</li><li>4x2像素图像：4 * 2 * 3 &#x3D; 24字节</li></ul><p><strong>比较</strong>：</p><ul><li>RGB和YUV 4:4:4采样的存储量相同，都是24字节。</li></ul><h4 id="6-2-2-YUV-4-2-2-采样"><a href="#6-2-2-YUV-4-2-2-采样" class="headerlink" title="6.2.2 YUV 4:2:2 采样"></a>6.2.2 YUV 4:2:2 采样</h4><p>每两个像素共享一个U和V分量。</p><p><strong>总存储量</strong>：</p><ul><li>Y分量：4 * 2 &#x3D; 8字节</li><li>U分量：4 * 2 &#x2F; 2 &#x3D; 4字节</li><li>V分量：4 * 2 &#x2F; 2 &#x3D; 4字节</li><li>总存储量：8 + 4 + 4 &#x3D; 16字节</li></ul><p><strong>比较</strong>：</p><ul><li>YUV 4:2:2采样的存储量比RGB少8字节（16字节 vs 24字节）。</li></ul><h4 id="6-2-3-YUV-4-2-0-采样"><a href="#6-2-3-YUV-4-2-0-采样" class="headerlink" title="6.2.3 YUV 4:2:0 采样"></a>6.2.3 YUV 4:2:0 采样</h4><p>每四个像素共享一个U和V分量。</p><p><strong>总存储量</strong>：</p><ul><li>Y分量：4 * 2 &#x3D; 8字节</li><li>U分量：4 * 2 &#x2F; 4 &#x3D; 2字节</li><li>V分量：4 * 2 &#x2F; 4 &#x3D; 2字节</li><li>总存储量：8 + 2 + 2 &#x3D; 12字节</li></ul><p><strong>比较</strong>：</p><ul><li>YUV 4:2:0采样的存储量比RGB少12字节（12字节 vs 24字节）。</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><table><thead><tr><th>采样方式</th><th>存储量（字节）</th><th>与RGB相比的差异</th></tr></thead><tbody><tr><td>RGB</td><td>24</td><td>0</td></tr><tr><td>YUV 4:4:4</td><td>24</td><td>0</td></tr><tr><td>YUV 4:2:2</td><td>16</td><td>-8</td></tr><tr><td>YUV 4:2:0</td><td>12</td><td>-12</td></tr></tbody></table><p>通过这个例子，我们可以清楚地看到YUV采样方式在存储方面的优势。YUV 4:4:4采样与RGB存储量相同，而YUV 4:2:2和YUV 4:2:0采样分别比RGB减少了8字节和12字节的存储量。这种存储量的减少在处理大尺寸图像或视频时尤为显著，有助于提高数据传输和存储的效率。</p><p>文章合集：<a href="https://github.com/chongzicbo/ReadWriteThink/tree/main">chongzicbo&#x2F;ReadWriteThink: 博学而笃志，切问而近思 (github.com)</a></p><p>个人博客：<a href="https://chongzicbo.github.io/">程博仕</a></p><p>微信公众号：</p><p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="微信公众号"></p>]]></content>
    
    
    <categories>
      
      <category>开发</category>
      
      <category>音视频</category>
      
      <category>基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>音视频开发</tag>
      
      <tag>音视频基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
