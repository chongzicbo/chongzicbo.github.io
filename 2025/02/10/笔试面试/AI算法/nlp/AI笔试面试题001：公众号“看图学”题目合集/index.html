

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/daxiong.jpg">
  <link rel="icon" href="/img/daxiong.jpg">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="程博">
  <meta name="keywords" content="">
  
    <meta name="description" content="1.如何让大模型输出合法的Json格式后处理最容易想到的当然是重试机制，在 Prompt 中要求 LLM 输出 json，拿到 LLM 的完整输出，判断是否是合法的 json。如果不是，则再重新生成一遍。 当然这里也有优化空间，比如可以通过 json parser 来判断解析到哪里出错了，重试的时候不需要从头输出了，而只需要从出错的地方往后输出即可。 比如 strict-json 库就采用的这种方">
<meta property="og:type" content="article">
<meta property="og:title" content="“看图学”试题合集">
<meta property="og:url" content="https://chongzicbo.github.io/2025/02/10/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/AI%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98001%EF%BC%9A%E5%85%AC%E4%BC%97%E5%8F%B7%E2%80%9C%E7%9C%8B%E5%9B%BE%E5%AD%A6%E2%80%9D%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86/index.html">
<meta property="og:site_name" content="程博仕">
<meta property="og:description" content="1.如何让大模型输出合法的Json格式后处理最容易想到的当然是重试机制，在 Prompt 中要求 LLM 输出 json，拿到 LLM 的完整输出，判断是否是合法的 json。如果不是，则再重新生成一遍。 当然这里也有优化空间，比如可以通过 json parser 来判断解析到哪里出错了，重试的时候不需要从头输出了，而只需要从出错的地方往后输出即可。 比如 strict-json 库就采用的这种方">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739151842011-3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739153523841-6.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210101113070.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210110458915.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210110526113.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739156742913-9.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739158268947-12.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210113748910.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739158723357-15.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739158832564-18.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739158866210-21.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739159014703-24.png">
<meta property="article:published_time" content="2025-02-10T04:00:00.000Z">
<meta property="article:modified_time" content="2025-02-10T03:56:43.444Z">
<meta property="article:author" content="程博">
<meta property="article:tag" content="nlp">
<meta property="article:tag" content="笔试面试">
<meta property="article:tag" content="算法面试">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>“看图学”试题合集 - 程博仕</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"chongzicbo.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"left","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":false,"baidu":"3841b375bfdd5627f0f840e1e289416e","google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?3841b375bfdd5627f0f840e1e289416e";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  

  

  

  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>程博仕</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-category-fill"></i>
                <span>笔试面试</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp" target="_self">
                    
                    <span>自然语言处理</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/computer-vision" target="_self">
                    
                    <span>计算机视觉</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/multi-modal" target="_self">
                    
                    <span>多模态</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/cpp" target="_self">
                    
                    <span>C++</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/java" target="_self">
                    
                    <span>Java</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/python" target="_self">
                    
                    <span>Python</span>
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-category-fill"></i>
                <span>计算机基础</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F" target="_self">
                    
                    <span>操作系统</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C" target="_self">
                    
                    <span>计算机网络</span>
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-category-fill"></i>
                <span>人工智能</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/nlp" target="_self">
                    
                    <span>自然语言处理</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/computer-vision" target="_self">
                    
                    <span>计算机视觉</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/multi-modal" target="_self">
                    
                    <span>多模态</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/asr" target="_self">
                    
                    <span>语音识别</span>
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-category-fill"></i>
                <span>开发</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E5%BC%80%E5%8F%91/%E9%9F%B3%E8%A7%86%E9%A2%91" target="_self">
                    
                    <span>音视频</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E5%BC%80%E5%8F%91/web&#39;" target="_self">
                    
                    <span>Web开发</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E5%BC%80%E5%8F%91/cpp" target="_self">
                    
                    <span>C++</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E5%BC%80%E5%8F%91/java" target="_self">
                    
                    <span>Java</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/%E5%BC%80%E5%8F%91/python" target="_self">
                    
                    <span>Python</span>
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>文章分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>时间轴</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="“看图学”试题合集"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        程博
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-02-10 12:00" pubdate>
          February 10, 2025 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          6.6k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          55 mins
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="padding-left: 2rem; margin-right: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">“看图学”试题合集</h1>
            
              <p id="updated-time" class="note note-info" style="">
                
                  
                    Last updated on February 10, 2025 am
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <h1 id="1-如何让大模型输出合法的Json格式"><a href="#1-如何让大模型输出合法的Json格式" class="headerlink" title="1.如何让大模型输出合法的Json格式"></a>1.如何让大模型输出合法的Json格式</h1><h3 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a><strong>后处理</strong></h3><p>最容易想到的当然是重试机制，在 Prompt 中要求 LLM 输出 json，拿到 LLM 的完整输出，判断是否是合法的 json。如果不是，则再重新生成一遍。</p>
<p>当然这里也有优化空间，比如可以通过 json parser 来判断解析到哪里出错了，重试的时候不需要从头输出了，而只需要从出错的地方往后输出即可。</p>
<p>比如 strict-json 库就采用的这种方法。</p>
<p>这属于后处理的方法，最容易实现，但是逼格似乎不太够，而且有点费钱。</p>
<h3 id="约束解码"><a href="#约束解码" class="headerlink" title="约束解码"></a><strong>约束解码</strong></h3><p>另一种复杂一点的方法就是约束解码，保证一次性生成合法的 json。</p>
<p>约束解码就是在生成下一个 token 的时候，对词表进行一次筛选，将词表符合约束的 token 留下，其他的 token 都 mask 掉，这样 LLM 在 generate 的时候，每一个时刻的 response 都是符合实现约束的条件的。</p>
<p>举一个例子来说，我们定义了输出的格式为 json，那么第一个字符就必须是 <code>&#123;</code> 符号，词表中其他的 token 都被 mask 掉了。</p>
<p>那第二个字符合法的有, 引号<code>&quot;</code>,空白字符还有 <code>&#125;</code>，其余的符号都被 mask 掉了。剩余的步骤也一样，直到生成完整的 json 就停止。</p>
<p>上面说的是比较简单的情况，有时候还需要约束 json 符合固定的格式，比如第一个 key 是 name，第二个 key 是 age 之类的。这个时候就要采用一些 Grammar Engine 来负责这些 schema 的处理。</p>
<p>其流程如下：</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640" srcset="/img/loading.gif" lazyload alt="约束解码"></p>
<p>示意代码如下（暂不考虑 kvcache）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">constrained_decode</span>(<span class="hljs-params">input_x, constraint_fn C, max_length</span>):<br>    <span class="hljs-comment"># 刚开始输出为空</span><br>    output = []<br><br>    <span class="hljs-comment"># 开始生成</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(output) &lt; max_length:<br>        <span class="hljs-comment"># C 为 Grammar Engine，解析当前的输出</span><br>        C.update(output)<br><br>        <span class="hljs-comment"># 计算 mask</span><br>        mask = C.mask()<br><br>        <span class="hljs-comment"># 下一个 token 的 logits</span><br>        logits = model_forward(input_x + output)<br><br>        <span class="hljs-comment"># 将不符合约束的 token mask 掉</span><br>        masked_logits = logits * mask<br><br>        <span class="hljs-comment"># 剩下的和正常的解码一样</span><br>        probs = softmax(masked_logits)<br>        next_token = sample_token(probs)<br><br>        output.append(next_token)<br><br>        <span class="hljs-keyword">if</span> next_token == EOS_TOKEN:<br>            <span class="hljs-keyword">break</span><br><br>    <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>

<p>通过上面图示和代码可以很清晰的看到约束生成的大概框架。</p>
<p>目前已经有很多框架支持约束解码，比如 Guidance (Guidance AI, 2023), Outlines (Willard &amp; Louf, 2023), XGrammar (Dong et al., 2024) and the grammar module of Llamacpp (Gerganov &amp; al., 2023)。</p>
<p>这些框架在这个基本框架上进一步做了很多优化，比如 mask 的计算和 LLM forward 的流程是可以并行的，然后 Grammer Engine 的处理通常比较耗时，可以加入缓存来提高重复或者相似的语法结构的处理，也有提前计算 mask 以达到加速的目的，因为 mask 可以在 cpu 上计算，对于算力的占用没有那么紧张。</p>
<p>Grammar Engine 内部的一些优化细节就涉及到编译原理了，会有很多自动机相关的研究，这里就不再展开了。</p>
<p>在论文 《Generating Structured Outputs from Language Models: Benchmark and Studies》 中，作者比较了上面几种常用的框架，结果如下：</p>
<p>- GCT 是语法编译的时间（Grammar Compilation Time）</p>
<p>- TTFT 是第一个 token 产出的时间（Time to First Token）</p>
<p>- TPOT 是产生第一个 token 之后，每个 token 的平均生成时间（Time per Output Token (TPOT)）</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640.png" srcset="/img/loading.gif" lazyload alt="框架比较"></p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739151842011-3.png" srcset="/img/loading.gif" lazyload alt="框架比较"></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/IbYjUlS1gN8vJ5doZpyVbA">Deepseek一面：“如何让大模型输出合法的 Json 格式？”</a></p>
<h1 id="2-FIM-Fill-in-the-Middle-的原理是什么？"><a href="#2-FIM-Fill-in-the-Middle-的原理是什么？" class="headerlink" title="2. FIM (Fill in the Middle) 的原理是什么？"></a>2. FIM (Fill in the Middle) 的原理是什么？</h1><p>假设有这么一个填空题：白日依山尽，_ _ 入海流。</p>
<p>在 Bert 的 Encoder 主导的时代，注意力是双向的，所以可以非常方便的填充中间 mask 的内容。</p>
<p>后来到了 GPT 的 Decoder only 的时代，token 只能从左往右生成，就没法填充中间的内容了。</p>
<p>然而填充中间内容的需求一直是存在的，比如有一段代码，想在函数名和函数体中间生成注释，这个时候按照 GPT 类似的 Decoder 的默认方法，就没填充，或者只能根据函数名进行填充。</p>
<p>OpenAI 的研究人员想了个好办法，在论文 《Efficient Training of Language Models to Fill in the Middle》中，提出了 Fill in the Middle 的方法。方法很简单，但是效果却异常的好。</p>
<p>其思想就是，<strong>既然没法改变 Decoder 的流程，那就改变数据的顺序</strong>。</p>
<p>比如“白日依山尽，黄河入海流”，采用 FIM 的方法进行训练的时候，可以表示成下面这样：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">PRE</span>&gt;</span>白日依山尽，<span class="hljs-tag">&lt;<span class="hljs-name">SUF</span>&gt;</span>入海流<span class="hljs-tag">&lt;<span class="hljs-name">MID</span>&gt;</span>黄河<span class="hljs-tag">&lt;<span class="hljs-name">EOM</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p>这样的话，模型就可以根据 special token 来预测中间的“黄河” 两个字。其中 <code>&lt;Pre&gt;</code> 代表前缀，<code>&lt;SUF&gt;</code> 代表后缀，<code>&lt;MID&gt;</code> 表示中间填充的开始，<code>&lt;EOM&gt;</code> 表示中间填充的结束。</p>
<p>预测的时候只需要输入</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">PRE</span>&gt;</span>白日依山尽，<span class="hljs-tag">&lt;<span class="hljs-name">SUF</span>&gt;</span>入海流<span class="hljs-tag">&lt;<span class="hljs-name">MID</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p>然后模型就会继续往后预测，直到遇到 <code>&lt;EOM&gt;</code> 符号。</p>
<p>OpenAI 同时发现，在预训练中引入这样的机制后，对于原来的模型性能几乎没有影响。</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739153523841-6.png" srcset="/img/loading.gif" lazyload alt="Fill in the Middle"></p>
<p>OpenAI 把这个现象称作 <strong>FIM-for-free property</strong>。</p>
<p>论文发出以后，基本上所有的 Code 相关的模型，都引入了这个机制。所以很多 copilot 调用的模型都具备中间代码补全的能力。</p>
<p>到了今天，基本上所有模型的预训练阶段都会加入 FIM。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/-X1WGsPWnS_dKYDSDzFnkw">百度大模型一面：“FIM (Fill in the Middle) 的原理是什么？有没有用过 Copilot？”</a></p>
<h1 id="3-大模型推理-repetition-penalty-是如何实现的？"><a href="#3-大模型推理-repetition-penalty-是如何实现的？" class="headerlink" title="3.大模型推理 repetition penalty 是如何实现的？"></a>3.大模型推理 repetition penalty 是如何实现的？</h1><p>大模型推理的时候，一般都有一个 <code>repetition_penalty</code> 的参数，看字面意思可以知道是加入了重复惩罚。但是具体是怎么实现的呢？</p>
<p>这个方法最早来源于 CTRL: A Conditional Transformer Language Model For Controllable Generation。</p>
<p>在 Sampling 那一章，作者提出了这么一个思路:</p>
<p>既然不想生成重复的内容，那就在预测下一个 token 的时候，<strong>让已经出现的 token 的预测概率变小就可以了。</strong></p>
<p>用公式表达如下：</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210101113070.png" srcset="/img/loading.gif" lazyload alt="公式"></p>
<p>其中  I(c) &#x3D;θ if c is True else 1， g 则是已经出现的 token。</p>
<p>θ 就是 repetition penalty 的大小。</p>
<p>也就是说，对于出现的 token，会让其 logits 变小。极限一点的话，可以让 input_ids 中的 token 再也没法出现。</p>
<p>具体实现也不难，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RepetitionPenaltyLogitsProcessor</span>(<span class="hljs-title class_ inherited__">LogitsProcessor</span>):<br>    <span class="hljs-string">r&quot;&quot;&quot;</span><br><span class="hljs-string">    [`LogitsProcessor`] enforcing an exponential penalty on repeated sequences.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        repetition_penalty (`float`):</span><br><span class="hljs-string">            The parameter for repetition penalty. 1.0 means no penalty. See [this</span><br><span class="hljs-string">            paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, penalty: <span class="hljs-built_in">float</span></span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(penalty, <span class="hljs-built_in">float</span>) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> (penalty &gt; <span class="hljs-number">0</span>):<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;`penalty` has to be a strictly positive float, but is <span class="hljs-subst">&#123;penalty&#125;</span>&quot;</span>)<br><br>        <span class="hljs-variable language_">self</span>.penalty = penalty<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, input_ids: torch.LongTensor, scores: torch.FloatTensor</span>) -&gt; torch.FloatTensor:<br>        score = torch.gather(scores, <span class="hljs-number">1</span>, input_ids)<br><br>        <span class="hljs-comment"># if score &lt; 0 then repetition penalty has to be multiplied to reduce the previous token probability</span><br>        score = torch.where(score &lt; <span class="hljs-number">0</span>, score * <span class="hljs-variable language_">self</span>.penalty, score / <span class="hljs-variable language_">self</span>.penalty)<br><br>        scores.scatter_(<span class="hljs-number">1</span>, input_ids, score)<br>        <span class="hljs-keyword">return</span> scores<br></code></pre></td></tr></table></figure>

<p>从代码可以清晰的看到，首先使用 <code>torch.gather(scores, 1, input_ids)</code> 获取到 input_ids 中的 logits，然后修改其 logits 的数值。</p>
<p>这里有个细节就是对于 小于 0 的数，是乘以 self.penalty, 否则是除以 self.penalty， 这样就让 input_ids 的 logits 统一都变小了，从而达到了避免重复的问题。</p>
<p>其他的参数比如 <code>no_repeat_ngram_size</code>，可以控制不重复的 ngram 长度，比如 <code>no_repeat_ngram_size = 3</code>， 就不会生成包含重复的 3-gram的序列。</p>
<p>但是需要注意，一旦总是出现那种不太正常的重复，比如无论怎么调节generate 的参数，输出总是一个词，这种情况大概率是模型用错了。</p>
<p>比如之前我测试某个模型，就因为使用了一个旧版本的 transformers，然后模型就一直重复，改成 <code>trust_remote_code = True</code> 就可以了。当然也可以升级 Transformers 库。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/k6SMaKrAtcZkdwO7trJ7Tw">字节实习面试：“大模型推理 repetition penalty 是如何实现的？”</a></p>
<h1 id="4-给一个-14B-的模型和-20B-token-的数据，在-32张-A100-上要训练多少时间？"><a href="#4-给一个-14B-的模型和-20B-token-的数据，在-32张-A100-上要训练多少时间？" class="headerlink" title="4.给一个 14B 的模型和 20B token 的数据，在 32张 A100 上要训练多少时间？"></a>4.给一个 14B 的模型和 20B token 的数据，在 32张 A100 上要训练多少时间？</h1><p>根据之前 OpenAI 的 Scaling Law 的论文，里面对 Transformers 的计算量进行了估算，大概的过程是这样的：</p>
<p>假设</p>
<ul>
<li>C 是训练的总 FLOPs (floating point operations)</li>
<li>N 为模型的参数量</li>
<li>D 为训练的 token 量。</li>
</ul>
<p>Transformers 里绝大多数运输都是矩阵相乘，矩阵相乘的运算量是矩阵大小的两倍，因为<strong>对于矩阵中的每个元素来说，需要一次乘法和一次加法</strong>。</p>
<p>先说前向运算，每一个 token 都会在模型里运算一遍，所以对于一个 token 来说，前向运算的 FLOPs 为 2N。</p>
<p>那么对于 D 个 token 来说，前向运算的 FLOPs 为 2ND。</p>
<p>而神经网络的反向传播的 FLOPs 量，大致为前向运算的 2倍，共计 4ND。原理可以看：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247486124&idx=1&sn=c9294e6696047bb616e0f640fca3ff5f&scene=21#wechat_redirect">学妹问：“反向传播的计算量是前向传播计算量的几倍？”</a></p>
<p>所以，最终 C &#x3D; 6ND。</p>
<p><strong>当然这只是个大概的估计，实际上运算量会更多一些</strong>，因为还有一些非矩阵的运算。如果为了节省显存，采用了一些比如 Activation recomputation 等，计算量会显著增加。</p>
<p>知道了总的运算量，下一步就是查一下显卡的运算速度，就能得出题目的答案。</p>
<p>对于 A100 来说，FP16 的峰值运算速度为 312 TFLOPS (Tera Floating Point Operations Per Second), 也就是 $312\times10^{12}$</p>
<p>那么对于前面的任务来说，需要的时间为</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210110458915.png" srcset="/img/loading.gif" lazyload alt="时间"></p>
<p>上面的单位是秒，换算成天为 1.95 天。</p>
<p>但是注意这里只是最理想的情况下，实际上运算量比这个大，而且对于大模型的训练来说，<strong>根本无法达到 A100 的峰值运算速度</strong>，因为有大量的通信，而且内部运算也并不全是 FP16 的计算。</p>
<p>我们看看 llama 当时的训练数据：</p>
<p>When training a 65B-parameter model, our code processes around 380 tokens&#x2F;sec&#x2F;GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.</p>
<p>带入上面的公式:</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210110526113.png" srcset="/img/loading.gif" lazyload alt="时间"></p>
<p>换算成天数为 9.89 天，但是实际上用了 21 天。从这里可以推断出，他们的 GPU 的平均运算速度为 146.94 TFLOPS, MFU（Model FLOPs Utilization， 算力的利用率）为 47.1%</p>
<p>而且模型越大，MFU 越低，比如 Llama 3 405B的训练数据如下图， 最高的 MFU 也就 43%：</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739156742913-9.png" srcset="/img/loading.gif" lazyload alt="LLAMA3"></p>
<p>这已经是业界几乎最顶尖的水平了。</p>
<p>所以我们按照业界的顶尖水平来估算的话，最开始问题的答案修正为 4.135 天。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/kCQJwlDIeSfRnLra0JreeA">小米一面：“给一个 14B 的模型和 20B token 的数据，在 32张 A100 上要训练多少时间？”</a></p>
<h1 id="5-手撕-MHA，阿里的一面问的真是太细了"><a href="#5-手撕-MHA，阿里的一面问的真是太细了" class="headerlink" title="5. 手撕 MHA，阿里的一面问的真是太细了"></a>5. 手撕 MHA，阿里的一面问的真是太细了</h1><p>这个在面试的时候经常会问到， 但是涉及到的细节比较多。</p>
<p>核心的代码其实并不多，下面结合代码来说一下。</p>
<p>代码大概 10 行，实际 11行。10行也不是不行，但是为了可读性，还是11行把。</p>
<p>首先，要将输入 x 做3个线性映射得到 QKV</p>
<figure class="highlight gml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gml">q, k, v = <span class="hljs-symbol">self</span>.q_proj(<span class="hljs-variable language_">x</span>), <span class="hljs-symbol">self</span>.k_proj(<span class="hljs-variable language_">x</span>), <span class="hljs-symbol">self</span>.v_proj(<span class="hljs-variable language_">x</span>)<br></code></pre></td></tr></table></figure>

<p>因为要实现 multi attention， 所以要将最后一维切割。比如本来是 (4， 512， 128) 大小的矩阵， 现在有 8个头， 最后一维被切成8块， 变成了 一个 (4, 512, 8, 16) 的矩阵。</p>
<p>但是现在序列长度 512在矩阵的位置是第二位， 我们要保持 512 这一维和 16 这一维在一起，然后做 attention， 所以还需要做一个 view 的变换， 代码如下</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">k = k<span class="hljs-selector-class">.view</span>(B, T, self<span class="hljs-selector-class">.n_head</span>, C <span class="hljs-comment">// self.n_head).transpose(1, 2) # (B, nh, T, hs)</span><br><span class="hljs-selector-tag">q</span> = <span class="hljs-selector-tag">q</span><span class="hljs-selector-class">.view</span>(B, T, self<span class="hljs-selector-class">.n_head</span>, C <span class="hljs-comment">// self.n_head).transpose(1, 2) # (B, nh, T, hs)</span><br>v = v<span class="hljs-selector-class">.view</span>(B, T, self<span class="hljs-selector-class">.n_head</span>, C <span class="hljs-comment">// self.n_head).transpose(1, 2) # (B, nh, T, hs)</span><br></code></pre></td></tr></table></figure>

<p>接下来就可以求 attention 的分数了</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">att</span> = (q @ k.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) * (<span class="hljs-number">1</span>.<span class="hljs-number">0</span> / math.sqrt(k.size(-<span class="hljs-number">1</span>)))<br><span class="hljs-attribute">att</span> = F.softmax(att, dim=-<span class="hljs-number">1</span>)          <br></code></pre></td></tr></table></figure>

<p>注意这里要除以根号d， 至于为什么，可以看：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247485665&idx=1&sn=76dc6c4cfc932e3ccfe37c32cab08c72&scene=21#wechat_redirect">NLP面试官：“Attention为什么要除以根号d” 算法女生这么回答当场想发 offer</a></p>
<p>得到 attention 之后， 我们还需要对得分进行 mask， 因为当前位置不应该看到后面的信息。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">self.bias</span> = torch.tril(torch.<span class="hljs-literal">on</span>es(max_length, max_length)).view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, max_length, max_length)<br><span class="hljs-attr">att</span> = att.masked_fill(self.bias[:,:,:T,:T] == <span class="hljs-number">0</span>, torch.finfo(x.dtype).min)<br></code></pre></td></tr></table></figure>

<p>这个 self.bias 是一个 tril 矩阵。右上角全为 0。</p>
<p>得到分数后， 就可以和 v 相乘了, 并且还原到多头前的维度。</p>
<figure class="highlight gml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs gml"><span class="hljs-variable language_">y</span> = att @ v<br><span class="hljs-variable language_">y</span> = <span class="hljs-variable language_">y</span>.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(B, T, C)<br><span class="hljs-variable language_">y</span> = <span class="hljs-symbol">self</span>.o_proj(<span class="hljs-variable language_">y</span>)<br></code></pre></td></tr></table></figure>

<p>上面代码的最后是再一次做了个线性变换， 至此 MHA 算是写完了。</p>
<p>当然只是最基础版本的 MHA， 现在 flash attention 也已经是标配，如果在面试过程中你说我会写 flash attention， 那绝对是大大的加分项。</p>
<p>但是写 flash attention 有点难度，大概要 100 行，可以参考这个：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/tspeterkim/flash-attention-minimal/blob/main/flash.cu">https://github.com/tspeterkim/flash-attention-minimal/blob/main/flash.cu</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/shExbs3NPKqxqWEGgvnt8Q">手撕 MHA，阿里的一面问的真是太细了</a></p>
<h1 id="6-大模型推理的时候-top-k-和-top-p-同时设置的时候怎么采样？"><a href="#6-大模型推理的时候-top-k-和-top-p-同时设置的时候怎么采样？" class="headerlink" title="6. 大模型推理的时候 top k 和 top p 同时设置的时候怎么采样？"></a>6. 大模型推理的时候 top k 和 top p 同时设置的时候怎么采样？</h1><p>这个题目就是考察细节， 直接从 transformers 的源代码中找答案即可。</p>
<p>其实在 transformers 的4.41.x 版本之前， 在 <code>src/transformers/generation_utils.py</code> 中专门有段 topk 和 topp 的处理代码，写的很简洁， 如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">top_k_top_p_filtering</span>(<span class="hljs-params"></span><br><span class="hljs-params">    logits: Tensor,</span><br><span class="hljs-params">    top_k: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>,</span><br><span class="hljs-params">    top_p: <span class="hljs-built_in">float</span> = <span class="hljs-number">1.0</span>,</span><br><span class="hljs-params">    filter_value: <span class="hljs-built_in">float</span> = -<span class="hljs-built_in">float</span>(<span class="hljs-params"><span class="hljs-string">&quot;Inf&quot;</span></span>),</span><br><span class="hljs-params">    min_tokens_to_keep: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params"></span>) -&gt; Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot; Filter a distribution of logits using top-k and/or nucleus (top-p) filtering</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            logits: logits distribution shape (batch size, vocabulary size)</span><br><span class="hljs-string">            if top_k &gt; 0: keep only top k tokens with highest probability (top-k filtering).</span><br><span class="hljs-string">            if top_p &lt; 1.0: keep the top tokens with cumulative probability &gt;= top_p (nucleus filtering).</span><br><span class="hljs-string">                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)</span><br><span class="hljs-string">            Make sure we keep at least min_tokens_to_keep per batch example in the output</span><br><span class="hljs-string">        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> top_k &gt; <span class="hljs-number">0</span>:<br>        top_k = <span class="hljs-built_in">min</span>(<span class="hljs-built_in">max</span>(top_k, min_tokens_to_keep), logits.size(-<span class="hljs-number">1</span>))  <span class="hljs-comment"># Safety check</span><br>        <span class="hljs-comment"># Remove all tokens with a probability less than the last token of the top-k</span><br>        indices_to_remove = logits &lt; torch.topk(logits, top_k)[<span class="hljs-number">0</span>][..., -<span class="hljs-number">1</span>, <span class="hljs-literal">None</span>]<br>        logits[indices_to_remove] = filter_value<br><br>    <span class="hljs-keyword">if</span> top_p &lt; <span class="hljs-number">1.0</span>:<br>        sorted_logits, sorted_indices = torch.sort(logits, descending=<span class="hljs-literal">True</span>)<br>        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-<span class="hljs-number">1</span>), dim=-<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># Remove tokens with cumulative probability above the threshold (token with 0 are kept)</span><br>        sorted_indices_to_remove = cumulative_probs &gt; top_p<br>        <span class="hljs-keyword">if</span> min_tokens_to_keep &gt; <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)</span><br>            sorted_indices_to_remove[..., :min_tokens_to_keep] = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># Shift the indices to the right to keep also the first token above the threshold</span><br>        sorted_indices_to_remove[..., <span class="hljs-number">1</span>:] = sorted_indices_to_remove[..., :-<span class="hljs-number">1</span>].clone()<br>        sorted_indices_to_remove[..., <span class="hljs-number">0</span>] = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># scatter sorted tensors to original indexing</span><br>        indices_to_remove = sorted_indices_to_remove.scatter(<span class="hljs-number">1</span>, sorted_indices, sorted_indices_to_remove)<br>        logits[indices_to_remove] = filter_value<br>    <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure>

<p>从这个函数中可以清晰的看到， 是先 topk 然后再进行 top p 的。</p>
<p>先进行 topk 的处理， 选出最大的 k 个 logits， 然后剩余的 logits 值都设置为负无穷， 这样 softmax 的时候概率就变成 0 了。</p>
<p>然后在这 top k 个 logits 中，计算累计概率， 选出符合条件的 top p 个 logits。</p>
<p>我之前在自己写一些调试代码的时候，经常 import 这段代码。但这随着版本更新， 发现这个函数没了。</p>
<p>这一段逻辑跑到了 <code>src/transformers/generation/utils.py</code> 中， 如下所示</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-keyword">if</span> generation_config<span class="hljs-selector-class">.top_k</span> is not None and generation_config<span class="hljs-selector-class">.top_k</span> != <span class="hljs-number">0</span>:<br>    processors<span class="hljs-selector-class">.append</span>(<br>        <span class="hljs-built_in">TopKLogitsWarper</span>(top_k=generation_config<span class="hljs-selector-class">.top_k</span>, min_tokens_to_keep=min_tokens_to_keep)<br>    )<br><span class="hljs-keyword">if</span> generation_config<span class="hljs-selector-class">.top_p</span> is not None and generation_config<span class="hljs-selector-class">.top_p</span> &lt; <span class="hljs-number">1.0</span>:<br>    processors<span class="hljs-selector-class">.append</span>(<br>        <span class="hljs-built_in">TopPLogitsWarper</span>(top_p=generation_config<span class="hljs-selector-class">.top_p</span>, min_tokens_to_keep=min_tokens_to_keep)<br>    )<br></code></pre></td></tr></table></figure>

<p>可以看到处理的顺序还是没变， 只不过变成了 TopKLogitsWarper 和 TopPLogitsWarper。</p>
<p>很多代码往往越写越复杂， 封装的越来越多， 比如 langchain， 个人感觉就封装的略有点过头了。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/bTYvYbDQAzTXtQQ-vmxmQA">大模型推理的时候 top k 和 top p 同时设置的时候怎么采样？</a></p>
<h1 id="7-给一些-token-id-和-对应的-tokenizer，-可以将其无损的还原为原始文本么？"><a href="#7-给一些-token-id-和-对应的-tokenizer，-可以将其无损的还原为原始文本么？" class="headerlink" title="7. 给一些 token id 和 对应的 tokenizer， 可以将其无损的还原为原始文本么？"></a>7. 给一些 token id 和 对应的 tokenizer， 可以将其无损的还原为原始文本么？</h1><p>需要看 tokenizer 的实现算法， 如果 tokenizer 是采用类似 <strong>Byte-level BPE 的算法， 就可以做到无损还原</strong>。</p>
<p>这是因为 Byte-level 的 BPE 算法在构建 token 的时候， 完全是基于字节来统计的， 所以其可以对任意的数据进行 encode， 不只是局限于文本数据。</p>
<p>举一个具体的例子可能更好理解。比如下面这句话：</p>
<p>stay foolish stay hungry</p>
<p>如果是 word 粒度的 tokenize 方法， 那处理的对象是 word， 那第一步可能先把 stay 给变成 token， 因为 stay 出现了2次。</p>
<p>如果是 char 粒度的， 那处理的力度就是字母， 那会先把 st, ta, y_, oo 先变成 token。</p>
<p>非 Byte level 的， 总是基于现有文本元素来进行组合。比如英文就是 26 个大小写字母还有其他标点等符号， 中文则是汉字。</p>
<p>这样做总是会存在一个 Out Of Vocabulary 的问题， 比如现在有个单词， clever， 那就没法表示了， 只能用一个统一的token 来表示。</p>
<p>那自然英文的 tokenizer 就没法给中文做 tokenzier。</p>
<p>Byte-level 是怎么做的呢？<strong>处理的对象是字节</strong>。不管是什么语言，最终在计算机里都是二进制的数据。</p>
<p>比如单词 stay 在计算机里表示如下：</p>
<ul>
<li>s: 01110011 (0x73)</li>
<li>t: 01110100 (0x74)</li>
<li>a: 01100001 (0x61)</li>
<li>y: 01111001 (0x79)</li>
</ul>
<p>在训练 tokenize 模型的时候， 直接统计字节的 pair. 比如 (0x74, 0x74) 出现次数最多， 则优先变成 token。</p>
<p><strong>由于所有的数据都是由字节组成的，而初始的 token 只需要 256 个， 就覆盖了所有的可能</strong>， 所以完全没有 OOV 的问题。</p>
<p>比如汉字是由 Unicode 的组成的， 比如“看图学”这三个字， 用 Unicode 的十六进制表示为</p>
<ul>
<li>看：0x770B</li>
<li>图：0x56FE</li>
<li>学：0x5B66</li>
</ul>
<p>Byte level 的不管原始的信息是什么，就是直接用字节组合。</p>
<p>所以这个时候， 英文的 tokenizer 也可以给汉字编码， 因为汉字也可以表示成字节。</p>
<p>比如 llama 2 的词表里大概有5k 多个汉字， 其他不在词表里的汉字，可能需要3-5个 token 来表示一个汉字， 这就造成了吞吐效率的极大浪费，所以需要加入新的词表对 llama 进行 post pretrain 才行。</p>
<p>比如你问 llama2 问题， 对于输入的中文还是可以理解的，但是输出的时候还是倾向于用英文来输出。如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739158268947-12.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/nf2qVGlSbu6GNB4ODc0epQ">给一些 token id 和 对应的 tokenizer， 可以将其无损的还原为原始文本么？</a></p>
<h1 id="8-为什么-Attention-最后采用了-Dot-Product-而不是-Addition？"><a href="#8-为什么-Attention-最后采用了-Dot-Product-而不是-Addition？" class="headerlink" title="8. 为什么 Attention 最后采用了 Dot Product 而不是 Addition？"></a>8. 为什么 Attention 最后采用了 Dot Product 而不是 Addition？</h1><p>在 Attention 刚被提出来的时候，其实是通过 Addition 来建立关联关系的。在 Bahdanau Attention (Bahdanau et al., 2014) 中， 表示为<br>$$<br>e_{ij}&#x3D;V_a^T\tanh(W_as_{i-1}+U_ah_j)&#x3D;V_a^T\tanh(Q+K)<br>$$<br>后来， Luong Attention (Luong et al., 2015) 对比了多种 Attention 的方式， 其中就包括了 Transformers 最终采用的 dot attention， 表示为：<br>$$<br>e_{ij}&#x3D;W_Qs_{i-1}(W_Kh_j)^T&#x3D;QK^T<br>$$<br>在论文中， 作者提到 Addition Attention 在他们的实验里的效果并不太好。</p>
<p>在 《Attention is All your Need》 的论文中， 作者提到：</p>
<p>“<strong>While the two are similar in theoretical complexity</strong><strong>, dot-product attention is much faster and more space-efficient in practice,</strong> <strong>since</strong> **it can be implemented using highly optimized matrix multiplication code.**”</p>
<p>所以说无论是效果还是计算效率上， dot attention 似乎都更好一点。</p>
<p>那么计算效率到底快多少呢？</p>
<p>在论文《Data Movement Is All You Need: A Case Study On Optimizing Transformers》 的文中， 作者对比了 BERT 中不同运算的耗时。如下表：</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/image-20250210113748910.png" srcset="/img/loading.gif" lazyload alt="计算效率"></p>
<p>其中三角形 △ 代表的是矩阵相乘， 方块 ◻️ 代表的是 softmax and layer normalization， 圆形 ○ 则是剩余的运算，比如 biases, dropout, activations, and residual connections。</p>
<p>可以看到， **非矩阵相乘的运算量只占了 0.2%， 但是耗时却占据了 39%**。</p>
<p>之所以差异如此明显， 是因为 Nvidia 的显卡里专门设置了 Tensor Core， 和其他运算模块的对比如下：</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739158723357-15.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>可以看出，差距还是很明显的。FP32 使用 Tensor Core 计算的 TFLOPS 是普通 FP32 模块的 156&#x2F;19.5&#x3D; 8 倍！</p>
<p>在这个疯狂 Scaling 的时代， dot-product attention 似乎也就成为了唯一的选择。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/SnTCX2RWG8cqNoSs5lNmsw">“为什么 Attention 最后采用了 Dot Product 而不是 Addition？” 字节一面问的好细</a></p>
<h1 id="9-机器学习的三要素是什么？"><a href="#9-机器学习的三要素是什么？" class="headerlink" title="9. 机器学习的三要素是什么？"></a>9. 机器学习的三要素是什么？</h1><p>现在网络上众说纷纭，大概有几种说法：</p>
<ol>
<li>模型+策略+算法。来源：《统计学习方法》</li>
<li>数据+模型+算法。</li>
<li>数据+特征+算法。</li>
<li>表示+评估+优化。来源：Domingos 2012 《A few useful things to know about machine learning》</li>
<li>表示+策略+优化。</li>
</ol>
<p>这几种说法中，1和4是更为确切的。2，3略有欠缺，缺少了评估这一环节。5是1和4的另一种说法。</p>
<p>巧合的是，李航老师和Domingos教授都是2012年提出来的概念，可以说是英雄所见略同了。</p>
<p>李航老师所说的模型(Model)，Domingos 教授所说的表示(Representation)，说的都是<strong>把数据和模型参数映射到学习的假设空间(hypothesis space)，更朴素一点的说法可能是建模</strong>。数据、特征、要训练的模型其实都是表示的一部分。</p>
<p>李航老师所说的策略(Strategy)，Domingos教授所说的评估(Evaluation),说的都是要<strong>构造一个损失函数或者评价函数</strong>，来评价表示&#x2F;模型的好坏。</p>
<p>李航老师所说的算法(Algorithm)，Domingos教授所说的优化(Optimization),说的都是损失函数的优化算法。<strong>借助于损失函数在假设空间中找到问题最优的解</strong>。</p>
<p>整体来说，Domingos 教授的表述更宏观一些，李航老师的更接地气一些。</p>
<p>如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739158832564-18.png" srcset="/img/loading.gif" lazyload alt="表示评估优化"></p>
<p>需要注意的是，三要素之间不是任意可以组合的，模型适用的损失函数和优化方法都又一些原理蕴含其中。下图列举了一些常见三要素的组合关系</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739158866210-21.png" srcset="/img/loading.gif" lazyload alt="机器学习三要素"></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/GEKC6t5gZgKRAc9xIHdnzQ">“机器学习的三要素是什么？” 看上去简单，其实一点也…</a></p>
<h1 id="10-大模型训练为什么用梯度下降，而不是收敛更快的牛顿法？"><a href="#10-大模型训练为什么用梯度下降，而不是收敛更快的牛顿法？" class="headerlink" title="10. 大模型训练为什么用梯度下降，而不是收敛更快的牛顿法？"></a>10. 大模型训练为什么用梯度下降，而不是收敛更快的牛顿法？</h1><p>梯度下降是基于泰勒展开一阶偏微分的优化方法， 而牛顿法则是基于泰勒展开二阶偏微分的方法。</p>
<p>理论上牛顿法的收敛速度要比梯度下降要快的多，那为什么大模型和机器学习都采用梯度下降这种比较慢的优化方法呢？</p>
<p>原因有三：</p>
<h3 id="性能问题"><a href="#性能问题" class="headerlink" title="性能问题"></a><strong>性能问题</strong></h3><p>从<strong>迭代的次数</strong>来看， 牛顿法确实需要更少的迭代次数， 因为牛顿法是直接算出下一个极值点在哪里。但是梯度下降则是一步一步的逼近， 尤其是在接近最小值的时候， 梯度往往越来越小， 迭代也越来越慢，所以在梯度下降的 loss 刚开始下降很快，后面则趋于平缓。用牛顿法的话， loss 则陡峭的多, 如下图所示， 牛顿在第4次的时候就可以停止了：</p>
<p><img src="https://raw.githubusercontent.com/chongzicbo/images/main/picgo/640-1739159014703-24.png" srcset="/img/loading.gif" lazyload alt="迭代"></p>
<p><strong>但是也不能光看迭代次数， 还得看每次迭代的时间复杂度。</strong></p>
<p>对于梯度下降来说， 每次迭代的时间复杂度是 O(N), 其中 N 是参数量， 是线性的。</p>
<p>但是对于牛顿迭代来说， 由于要计算 Hessian 矩阵， 时间复杂度是 O(N2), 更糟糕的是， 牛顿法需要计算 Hessian 矩阵的逆， 时间复杂度一下子变成了 O(N3).</p>
<p>所以当参数规模较小的时候， 牛顿法确实会更快。但是当模型参数量很大的时候，牛顿法虽然总迭代次数少，但是每一步的时间复杂度很高， 最终的结果反而更慢。</p>
<h3 id="收敛稳定性问题"><a href="#收敛稳定性问题" class="headerlink" title="收敛稳定性问题"></a><strong>收敛稳定性问题</strong></h3><p><strong>牛顿法对于初始值的选择尤为敏感</strong>。如果刚开始就选择在了某个维度比较平坦的地方， 也就是二阶导数接近 0 的地方， 由于要求二阶导数的倒数， 计算出的下一个位置会和当前位置非常远， <strong>由于泰勒展开的二阶近似只在局部有效</strong>， 此时牛顿法的精确性已经失去了， 所以优化过程很容易就发散了。</p>
<p>当然可以通过加入一些阻尼来限制更新的不要太剧烈， 但是加入阻尼有时候反而丧失了牛顿收敛快的特性。</p>
<h3 id="鞍点问题"><a href="#鞍点问题" class="headerlink" title="鞍点问题"></a><strong>鞍点问题</strong></h3><p>相对来说， 梯度下降比起牛顿更容易脱离鞍点区域， 因为<strong>鞍点就是牛顿法的一个解， 到了鞍点牛顿法就认为已经完成优化了</strong>。</p>
<p>而梯度下降总是有个步长， 虽然在鞍点附近走的很慢，但是还是有机会走出来的。除非恰好就在某一个维度的鞍点上来回震荡， 这个时候可能需要加入点随机扰动才能走出鞍点。</p>
<h2 id="牛顿法的优化"><a href="#牛顿法的优化" class="headerlink" title="牛顿法的优化"></a><strong>牛顿法的优化</strong></h2><p>虽然直接用牛顿法优化不太可行，但是这么多年来也有很多算法借鉴了牛顿法的二阶优化的思想。</p>
<p>比如拟牛顿法 BFGS和L-BFGS， 而现在流行的一些基于动量的优化器， 比如 Adam中的二阶矩项（momentum term）,个人感觉其实就是在近似二阶Hessian信息，或者至少是在近似其对角线元素。这种近似使得Adam能够在保持计算效率的同时，捕获一定的二阶优化信息。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/DgH2MneljoGDxUx8tJAQig">大模型训练为什么用梯度下降，而不是收敛更快的牛顿法？阿里二面问的好有难度</a></p>
<h1 id="11-如何评估大模型的性能？目前的评估方法都有什么？"><a href="#11-如何评估大模型的性能？目前的评估方法都有什么？" class="headerlink" title="11.如何评估大模型的性能？目前的评估方法都有什么？"></a>11.如何评估大模型的性能？目前的评估方法都有什么？</h1><p>关于如何系统的评估一个大模型，之前微软有一篇综述写的很好，而且有中文版，可以直接点击下面网页查看：</p>
<p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/articles/evaluation-of-large-language-models/">https://www.microsoft.com/en-us/research/articles/evaluation-of-large-language-models/</a></p>
<p>但是，这些传统的测评方法目前都面临着一些问题。下面稍微列举几点：</p>
<h3 id="数据污染造成的刷榜问题"><a href="#数据污染造成的刷榜问题" class="headerlink" title="数据污染造成的刷榜问题"></a><strong>数据污染造成的刷榜问题</strong></h3><p>由于大多数测试集合都是开源的，很容易通过合成数据来构造一批类似的样本。这都报速成班了，那结果自然很不错。</p>
<p>这种刷榜怎么识别呢？比较简单的方法就是用新的数据去检测，比如每年新出的高考题，之前肯定谁都没见过，在这些题目上做的好，才是真的好。</p>
<h3 id="当前的测评数据已经落后于模型的真正性能"><a href="#当前的测评数据已经落后于模型的真正性能" class="headerlink" title="当前的测评数据已经落后于模型的真正性能"></a><strong>当前的测评数据已经落后于模型的真正性能</strong></h3><p>现在的模型已经逐渐能解决越来越复杂的问题，比如 GPT o1， 而这些传统的测试集合往往都比较简单，并不能测试出模型的真正能力。</p>
<h3 id="王自如困境"><a href="#王自如困境" class="headerlink" title="王自如困境"></a><strong>王自如困境</strong></h3><p>除了开源的一些测评，业内也有一些野榜混淆视听。</p>
<p>毕竟搞测评也得花钱，团队里的人也得发工资，现在有人赞助测评机构了，把人家的排名弄的很低，好意思么？</p>
<p>所以很多野榜都很难保证公平性。</p>
<h2 id="目前唯一可信的测评榜单"><a href="#目前唯一可信的测评榜单" class="headerlink" title="目前唯一可信的测评榜单"></a><strong>目前唯一可信的测评榜单</strong></h2><p>目前没办法刷榜的就是  Chatbot Arena 了。这个排行是让大模型随机匹配对手打天梯。</p>
<p>然后背后有一套积分系统，应该是 Elo 评分系统。随机让两个模型对相同的输入生产答案，然后让人盲评这两个模型的好坏。</p>
<p>当模型击败一个积分更高的模型时，得分会提升，反之则会下降。就这样最后所有的模型都有一个积分，就可以看出每个模型的好坏。</p>
<p>但是这个评测有一点需要注意，那就是<strong>一个刚进入系统的新模型，可能会因为对战的数据不足导致测评结果有偏差</strong>。比如 Claude 2 刚进系统的时候，评分要比 Claude 1 要低，但是随着对战次数的增加，效果就越来越好了。</p>
<p>然后这个榜单还有一些细分选项，比如 Style-controlled ranking。让模型生成一些有约束的风格，比如长度，格式等。这时每个模型等排名会发生变化，也挺有意思。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/bBCndI-cSuJI_5myzb74vA">字节一面：大模型如何评测以及当前测评的困境</a></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/" class="category-chain-item">笔试面试</a>
  
  
    <span>></span>
    
  <a href="/categories/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/" class="category-chain-item">AI算法</a>
  
  
    <span>></span>
    
  <a href="/categories/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/" class="category-chain-item">nlp</a>
  
  

  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/nlp/" class="print-no-link">#nlp</a>
      
        <a href="/tags/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/" class="print-no-link">#笔试面试</a>
      
        <a href="/tags/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/" class="print-no-link">#算法面试</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>“看图学”试题合集</div>
      <div>https://chongzicbo.github.io/2025/02/10/笔试面试/AI算法/nlp/AI笔试面试题001：公众号“看图学”题目合集/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>程博</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>February 10, 2025</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/01/02/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP%E5%9F%BA%E7%A1%8003%EF%BC%9A%E7%AE%80%E5%8D%95%E7%9A%84%20HTTP%E5%8D%8F%E8%AE%AE/" title="HTTP基础03：简单的HTTP协议">
                        <span class="hidden-mobile">HTTP基础03：简单的HTTP协议</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar category-bar" style="margin-left: -1rem">
    





<div class="category-list">
  
  
    
    
    
    <div class="category row nomargin-x">
      <a class="category-item 
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="笔试面试"
        id="heading-c0a64144467cb7c553d64e018103720a" role="tab" data-toggle="collapse" href="#collapse-c0a64144467cb7c553d64e018103720a"
        aria-expanded="true"
      >
        笔试面试
        <span class="list-group-count">(1)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse show" id="collapse-c0a64144467cb7c553d64e018103720a"
           role="tabpanel" aria-labelledby="heading-c0a64144467cb7c553d64e018103720a">
        
        
          
          
  <div class="category-post-list">
    
    
  </div>

          
  
    
    
    
    <div class="category-sub row nomargin-x">
      <a class="category-subitem 
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="AI算法"
        id="heading-497d6b71d600944e9158c90081f69bcd" role="tab" data-toggle="collapse" href="#collapse-497d6b71d600944e9158c90081f69bcd"
        aria-expanded="true"
      >
        AI算法
        <span class="list-group-count">(1)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse show" id="collapse-497d6b71d600944e9158c90081f69bcd"
           role="tabpanel" aria-labelledby="heading-497d6b71d600944e9158c90081f69bcd">
        
        
          
          
  <div class="category-post-list">
    
    
  </div>

          
  
    
    
    
    <div class="category-sub row nomargin-x">
      <a class="category-subitem 
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="nlp"
        id="heading-4c5adbed16b4c9d16698f71cca4218cb" role="tab" data-toggle="collapse" href="#collapse-4c5adbed16b4c9d16698f71cca4218cb"
        aria-expanded="true"
      >
        nlp
        <span class="list-group-count">(1)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse show" id="collapse-4c5adbed16b4c9d16698f71cca4218cb"
           role="tabpanel" aria-labelledby="heading-4c5adbed16b4c9d16698f71cca4218cb">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/2025/02/10/%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95/AI%E7%AE%97%E6%B3%95/nlp/AI%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E9%A2%98001%EF%BC%9A%E5%85%AC%E4%BC%97%E5%8F%B7%E2%80%9C%E7%9C%8B%E5%9B%BE%E5%AD%A6%E2%80%9D%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86/" title="“看图学”试题合集"
           class="list-group-item list-group-item-action
           active">
          <span class="category-post">“看图学”试题合集</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
        
      </div>
    </div>
  
        
      </div>
    </div>
  
</div>


  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.utils.listenDOMLoaded(function() {
      Fluid.events.registerRefreshCallback(function() {
        if ('mermaid' in window) {
          mermaid.init();
        }
      });
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://github.com/chongzicbo" target="_blank" rel="nofollow noopener"><span>Github</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
